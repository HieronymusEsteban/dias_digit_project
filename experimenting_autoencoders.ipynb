{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44707144-936b-44fc-914e-9f3d9fd9cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f297ae-8880-4772-b39e-5932ac40d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "import sklearn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd72b0-327c-4bfe-8cfa-726d7aa2e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\" \n",
    "\n",
    "import tf_keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d2a4c-027b-4f05-a9cc-fe93ce93f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442eedd5-6b15-4dfa-b7b6-25638bb81457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ed336-6528-4254-8276-bc3c5078d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"generative\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3636f82-ab5a-41a5-a16a-378af7ac4fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346175a-4ac6-435d-b370-2e134c7f66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb36e244-09ad-42f8-b859-f5164a071211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc417f99-140e-4b57-83f9-dd1da1e84a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91236680-0279-4693-918d-bfca947e767c",
   "metadata": {},
   "source": [
    "## Define paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e77575c-5ef3-4639-bda9-1d2776193d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4da0ff-3e0a-4f27-8af6-54815843d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_path = Path('/Users/stephanehess/Documents/CAS_AML/dias_digit_project/project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f1c08-f9b3-478c-ac51-2c60475c403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = Path.cwd()\n",
    "root_path = (project_path / '..').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced5f023-21fa-46e6-8deb-19e6ceee193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_1 = root_path/\"data_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c8057-530c-4e80-8e32-73e9dc2793d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21f424-baa8-4f79-b6da-b68ca1be6709",
   "metadata": {},
   "source": [
    "## Load and process all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde06d2-e6bf-4201-831d-66e6d4a9d5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2666cb32-1bb3-496c-a7ea-f1a05ff79fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a6c4e-ef75-48b5-bebe-d75d961da7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(image_dir_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a2324-cf37-41a9-a1dc-db122ad12f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in Path(image_dir_1).glob(\"*.tif\"):\n",
    "    print(image_path)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922323fe-8883-4707-9516-c4d7c55dda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_images(image_dir, validation_split=0.2):\n",
    "    image_arrays = []\n",
    "    image_identifiers = []\n",
    "\n",
    "    # Determine the smallest square size\n",
    "    target_size = None\n",
    "    for image_path in Path(image_dir).glob(\"*.tif\"):\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "                min_dim = min(width, height)\n",
    "                if target_size is None or min_dim < target_size:\n",
    "                    target_size = min_dim\n",
    "        except Exception as e:\n",
    "            print(f\"Error determining size for {image_path}: {e}\")\n",
    "\n",
    "    if target_size is None:\n",
    "        raise ValueError(\"No valid images found in the directory.\")\n",
    "\n",
    "    # Process and resize all images\n",
    "    for image_path in Path(image_dir).glob(\"*.tif\"):\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                # Convert to grayscale\n",
    "                img = img.convert(\"L\")  # 'L' mode ensures single-channel grayscale\n",
    "\n",
    "                # Extract dimensions\n",
    "                width, height = img.size\n",
    "\n",
    "                # Calculate cropping box to center the crop\n",
    "                top = (height - target_size) // 2\n",
    "                left = (width - target_size) // 2\n",
    "                bottom = top + target_size\n",
    "                right = left + target_size\n",
    "\n",
    "                # Crop and resize the image to ensure square dimensions\n",
    "                img_cropped = img.crop((left, top, right, bottom))\n",
    "                img_resized = img_cropped.resize((target_size, target_size))\n",
    "                \n",
    "                # Convert to numpy array and append to list\n",
    "                image_arrays.append(np.array(img_resized))\n",
    "                \n",
    "                \n",
    "                # Extract identifier from the last three characters of the file name\n",
    "                identifier = image_path.stem[-3:]\n",
    "                image_identifiers.append(identifier)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "    # Convert to numpy array (all images are guaranteed to have the same size)\n",
    "    image_arrays = np.array(image_arrays)\n",
    "\n",
    "    # Shuffle and split into training and validation sets\n",
    "    data_indices = list(range(len(image_arrays)))\n",
    "    random.shuffle(data_indices)\n",
    "\n",
    "    val_count = int(len(image_arrays) * validation_split)\n",
    "    val_indices = data_indices[:val_count]\n",
    "    train_indices = data_indices[val_count:]\n",
    "\n",
    "    train_data = image_arrays[train_indices]\n",
    "    val_data = image_arrays[val_indices]\n",
    "\n",
    "    train_identifiers = [image_identifiers[i] for i in train_indices]\n",
    "    val_identifiers = [image_identifiers[i] for i in val_indices]\n",
    "\n",
    "    return train_data, val_data, train_identifiers, val_identifiers\n",
    "\n",
    "# Process images and split into training/validation sets\n",
    "train_images, val_images, train_ids, val_ids = load_process_images(image_dir_1)\n",
    "\n",
    "# Print shapes and identifiers\n",
    "print(f\"Training data shape: {train_images.shape}\")\n",
    "print(f\"Validation data shape: {val_images.shape}\")\n",
    "print(f\"Training identifiers: {train_ids}\")\n",
    "print(f\"Validation identifiers: {val_ids}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cbfc6-1c62-484c-a3cf-298f8b5deeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2972174e-a3d6-458c-b479-8e60bb538cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b67f63b-4fe1-4fdc-a8fd-af89c7042b1d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def load_and_downsample_images(image_paths):\n",
    "    \"\"\"\n",
    "    Load images, downsample them to the smallest dimensions, and return as a numpy array.\n",
    "    \"\"\"\n",
    "    image_arrays = []\n",
    "    for path in image_paths:\n",
    "        img = Image.open(path).convert(\"L\")  # Convert to grayscale\n",
    "        image_arrays.append(np.array(img))\n",
    "    \n",
    "    # Find the smallest dimensions among all images\n",
    "    min_height = min(img.shape[0] for img in image_arrays)\n",
    "    min_width = min(img.shape[1] for img in image_arrays)\n",
    "    \n",
    "    # Resize images to the smallest dimensions\n",
    "    resized_images = [\n",
    "        np.array(Image.fromarray(img).resize((min_width, min_height)))\n",
    "        for img in image_arrays\n",
    "    ]\n",
    "    \n",
    "    # Stack into a 3D numpy array\n",
    "    return np.stack(resized_images, axis=0)\n",
    "\n",
    "def load_images_and_split(image_dir, validation_split=0.2):\n",
    "    \"\"\"\n",
    "    Load images from a directory, downsample them, split into training and validation sets, \n",
    "    and extract image identifiers.\n",
    "    \"\"\"\n",
    "    # Step 1: Gather all image paths\n",
    "    image_paths = list(Path(image_dir).glob(\"*.tif\"))\n",
    "    if not image_paths:\n",
    "        raise ValueError(\"No images found in the specified directory.\")\n",
    "\n",
    "    # Extract image identifiers (last three characters of the filename)\n",
    "    image_identifiers = [path.stem[-3:] for path in image_paths]\n",
    "    \n",
    "    # Step 2: Load and downsample all images\n",
    "    all_images = load_and_downsample_images(image_paths)\n",
    "    \n",
    "    # Step 3: Split into training and validation sets\n",
    "    train_data, val_data, train_ids, val_ids = train_test_split(\n",
    "        all_images, image_identifiers, test_size=validation_split, random_state=42\n",
    "    )\n",
    "    \n",
    "    return train_data, val_data, train_ids, val_ids\n",
    "\n",
    "# Example usage\n",
    "image_dir = root_path/\"../data\"\n",
    "train_data, val_data, train_ids, val_ids = load_images_and_split(image_dir)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "print(f\"Training image identifiers: {train_ids}\")\n",
    "print(f\"Validation image identifiers: {val_ids}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb16f6d7-e7d9-4aaf-8595-f84b16bc35dd",
   "metadata": {},
   "source": [
    "## Encode one single image as the simple most possible task (using dense network):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245fca1-7dda-4efc-830c-d14ab19f9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_image = train_images[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b5fc0-85bc-4269-b5fe-a6b138db8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e51a57-02d6-4619-9c64-c893aeed49a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082489b-45cd-4c9f-9308-0cdee9933fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(try_image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea77631-2178-426c-ac7e-69d85fd9cac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdf56bb1-0885-4822-affa-9ee03d879336",
   "metadata": {},
   "source": [
    "## Set image dimensions to be used in dense model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2c927-84c1-4368-9939-fd9f42ebdb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dim = try_image.shape[1]\n",
    "min_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa65b2d-6d11-4680-bf20-1302e57f80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "stacked_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "])\n",
    "stacked_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(min_dim * min_dim),\n",
    "    tf.keras.layers.Reshape([min_dim, min_dim])\n",
    "])\n",
    "stacked_ae = tf.keras.Sequential([stacked_encoder, stacked_decoder])\n",
    "\n",
    "stacked_ae.compile(loss=\"mse\", optimizer=\"nadam\")                   \n",
    "history = stacked_ae.fit(try_image, try_image, epochs=30,\n",
    "                         validation_data=(try_image, try_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f559cb1-5eee-4f8a-9a3f-c1dcb210c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = stacked_ae.predict(try_image[0:1])\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(try_image[0])\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(reconstructions[0])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71352820-2c64-4283-8489-7ad89aedf5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3129590e-a300-4b75-8e65-5078a10d0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045f71e-9c6b-4ba1-bab0-7f65ee853fb4",
   "metadata": {},
   "source": [
    "## Find two similar images to make the task a bit more complicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc246469-3624-4501-94f6-878636e7bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1 = train_ids.index('022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e831ec6e-cc0c-46ae-bdcf-1bb2fa32f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_2 = train_ids.index('023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77718c1a-57de-4eb5-ad3b-6e2373828cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[idx_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78082678-cb1f-4be0-b0ec-0fad7e79121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[idx_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26748a36-ce32-4502-ab39-599d707f53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images_1 = np.array([train_images[idx_1], train_images[idx_2]])\n",
    "similar_images_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034e5fb-fcc6-4115-9981-eee1f9834e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "stacked_ae = tf.keras.Sequential([stacked_encoder, stacked_decoder])\n",
    "\n",
    "stacked_ae.compile(loss=\"mse\", optimizer=\"nadam\")                   \n",
    "history = stacked_ae.fit(similar_images_1, similar_images_1, epochs=30,\n",
    "                         validation_data=(similar_images_1, similar_images_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbeae0c-9b12-4d54-88c8-03b409fe18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = stacked_ae.predict(similar_images_1[0:2])\n",
    "for image_idx in range(0, similar_images_1.shape[0]):\n",
    "    print(image_idx)\n",
    "    plt.subplot(2, 2, image_idx + 1)\n",
    "    plt.imshow(similar_images_1[image_idx])\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(2, 2, image_idx + 2 + 1)\n",
    "    plt.imshow(reconstructions[image_idx])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78425b5d-9694-4c41-a5cb-f5e452e0df34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13fd9fb8-8d20-4b79-9157-a4e814d70717",
   "metadata": {},
   "source": [
    "## Rebuild model with latent space of 300 and train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31528baa-6118-4273-983f-44df614a6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "stacked_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(500, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "])\n",
    "stacked_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(500, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(min_dim * min_dim),\n",
    "    tf.keras.layers.Reshape([min_dim, min_dim])\n",
    "])\n",
    "stacked_ae = tf.keras.Sequential([stacked_encoder, stacked_decoder])\n",
    "\n",
    "stacked_ae.compile(loss=\"mse\", optimizer=\"nadam\")                   \n",
    "history = stacked_ae.fit(similar_images_1, similar_images_1, epochs=55,\n",
    "                         validation_data=(similar_images_1, similar_images_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4210e335-77ae-4ff5-ae74-a24da0c03e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = stacked_ae.predict(similar_images_1[0:2])\n",
    "for image_idx in range(0, similar_images_1.shape[0]):\n",
    "    print(image_idx)\n",
    "    plt.subplot(2, 2, image_idx + 1)\n",
    "    plt.imshow(similar_images_1[image_idx])\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(2, 2, image_idx + 2 + 1)\n",
    "    plt.imshow(reconstructions[image_idx])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfbeddc-efe0-42b9-bf92-d38557d39034",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(reconstructions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af84c954-4c62-48e8-a56e-4b511a4e7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(reconstructions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28459df6-74f8-4a6b-b544-5df10513805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "similar_images_1_compressed = stacked_encoder.predict(similar_images_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b6452-5dc7-4984-aed2-cacb76f6d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(similar_images_1_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088128e2-c6e0-4d3d-a03f-28fba85ec9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images_1_compressed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e044f-623b-4929-9e5b-10feaa5b6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#similar_images_1_compressed = stacked_encoder.predict(similar_images_1)\n",
    "tsne = TSNE(perplexity= 1, init=\"pca\", learning_rate=\"auto\", random_state=42)\n",
    "X_valid_2D = tsne.fit_transform(similar_images_1_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f1a5d-95e2-4bf7-90e3-26affb4b27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], s=10, cmap=\"tab10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5f34b-3c72-40ca-b7cb-bf026095e989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bd7fb4a-ff34-4e4d-b6ef-ce608900c279",
   "metadata": {},
   "source": [
    "## Try the same with a convolutional neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28213a26-1285-47da-bc95-96366b6b951a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66fa3a6-a294-4b53-9637-6b72361ccfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_img_square = 572"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52b329-7c03-4b3c-b17c-d10b522167db",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images_1 = similar_images_1.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb5986-ceb9-452f-8c55-3244b5268508",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images_1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88702c3b-5020-44cf-88c2-635952ecefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e8dc3-410f-49f7-9f2d-68224276fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ecb84f-c643-49bb-8332-2a93c0daa00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c444dea5-cbb9-4ce1-92be-0d22d9be27fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed79902-079c-444b-bba8-c69a1c9763e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_images(image_data, crop_size):\n",
    "    \"\"\"\n",
    "    Crops images stored in a NumPy array to the specified size.\n",
    "    \n",
    "    Parameters:\n",
    "        image_data (numpy.ndarray): The input image data of shape (n, m, m), where n is the number of images.\n",
    "        crop_size (int): The desired dimension e for the cropped images (output shape will be (n, e, e)).\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Cropped image data of shape (n, e, e).\n",
    "    \"\"\"\n",
    "    n, m, _ = image_data.shape  # Get number of images and dimensions of each image\n",
    "    e = crop_size  # Target size (e x e)\n",
    "    \n",
    "    if e >= m:\n",
    "        raise ValueError(\"Crop size must be smaller than the original image size.\")\n",
    "    \n",
    "    # Calculate cropping margins\n",
    "    start = (m - e) // 2  # Start index for cropping\n",
    "    end = start + e       # End index for cropping\n",
    "    \n",
    "    # Crop each image\n",
    "    cropped_images = image_data[:, start:end, start:end]\n",
    "    \n",
    "    return cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b1b733-2dd6-4607-887d-ad9976d21b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "9*3*4*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211c0b5-e9a3-4f60-986f-7b897b5ced5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_dim = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22854200-73bd-46c9-bee2-242c2c60a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images = crop_images(similar_images_1, desired_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac4460-0875-40da-95a0-a26b886aa056",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f90a2-1562-4542-8e84-4992e177e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cropped_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c9f86-d6f6-4c9e-a8a8-4a5cd3e2ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cropped_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648bf0c5-0ed2-45b1-abf6-dd090c0fbf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images = cropped_images.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba0d1a-d640-4a68-ac0f-cc81c3ff25fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178c17a-a85a-4d01-8ddf-6c2add64f34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe7c24c-f252-4247-b002-4357b1810304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feadc78-dc92-45c1-a864-78bf368d8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train_full = X_train_full.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e5684-907b-4431-817b-7c330f15222c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320f0567-4f18-4440-8853-595f0236585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509aa7ef-75fb-456b-9b1e-9960084b6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_select = X_train[0:2].copy()\n",
    "X_train_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a59121-5f30-4502-b63d-385b178d2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be4de0-6700-46eb-af2a-de9df1586e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_select = X_valid[0:500].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2cf5c1-0cd8-452b-bc53-448be3b93d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train_select[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca56de4-16fb-467f-8040-88020ff929d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train_select[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681b07d-7acb-422b-a47c-d0fd0a309020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a6bf1-61f4-46d4-88be-cb9973b8c5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf041ce-194b-4612-a466-6d51d389a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment_data = cropped_images\n",
    "experiment_data = X_train_select\n",
    "\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "conv_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape([28, 28, 1]),\n",
    "    tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 14 × 14 x 16\n",
    "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 7 × 7 x 326\n",
    "    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 3 × 3 x 64\n",
    "    tf.keras.layers.Conv2D(30, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.GlobalAvgPool2D()  # output: 30\n",
    "])\n",
    "conv_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3 * 3 * 16),\n",
    "    tf.keras.layers.Reshape((3, 3, 16)),\n",
    "    tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(16, 3, strides=2, padding=\"same\",\n",
    "                                    activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(1, 3, strides=2, padding=\"same\"),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "conv_ae = tf.keras.Sequential([conv_encoder, conv_decoder])\n",
    "\n",
    "# extra code – compiles and fits the model\n",
    "conv_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = conv_ae.fit(experiment_data, experiment_data, epochs=10,\n",
    "                      validation_data=(experiment_data, experiment_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340cb2f-6e56-4cba-b846-6011e85c6af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = conv_ae.predict(X_train_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568a4e4-f211-41d2-abd4-dc2bb4b3932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_idx in range(0, experiment_data.shape[0]):\n",
    "    #print(image_idx)\n",
    "    plt.subplot(2, experiment_data.shape[0], image_idx + 1)\n",
    "    plt.imshow(experiment_data[image_idx], cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(2, experiment_data.shape[0], image_idx + experiment_data.shape[0] + 1)\n",
    "    plt.imshow(reconstructions[image_idx], cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd1b42-db44-4444-8112-0e1de6f056a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ae6a9-bee1-4652-93c1-45bff005acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628c2f5-e74a-45f5-a7b4-8d70ae9b971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e337062-e97f-4f82-8c35-2f97e82e8a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd3791-d310-4e98-8bc0-5696827cdd7c",
   "metadata": {},
   "source": [
    "## Use larger data set to showcase that the convolutional network needs training to adequately encode images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab829f1-9097-44a0-a2f6-465514b063ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "conv_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape([28, 28, 1]),\n",
    "    tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 14 × 14 x 16\n",
    "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 7 × 7 x 326\n",
    "    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 3 × 3 x 64\n",
    "    tf.keras.layers.Conv2D(30, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.GlobalAvgPool2D()  # output: 30\n",
    "])\n",
    "conv_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3 * 3 * 16),\n",
    "    tf.keras.layers.Reshape((3, 3, 16)),\n",
    "    tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(16, 3, strides=2, padding=\"same\",\n",
    "                                    activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(1, 3, strides=2, padding=\"same\"),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "conv_ae = tf.keras.Sequential([conv_encoder, conv_decoder])\n",
    "\n",
    "# extra code – compiles and fits the model\n",
    "conv_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = conv_ae.fit(X_train, X_train, epochs=10,\n",
    "                      validation_data=(X_valid_select, X_valid_select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bb4b2-985c-4851-bb3a-5b1ab81b3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = conv_ae.predict(X_train_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f912ac0-5452-4c31-86e5-3dbf9f094330",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadafdee-402f-434a-9fe4-28dc852b71af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ea714-bd42-43aa-bb11-aff1d11362aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb00157-7209-4649-b2a5-c26ac9eb80f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c106bc-65c5-42f2-a689-2c1d8d4ed430",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b16dae-bbcc-41ca-a553-0252a06a975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_select.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ef11f-11ac-46b3-9b30-63ad888b6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for image_idx in range(0, X_train_select.shape[0]):\n",
    "    #print(image_idx)\n",
    "    plt.subplot(2, X_train_select.shape[0], image_idx + 1)\n",
    "    plt.imshow(X_train_select[image_idx], cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(2, X_train_select.shape[0], image_idx + X_train_select.shape[0] + 1)\n",
    "    plt.imshow(reconstructions[image_idx], cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf87ccc-9f18-4c1c-9044-729ebb48ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstructions(conv_ae, images=X_train_select, n_images=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63bf4da-0e2c-4245-b8a0-f2e42098e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU¨\n",
    "\n",
    "\n",
    "#DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size = 3, padding='same', \n",
    " #                       activation = 'relu', kernel_initializer='he_normal')\n",
    "\n",
    "conv_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape([desired_dim, desired_dim, 1]),\n",
    "    tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=4),  \n",
    "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=4),  \n",
    "    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=3),  \n",
    "    tf.keras.layers.Conv2D(64, 9, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.GlobalAvgPool2D()  # output: 30\n",
    "])\n",
    "conv_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(9 * 9 * 16),\n",
    "    tf.keras.layers.Reshape((9, 9, 16)),\n",
    "    tf.keras.layers.Conv2DTranspose(32, 3, strides=3, activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(16, 3, strides=4, padding=\"same\",\n",
    "                                    activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(1, 3, strides=4, padding=\"same\"),\n",
    "    tf.keras.layers.Reshape([desired_dim, desired_dim])\n",
    "])\n",
    "conv_ae = tf.keras.Sequential([conv_encoder, conv_decoder])\n",
    "\n",
    "# extra code – compiles and fits the model\n",
    "conv_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = conv_ae.fit(cropped_images, cropped_images, epochs=30,\n",
    "                      validation_data=(cropped_images, cropped_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ccd662-95ae-4fbf-bc17-76d4c05dafde",
   "metadata": {},
   "source": [
    "## Putative formula for calculating outputs of hidden layers:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3d84f9e-1454-4f4c-a0f6-82fca7c5a3c1",
   "metadata": {},
   "source": [
    "Hout=(Hin−1)×stride+kernel size−2×padding\n",
    "Wout=(Win−1)×stride+kernel size−2×padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26a99e-ba71-46a2-89bc-67293655d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bdbd6-8a9c-41d9-8bb1-2284eefc45d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba7e5e-8a68-4d0c-9774-dd141a15a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b746e2f-87c8-4422-9737-7d2d86faa17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = conv_ae.predict(cropped_images[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0b6d9-492c-46fc-87b4-d24f229c5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eec39b-cbeb-41f6-8706-ee90e32d071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40fa7b7-8163-4b5b-b18f-c4cda4868575",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(reconstructions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd782ed-caa8-482a-abd5-ce6a0fb9f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = conv_ae.predict(cropped_images[0:2])\n",
    "for image_idx in range(0, cropped_images.shape[0]):\n",
    "    print(image_idx)\n",
    "    plt.subplot(2, 2, image_idx + 1)\n",
    "    plt.imshow(cropped_images[image_idx], cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(2, 2, image_idx + 2 + 1)\n",
    "    plt.imshow(reconstructions[image_idx], cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034fda6d-5bf2-416d-8de9-f7158972512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ff8e5-6d5e-4a98-a523-8d841c6529ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b59bf7a-5a10-471c-bd94-79291b5c0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train_full = X_train_full.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21ba32-b24c-4dc6-910c-351de62fc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb947bb7-6e80-44e9-9385-c57998255db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedb0af-01dd-4777-87dc-b72c7ca94934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_reconstructions(model, images=X_valid, n_images=5):\n",
    "    reconstructions = np.clip(model.predict(images[:n_images]), 0, 1)\n",
    "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plt.imshow(images[image_index], cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plt.imshow(reconstructions[image_index], cmap=\"binary\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6d07c-64bc-4d63-8c28-e918eccd0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "conv_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape([28, 28, 1]),\n",
    "    tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 14 × 14 x 16\n",
    "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 7 × 7 x 326\n",
    "    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 3 × 3 x 64\n",
    "    tf.keras.layers.Conv2D(30, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.GlobalAvgPool2D()  # output: 30\n",
    "])\n",
    "conv_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3 * 3 * 16),\n",
    "    tf.keras.layers.Reshape((3, 3, 16)),\n",
    "    tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(16, 3, strides=2, padding=\"same\",\n",
    "                                    activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(1, 3, strides=2, padding=\"same\"),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "conv_ae = tf.keras.Sequential([conv_encoder, conv_decoder])\n",
    "\n",
    "# extra code – compiles and fits the model\n",
    "conv_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = conv_ae.fit(X_train, X_train, epochs=10,\n",
    "                      validation_data=(X_valid, X_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce38124-58f7-40ca-a08e-14a453ccf487",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstructions(conv_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7161c511-44bf-4c8a-940d-1cab0ad5fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0], cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94228c-be73-4600-90e5-310c58da8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02212ac-4cf8-4ad2-8432-af25440ddb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_reconstructions(model, images=X_valid, n_images=5):\n",
    "    reconstructions = np.clip(model.predict(images[:n_images]), 0, 1)\n",
    "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plt.imshow(images[image_index], cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plt.imshow(reconstructions[image_index], cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plot_reconstructions(stacked_ae)\n",
    "save_fig(\"reconstruction_plot\")  # extra code – saves the high res figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9085ab7-4055-4b24-85cd-86843b2c97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_valid_compressed = stacked_encoder.predict(X_valid)\n",
    "tsne = TSNE(init=\"pca\", learning_rate=\"auto\", random_state=42)\n",
    "X_valid_2D = tsne.fit_transform(X_valid_compressed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac329c-16d5-4727-bf42-3bd6f901a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=\"tab10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d61e7e-f9a4-4dc9-a1e0-578ac5e15cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd9f84-b544-46da-b05f-efccacb5045b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
