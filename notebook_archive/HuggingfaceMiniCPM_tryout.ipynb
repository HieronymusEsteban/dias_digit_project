{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e1692-ff1e-4467-a48b-576e6f79c7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610b8e5-9321-44f6-a489-d620063dc0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "165/6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c115cad-f208-4715-bbcd-b3f07179ec16",
   "metadata": {},
   "source": [
    "#### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883f168-eda8-4513-a2c3-4cf8ba41d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "4+1+6+1+5+1+1+1+1+3+4+7+3+3+1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95838a38-44ec-4260-8572-8956a7ecadca",
   "metadata": {},
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0893327-015f-4832-ba98-b5c775ab97df",
   "metadata": {},
   "outputs": [],
   "source": [
    "4+6+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e539e3-0fb5-4b1b-b988-4a3a4043ffa1",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9a130-661a-4947-9a43-113cc384d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+2+8+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afc339-4065-4cb4-9881-bd1438292784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", dtype=\"auto\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9f0d1-de20-476a-8c45-7b14e9b8f806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc77909c-abfb-4aa0-8dbb-98f786294000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc95bd8-4bac-4176-bbad-cafe15ced7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e255f3-26e7-494a-9d7d-4c8f15b70e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b760da-2fb2-42fa-a1c2-557614786b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a4cb2-368c-40dc-9d44-4523240418df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6312b6-a1f9-4883-82e4-9ebe753c3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "project_path = Path.cwd()\n",
    "root_path = (project_path / '..').resolve()\n",
    "\n",
    "# Define paths\n",
    "image_dir = root_path/'visual_genome_proc_data'  # Replace with your directory containing images\n",
    "image_dir_2 = root_path/'data_1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9f5b4-5cc4-4b3c-aa80-4445c708c8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c98a7ca-7466-4578-b4c3-ece4c953188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892335b0-4b67-4ea1-9f79-8fcb4f904063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_file_path = image_dir / image_files[0]\n",
    "image_file_path = image_dir / 'visual_genome_proc_2356444.jpg'\n",
    "image_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52bec8f-7338-4af2-b503-8e0947275564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained('openbmb/MiniCPM-V-2', trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "# For Nvidia GPUs support BF16 (like A100, H100, RTX3090)\n",
    "model = model.to(device='cuda', dtype=torch.bfloat16)\n",
    "# For Nvidia GPUs do NOT support BF16 (like V100, T4, RTX2080)\n",
    "#model = model.to(device='cuda', dtype=torch.float16)\n",
    "# For Mac with MPS (Apple silicon or AMD GPUs).\n",
    "# Run with `PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py`\n",
    "#model = model.to(device='mps', dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2', trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "image = Image.open(image_file_path).convert('RGB')\n",
    "question = 'What is in the image?'\n",
    "msgs = [{'role': 'user', 'content': question}]\n",
    "\n",
    "res, context, _ = model.chat(\n",
    "    image=image,\n",
    "    msgs=msgs,\n",
    "    context=None,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab635a-90b2-4b49-af9d-604974b62114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "model = AutoModel.from_pretrained('openbmb/MiniCPM-V', trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#model = AutoModelForCausalLM.from_pretrained('openbmb/MiniCPM-V', trust_remote_code=True, dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "# For Nvidia GPUs support BF16 (like A100, H100, RTX3090)\n",
    "#model = model.to(device='cuda', dtype=torch.bfloat16)\n",
    "# For Nvidia GPUs do NOT support BF16 (like V100, T4, RTX2080)\n",
    "#model = model.to(device='cuda', dtype=torch.float16)\n",
    "# For Mac with MPS (Apple silicon or AMD GPUs).\n",
    "# Run with `PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py`\n",
    "model = model.to(device='mps', dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V', trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "image = Image.open(image_file_path).convert('RGB')\n",
    "question = 'What is in the image?'\n",
    "msgs = [{'role': 'user', 'content': question}]\n",
    "\n",
    "res, context, _ = model.chat(\n",
    "    image=image,\n",
    "    msgs=msgs,\n",
    "    context=None,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ddea1-03d7-4fee-ba2c-af38f0250329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer, GenerationMixin\n",
    "\n",
    "# Load your model normally\n",
    "model = AutoModel.from_pretrained('openbmb/MiniCPM-V', trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "model = model.to(device='mps', dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V', trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "# Add the generate method from GenerationMixin\n",
    "model.generate = GenerationMixin.generate.__get__(model, model.__class__)\n",
    "\n",
    "# Now your model should have the generate method\n",
    "image = Image.open(image_file_path).convert('RGB')\n",
    "question = 'What is in the image?'\n",
    "msgs = [{'role': 'user', 'content': question}]\n",
    "\n",
    "res, context, _ = model.chat(\n",
    "    image=image,\n",
    "    msgs=msgs,\n",
    "    context=None,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a82fed2-ce13-4909-bec3-492dfef63e73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
