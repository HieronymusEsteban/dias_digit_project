{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de50bf5-56eb-40d7-8016-22959f1f1264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from source import image_id_converter as img_idc\n",
    "from source import sort_img_files as sif\n",
    "from source import llm_input as llm_i\n",
    "from source import llm_output as llm_o\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d38edd-bef0-48c4-84c5-8c89603d07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd0d8e-bb05-411b-9684-8e0edd348996",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a6ec2-42c9-4f70-ae27-334220fea2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89db57a-663c-4cfc-8438-f5723c0170f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_minicpm_model(image_path, prompt_function):\n",
    "    \"\"\"\n",
    "    Call MiniCPM-V model for image analysis.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        prompt (str): Text prompt for the model\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response from the model\n",
    "    \"\"\"\n",
    "\n",
    "    print(prompt_function())\n",
    "    prompt = prompt_function()\n",
    "\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(100)\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model = AutoModel.from_pretrained(\n",
    "        'openbmb/MiniCPM-V-4_5', \n",
    "        trust_remote_code=True,\n",
    "        attn_implementation='sdpa', \n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = model.eval().cuda()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'openbmb/MiniCPM-V-4_5', \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Configure generation settings\n",
    "    enable_thinking = False\n",
    "    stream = True\n",
    "    \n",
    "    # Create conversation message\n",
    "    msgs = [{'role': 'user', 'content': [image, prompt]}]\n",
    "    \n",
    "    # Generate response\n",
    "    answer = model.chat(\n",
    "        msgs=msgs,\n",
    "        tokenizer=tokenizer,\n",
    "        enable_thinking=enable_thinking,\n",
    "        stream=stream\n",
    "    )\n",
    "    \n",
    "    # Collect streamed response\n",
    "    generated_text = \"\"\n",
    "    for new_text in answer:\n",
    "        generated_text += new_text\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14a5316f-d4b4-4fe9-bb32-6f45e7810a69",
   "metadata": {},
   "source": [
    "def call_nanollava_model(image_path: str, prompt_function) -> str:\n",
    "    \"\"\"\n",
    "    Call nanoLLaVA model for image analysis.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        prompt (str): Text prompt for the model\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response from the model\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prompt_function()\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def fixed_get_imports(filename: str | os.PathLike) -> list[str]:\n",
    "        \"\"\"Work around for flash_attn compatibility issues.\"\"\"\n",
    "        imports = get_imports(filename)\n",
    "        if not torch.cuda.is_available() and \"flash_attn\" in imports:\n",
    "            imports.remove(\"flash_attn\")\n",
    "        return imports\n",
    "    \n",
    "    model_name = 'qnguyen3/nanoLLaVA-1.5'\n",
    "    \n",
    "    # Load model and tokenizer with workaround\n",
    "    with patch(\"transformers.dynamic_module_utils.get_imports\", fixed_get_imports):\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map='auto' if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    \n",
    "    # Prepare conversation messages\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f'<image>\\n{prompt}'}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Process text chunks\n",
    "    text_chunks = [tokenizer(chunk).input_ids for chunk in text.split('<image>')]\n",
    "    input_ids = torch.tensor(\n",
    "        text_chunks[0] + [-200] + text_chunks[1], \n",
    "        dtype=torch.long\n",
    "    ).unsqueeze(0)\n",
    "    \n",
    "    # Load and process image\n",
    "    image = Image.open(image_path)\n",
    "    image_tensor = model.process_images([image], model.config).to(dtype=model.dtype)\n",
    "    \n",
    "    # Generate response\n",
    "    output_ids = model.generate(\n",
    "        input_ids.to(device),\n",
    "        images=image_tensor.to(device),\n",
    "        max_new_tokens=2048,\n",
    "        use_cache=True\n",
    "    )[0]\n",
    "    \n",
    "    # Decode response\n",
    "    generated_text = tokenizer.decode(\n",
    "        output_ids[input_ids.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621bc1ba-2883-4a54-b539-5c21b1d57c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a770f-9f53-4d26-8777-c2ef2df4a229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ba210-1f3d-4296-a15d-c8307a8d9aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7bea40-b27c-431f-84d2-40c986b376eb",
   "metadata": {},
   "source": [
    "# Using LLM (mini-CPM) for image analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8419f-b468-417d-963b-e80799c4f034",
   "metadata": {},
   "source": [
    "### Define Functions:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1d4698c-b477-40f7-8ce8-d35b0f8bd776",
   "metadata": {},
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "def convert_tif_to_jpg(source_folder, destination_folder, quality=85):\n",
    "    \"\"\"\n",
    "    Convert .tif files to .jpg format and move copies to destination folder.\n",
    "    Original .tif files remain in source folder.\n",
    "    \n",
    "    Args:\n",
    "        source_folder (str): Path to folder containing .tif files\n",
    "        destination_folder (str): Path to destination folder for .jpg files\n",
    "        quality (int): JPEG quality (1-100, default 85)\n",
    "    \"\"\"\n",
    "    # Create destination folder if it doesn't exist\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    \n",
    "    converted_files = []\n",
    "    \n",
    "    # Process all .tif files in source folder\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.lower().endswith(('.tif', '.tiff')):\n",
    "            source_path = os.path.join(source_folder, filename)\n",
    "            \n",
    "            # Create new filename with .jpg extension\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            jpg_filename = f\"{base_name}.jpg\"\n",
    "            destination_path = os.path.join(destination_folder, jpg_filename)\n",
    "            \n",
    "            try:\n",
    "                # Open and convert image\n",
    "                with Image.open(source_path) as img:\n",
    "                    # Convert to RGB if necessary (TIFF might be in different modes)\n",
    "                    if img.mode != 'RGB':\n",
    "                        img = img.convert('RGB')\n",
    "                    \n",
    "                    # Save as JPEG in destination folder\n",
    "                    img.save(destination_path, 'JPEG', quality=quality, optimize=True)\n",
    "                \n",
    "                converted_files.append(jpg_filename)\n",
    "                print(f\"Converted: {filename} -> {jpg_filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {filename}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Successfully converted {len(converted_files)} files\")\n",
    "    return converted_files\n",
    "\n",
    "# Example usage:\n",
    "# convert_tif_to_jpg(\"/path/to/source\", \"/path/to/destination\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82caef2e-96f2-4043-a703-c87d0866858e",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_image_if_needed(image_path):\n",
    "    \"\"\"Convert TIFF (and other unsupported formats) to JPG.\"\"\"\n",
    "    path = Path(image_path)\n",
    "    \n",
    "    if path.suffix.lower() in ['.tif', '.tiff']:\n",
    "        try:\n",
    "            img = Image.open(path)\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Build new path manually\n",
    "            jpg_path = path.parent / f\"{path.stem}_converted.jpg\"\n",
    "            \n",
    "            img.save(jpg_path, 'JPEG', quality=95)\n",
    "            print(f\"Converted {path} to {jpg_path}\")\n",
    "            return str(jpg_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        return str(path)\n",
    "\n",
    "\n",
    "def call_ollama_model(image_path, prompt):\n",
    "    \"\"\"Make the API call to Ollama.\"\"\"\n",
    "    # Convert image if needed\n",
    "    processed_path = convert_image_if_needed(image_path)\n",
    "    if processed_path is None:\n",
    "        raise ValueError(f\"Could not process image: {image_path}\")\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        #model=\"minicpm-v\",\n",
    "        model=\"llama3.2-vision:90b\",\n",
    "        messages=[{\n",
    "            'role': 'user', \n",
    "            'content': prompt,\n",
    "            'images': [processed_path]\n",
    "        }],\n",
    "        options={\n",
    "        'temperature': 0.1,  # Lower = more deterministic (0.0 to 1.0)\n",
    "        'seed': 42           # Fixed seed for reproducibility\n",
    "    }\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578097bb-67ae-4f4b-b4b4-6e50b9c3ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc124a90-944e-41e7-a530-b3fa50bf9010",
   "metadata": {},
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "def parse_response_to_dict(response_text):\n",
    "    \"\"\"Parse the model response into a Python dictionary.\"\"\"\n",
    "    try:\n",
    "        # First try to find dictionary in code blocks\n",
    "        code_block_match = re.search(r'```(?:python)?\\s*(\\{.*?\\})\\s*```', response_text, re.DOTALL)\n",
    "        if code_block_match:\n",
    "            dict_str = code_block_match.group(1)\n",
    "        else:\n",
    "            # Fallback to finding any dictionary pattern\n",
    "            dict_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if dict_match:\n",
    "                dict_str = dict_match.group()\n",
    "            else:\n",
    "                return False, None\n",
    "        \n",
    "        # Clean up the dictionary string\n",
    "        dict_str = dict_str.replace('\\\\_', '_')\n",
    "        dict_str = dict_str.strip()\n",
    "        \n",
    "        # Parse the dictionary\n",
    "        result_dict = ast.literal_eval(dict_str)\n",
    "        return True, result_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60ffe502-d2e9-4816-991a-b69ff95f0d58",
   "metadata": {},
   "source": [
    "def analyze_image_structured(image_path):\n",
    "    \"\"\"Main function that orchestrates the image analysis.\"\"\"\n",
    "    # Define prompt for LLM model:\n",
    "    prompt = create_analysis_prompt()\n",
    "    # Ask LLM to analyse image, by calling the model and providing \n",
    "    # the defined prompt: \n",
    "    response_text = call_ollama_model(image_path, prompt)\n",
    "    # Parse response text, i.e. find dictionary of expected structure\n",
    "    # in the response text:\n",
    "    success, result_dict = parse_response_to_dict(response_text)\n",
    "    \n",
    "    if success:\n",
    "        return result_dict\n",
    "    else:\n",
    "        # Save response text in dictionary paired with key \"raw_response\"\n",
    "        # if parsing the response text fails:\n",
    "        llm_response = {\"raw_response\": response_text}\n",
    "        return llm_response"
   ]
  },
  {
   "cell_type": "raw",
   "id": "071cac96-54b5-4f7c-b911-ebe6fdf86783",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def first_word_is_yes_or_no(text):\n",
    "    \"\"\"\n",
    "    Check if the first word of a text is 'yes' or 'no' (case-insensitive).\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to check\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (bool, str or None) - (True if first word is 'yes' or 'no', the cleaned first word or None)\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return False, None\n",
    "    \n",
    "    first_token = text.strip().split()[0].lower()\n",
    "    \n",
    "    # Check if it starts with 'yes' or 'no' followed by punctuation\n",
    "    if re.match(r'^(yes|no)[.,!?;:\"()[\\]{}]', first_token):\n",
    "        answer = re.match(r'^(yes|no)', first_token).group(1)\n",
    "        return True, answer\n",
    "    \n",
    "    # Fallback to the original method for clean words\n",
    "    cleaned_word = re.sub(r'[^a-zA-Z]', '', first_token)\n",
    "    is_yes_or_no = cleaned_word in ['yes', 'no']\n",
    "    return is_yes_or_no, cleaned_word if is_yes_or_no else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a3e8846-c795-4fe5-b05d-305ccf520cc2",
   "metadata": {},
   "source": [
    "def parse_yes_no_text(text):\n",
    "    \"\"\"\n",
    "    Parse text that may start with 'yes' or 'no' and return structured data.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to parse\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (bool, dict or None) - Success status and result dictionary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not text or not text.strip():\n",
    "            return False, None\n",
    "        \n",
    "        text = text.strip()\n",
    "        is_yes_or_no, first_word = first_word_is_yes_or_no(text)\n",
    "        \n",
    "        if is_yes_or_no:\n",
    "            return True, {'answer': first_word, 'additional_comments': text}\n",
    "        else:\n",
    "            return False, None\n",
    "    except Exception as e:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc4d62-f8e0-4ce9-9bf1-1446c7aecb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_o.parse_yes_no_text(\"yes,e, no, ay\"))      # Should be None (correctly)\n",
    "print(llm_o.parse_yes_no_text(\"yes, I agree\"))       # Should be 'yes' \n",
    "print(llm_o.parse_yes_no_text(\"yes!\"))              # Should be 'yes'\n",
    "print(llm_o.parse_yes_no_text(\"YES,\"))              # Should be 'yes'\n",
    "# Example usage:\n",
    "print(llm_o.parse_yes_no_text(\"Yes, I completely agree with this\"))\n",
    "# {'answer': 'yes', 'additional_comments': 'Yes, I completely agree with this'}\n",
    "\n",
    "print(llm_o.parse_yes_no_text(\"No problem at all\"))\n",
    "# {'answer': 'no', 'additional_comments': 'No problem at all'}\n",
    "\n",
    "print(llm_o.parse_yes_no_text(\"Maybe we should consider this\"))\n",
    "# {'answer': None, 'additional_comments': 'Maybe we should consider this'}\n",
    "\n",
    "print(llm_o.parse_yes_no_text(\"YES!\"))\n",
    "# {'answer': 'yes', 'additional_comments': 'YES!'}\n",
    "\n",
    "print(llm_o.parse_yes_no_text(\"\"))\n",
    "# {'answer': None, 'additional_comments': ''}\n",
    "\n",
    "print(llm_o.parse_yes_no_text(\",yesjaja\"))\n",
    "# {'answer': True, 'additional_comments': ',yesjaja'}\n",
    "\n",
    "print(llm_o.parse_yes_no_text(\" no. jaja\"))\n",
    "\n",
    "print(llm_o.parse_yes_no_text(\" kaki no, ay\"))\n",
    "\n",
    "print(llm_o.parse_yes_no_text(\"yes,e, no, ay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb471d-bda2-42b5-aec1-b834f0a50314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "094594d4-d80c-4c02-a998-3072caffb1c0",
   "metadata": {},
   "source": [
    "def analyze_image_yes_no(image_path, create_analysis_prompt, model_function):\n",
    "    \"\"\"Main function that orchestrates the image analysis.\"\"\"\n",
    "    # Define prompt for LLM model:\n",
    "    #prompt = create_analysis_prompt()\n",
    "    #print(prompt)\n",
    "    # Ask LLM to analyse image, by calling the model and providing \n",
    "    # the defined prompt: \n",
    "    response_text = model_function(image_path, create_analysis_prompt)\n",
    "    # Parse response text:\n",
    "    response_text\n",
    "    print(response_text)\n",
    "    success, result_dict = parse_yes_no_text(response_text)\n",
    "    \n",
    "    if success:\n",
    "        return result_dict\n",
    "    else:\n",
    "        # Save response text in dictionary paired with key \"raw_response\"\n",
    "        # if parsing the response text fails:\n",
    "        llm_response = {\"raw_response\": response_text}\n",
    "        return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce828df-9d7c-4849-9609-84ecda24da4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ddbd0a53-9567-4ac7-9501-828cced50573",
   "metadata": {},
   "source": [
    "def analyse_giub_img_dir_llm_yn_text(jpg_data_path, create_analysis_prompt, model_function):\n",
    "    # Get time stamp:\n",
    "    timestamp_start_is_photo_analysis = pd.Timestamp.now()\n",
    "    \n",
    "    # Get list of image files to analyse: \n",
    "    image_files = os.listdir(jpg_data_path)\n",
    "    img_ids = [image_file.split('Oberland')[1].split('.')[0] for image_file in image_files]\n",
    "    \n",
    "    # Make empty dictionary to store results:\n",
    "    image_descr = {}\n",
    "    \n",
    "    # Loop through images to get answers: \n",
    "    for image_file in image_files:\n",
    "        image_path = jpg_data_path / image_file\n",
    "        path_str = str(image_path)\n",
    "        #print('\\n')\n",
    "        #print(path_str)\n",
    "        parts = path_str.split('.jpg')\n",
    "        img_id = parts[-2][-3:]\n",
    "    \n",
    "        # Analyse image, get values for each of the categorical variables specified above:\n",
    "        #image_description = analyze_image_structured(image_path)\n",
    "        #image_description = llm_o.analyze_image_structured(image_path, create_analysis_prompt)\n",
    "        #image_description = llm_o.analyze_image_structured(image_path, create_analysis_prompt, model_function)\n",
    "        image_description = llm_o.analyze_image_yes_no(image_path, create_analysis_prompt, model_function)\n",
    "        \n",
    "        dict_type_bool = type(image_description) == dict\n",
    "            \n",
    "        #print(image_description)\n",
    "        image_descr[img_id] = image_description\n",
    "    \n",
    "    timestamp_end_is_photo_analysis = pd.Timestamp.now()\n",
    "\n",
    "    return timestamp_start_is_photo_analysis, timestamp_end_is_photo_analysis, image_descr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1cdf43-cb77-44c8-97d3-1b402679ff7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0dde51-cabc-4fee-a18c-3d08184284d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9c20f-792f-4e58-907c-92fd7a496a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad699b59-6d63-4896-8367-7cab5cd4a53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9fb3f1-c6af-4fda-8ad7-a271bc19c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '  yes kekeeaÃ© lei'\n",
    "first_word = text.strip().split()[0].lower()\n",
    "first_word\n",
    "first_word in ['yes', 'no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a514206e-71e8-450a-a637-238b6cd6dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c57f3-8585-43d0-a1e6-e0d343cdef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pred_values(idx, labels_results, columns, values_to_add):\n",
    "    selection_bools = labels_results.image_id == inspection_idx\n",
    "    \n",
    "    labels_results.loc[selection_bools, columns] = values_to_add"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88d1e288-dbe9-41cd-aa6d-8322578f575a",
   "metadata": {},
   "source": [
    "def extract_vals_from_response_dict(img_ids, image_descr, keys_list_expected, \n",
    "                                    response_variable): \n",
    "   #img_type = []\n",
    "   response_values = []\n",
    "   #with_person = []\n",
    "   #with_church = []\n",
    "   \n",
    "   # Make empty list to store responses that cannot be parsed\n",
    "   # due to faulty structure for closer inspection: \n",
    "   img_ids_closer_inspection = []\n",
    "   \n",
    "   iter_count = 0\n",
    "   \n",
    "   # Loop through image ids:\n",
    "   for img_id in img_ids:\n",
    "   \n",
    "       # Get response from LLM for image id in question:\n",
    "       img_pred = image_descr[img_id]\n",
    "   \n",
    "       # Get keys from response dictionary:\n",
    "       keys_list = list(img_pred.keys())\n",
    "   \n",
    "       # Check if structure and keys of response match expectation:\n",
    "       dict_struct_condition = (type(img_pred) == dict)\n",
    "       keys_condition = (keys_list_expected == keys_list)\n",
    "   \n",
    "       # Check if response key \n",
    "       raw_key_condition = keys_list == ['raw_response']\n",
    "       \n",
    "       # If the llm response corresponds to the expected\n",
    "       # structure, get response values as planned:\n",
    "       if dict_struct_condition and keys_condition:\n",
    "           \n",
    "           bool_value = img_pred[response_variable]\n",
    "   \n",
    "           if bool_value:\n",
    "               int_value = int(1)\n",
    "           else:\n",
    "               int_value = int(0)\n",
    "   \n",
    "           response_values.append(int_value)\n",
    "           \n",
    "       # If llm response does not correspond to the expected \n",
    "       # structure but does have the 'raw_response' key\n",
    "       # try to identify a dictionary inside the response text\n",
    "       # and try to parse this dictionary as planned:\n",
    "       elif dict_struct_condition and raw_key_condition:\n",
    "           print('\\n')\n",
    "           print('raw_repsonse_dict:')\n",
    "           print(img_id)\n",
    "           print(dict_struct_condition)\n",
    "           print(raw_key_condition)\n",
    "   \n",
    "           response_text = img_pred['raw_response']\n",
    "   \n",
    "           start_indices = [i for i, char in enumerate(response_text) if char == '{']\n",
    "           start_idx = start_indices[0]\n",
    "           \n",
    "           end_indices = [i for i, char in enumerate(response_text) if char == '}']\n",
    "           end_idx = end_indices[0]\n",
    "   \n",
    "           dict_in_text = response_text[start_idx:end_idx+1]\n",
    "   \n",
    "           success_bool, img_pred = parse_response_to_dict(dict_in_text)\n",
    "           print('success_bool:')\n",
    "           print(success_bool)\n",
    "   \n",
    "           # If a dictionary is found and parsed successfully\n",
    "           # get response values as planned:\n",
    "           if success_bool:\n",
    "               print(type(img_pred))\n",
    "               print(img_pred.keys())\n",
    "               \n",
    "               bool_value = img_pred[response_variable]\n",
    "       \n",
    "               if bool_value:\n",
    "                   int_value = int(1)\n",
    "               else:\n",
    "                   int_value = int(0)\n",
    "               \n",
    "               response_values.append(int_value)\n",
    "               \n",
    "           else:\n",
    "               # If dictionary is not found or not successfully\n",
    "               # parsed, add the image in question to the list\n",
    "               # of images for closer (visual) inspection:\n",
    "               print('parse unsuccessful')\n",
    "               print(img_id)\n",
    "               img_ids_closer_inspection.append(img_id)\n",
    "               response_values.append(None)\n",
    "   \n",
    "       # If the llm response does not have the expected struture\n",
    "       # and no 'raw_response' key is found, add the image in \n",
    "       # question to the list of images for closer (visual)\n",
    "       # inspection:\n",
    "       else:\n",
    "           print('\\n')\n",
    "           print('no structure at all:')\n",
    "           print(img_id)\n",
    "           img_ids_closer_inspection.append(img_id)\n",
    "           response_values.append(None)\n",
    "           \n",
    "       \n",
    "       iter_count += 1\n",
    "   return img_ids, response_values, img_ids_closer_inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa1728-4b12-4428-bb9b-802057ea26ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vals_from_yes_no_response(img_ids, image_descr, keys_list_expected, \n",
    "                                    response_variable): \n",
    "   #img_type = []\n",
    "   response_values = []\n",
    "   #with_person = []\n",
    "   #with_church = []\n",
    "   \n",
    "   # Make empty list to store responses that cannot be parsed\n",
    "   # due to faulty structure for closer inspection: \n",
    "   img_ids_closer_inspection = []\n",
    "   \n",
    "   iter_count = 0\n",
    "   \n",
    "   # Loop through image ids:\n",
    "   for img_id in img_ids:\n",
    "   \n",
    "       # Get response from LLM for image id in question:\n",
    "       img_pred = image_descr[img_id]\n",
    "   \n",
    "       # Get keys from response dictionary:\n",
    "       keys_list = list(img_pred.keys())\n",
    "   \n",
    "       # Check if structure and keys of response match expectation:\n",
    "       dict_struct_condition = (type(img_pred) == dict)\n",
    "       keys_condition = (keys_list_expected == keys_list)\n",
    "   \n",
    "       # Check if response key \n",
    "       raw_key_condition = keys_list == ['raw_response']\n",
    "       \n",
    "       # If the llm response corresponds to the expected\n",
    "       # structure, get response values as planned:\n",
    "       if dict_struct_condition and keys_condition:\n",
    "           \n",
    "           response_val = img_pred[response_variable]\n",
    "   \n",
    "           if response_val == 'yes':\n",
    "               int_value = int(1)\n",
    "           else:\n",
    "               int_value = int(0)\n",
    "   \n",
    "           response_values.append(int_value)\n",
    "           \n",
    "       else:\n",
    "           print('\\n')\n",
    "           print('not expected structure:')\n",
    "           print(img_id)\n",
    "           img_ids_closer_inspection.append(img_id)\n",
    "           response_values.append(None)\n",
    "           \n",
    "       \n",
    "       iter_count += 1\n",
    "   return img_ids, response_values, img_ids_closer_inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e062740-b8c2-487e-8c38-3e873b9c787c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf63d5-4e16-42be-9921-6dadc4dc49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More comprehensive check including empty strings and whitespace\n",
    "def has_missing_comprehensive(df):\n",
    "   # Standard missing values\n",
    "   has_standard_missing = df.isnull().any().any()\n",
    "   \n",
    "   # Empty strings and whitespace-only strings\n",
    "   has_empty_strings = False\n",
    "   for col in df.select_dtypes(include=['object']):\n",
    "       if (df[col].astype(str).str.strip() == '').any():\n",
    "           has_empty_strings = True\n",
    "           break\n",
    "   \n",
    "   return has_standard_missing or has_empty_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb7c4d-86da-42d7-aac2-f2b9d870ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_subsets_metrics(labels_results, var_name, pred_var_name):\n",
    "    positive_bools = labels_results[var_name] == 1\n",
    "    negative_bools = labels_results[var_name] == 0\n",
    "    positive_pred_bools = labels_results[pred_var_name] == 1\n",
    "    negative_pred_bools = labels_results[pred_var_name] == 0\n",
    "    \n",
    "    positives = labels_results[positive_bools]\n",
    "    negatives = labels_results[negative_bools]\n",
    "    true_positives = labels_results[positive_bools & positive_pred_bools]\n",
    "    true_negatives = labels_results[negative_bools & negative_pred_bools]\n",
    "    \n",
    "    false_negatives = labels_results[positive_bools & negative_pred_bools]\n",
    "    false_positives = labels_results[negative_bools & positive_pred_bools]\n",
    "\n",
    "    sensitivity = true_positives.shape[0] / positives.shape[0]\n",
    "    print('sensitivity:')\n",
    "    print(sensitivity)\n",
    "    \n",
    "    specificity = true_negatives.shape[0] / negatives.shape[0]\n",
    "    print('specificity:')\n",
    "    print(specificity)\n",
    "\n",
    "    subsets_and_metrics = (positives, negatives, true_positives, true_negatives, \n",
    "                           false_negatives, false_positives, sensitivity, specificity)\n",
    "    \n",
    "    return subsets_and_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9006398-1f75-4e18-95ba-67d0f39f3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(labels_results, label, prediction, cases):\n",
    "    true_positives, false_positives, true_negatives, false_negatives, positives, negatives = cases\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(labels_results[label], labels_results[prediction])\n",
    "    \n",
    "    number_true_positives = true_positives.shape[0]\n",
    "    number_false_positives = false_positives.shape[0]\n",
    "    number_true_negatives = true_negatives.shape[0]\n",
    "    number_false_negatives = false_negatives.shape[0]\n",
    "    \n",
    "    sensitivity = number_true_positives / positives.shape[0]\n",
    "    specificity = number_true_negatives / negatives.shape[0]\n",
    "    if (number_true_positives + number_false_positives) > 0:\n",
    "        precision = number_true_positives / (number_true_positives + number_false_positives)\n",
    "        f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
    "    else:\n",
    "        precision = None\n",
    "        f1_score = None\n",
    "    if positives.shape[0] > 0:\n",
    "        miss_rate = number_false_negatives / positives.shape[0]\n",
    "    else:\n",
    "        miss_rate = None\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    confusion_matrix_data = [[number_true_negatives, number_false_positives], \n",
    "                              [number_false_negatives, number_true_positives]]\n",
    "    sns.heatmap(confusion_matrix_data, annot=True, fmt='d', \n",
    "                xticklabels=['Predicted Negative', 'Predicted Positive'], \n",
    "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'True Positives: {number_true_positives}')\n",
    "    print(f'False Positives: {number_false_positives}')\n",
    "    print(f'True Negatives: {number_true_negatives}')\n",
    "    print(f'False Negatives: {number_false_negatives}')\n",
    "    print(f'\\nSensitivity (Recall): {sensitivity:.4f}')\n",
    "    print(f'Specificity: {specificity:.4f}')\n",
    "    if precision is not None:\n",
    "        print(f'Precision: {precision:.4f}')\n",
    "        print(f'Miss Rate (False Negative Rate): {miss_rate:.4f}')\n",
    "        print(f'F1 Score: {f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20297625-08fa-4b95-be19-5b4bb505e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conf_matrix(labels_results, label, prediction, cases):\n",
    "    true_positives, false_positives, true_negatives, false_negatives, positives, negatives = cases\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(labels_results[label], labels_results[prediction])\n",
    "    \n",
    "    number_true_positives = true_positives.shape[0]\n",
    "    number_false_positives = false_positives.shape[0]\n",
    "    number_true_negatives = true_negatives.shape[0]\n",
    "    number_false_negatives = false_negatives.shape[0]\n",
    "    \n",
    "    sensitivity = number_true_positives / positives.shape[0]\n",
    "    specificity = number_true_negatives / negatives.shape[0]\n",
    "    precision = number_true_positives / (number_true_positives + number_false_positives)\n",
    "    miss_rate = number_false_negatives / positives.shape[0]\n",
    "    f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
    "    \n",
    "    plt.figure(figsize=(15,8))\n",
    "    gs = plt.GridSpec(1, 2, width_ratios=[2, 1])\n",
    "    \n",
    "    plt.subplot(gs[0])\n",
    "    confusion_matrix_data = [[number_true_negatives, number_false_positives], \n",
    "                             [number_false_negatives, number_true_positives]]\n",
    "    heatmap = sns.heatmap(confusion_matrix_data, annot=True, fmt='d', \n",
    "               xticklabels=['Predicted Negative', 'Predicted Positive'], \n",
    "               yticklabels=['Actual Negative', 'Actual Positive'],\n",
    "               cbar_kws={'label': 'Number of Instances'})\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    plt.subplot(gs[1])\n",
    "    plt.axis('off')\n",
    "    metrics_text = (f'Performance Metrics:\\n\\n'\n",
    "                   f'True Positives: {number_true_positives}\\n'\n",
    "                   f'False Positives: {number_false_positives}\\n'\n",
    "                   f'True Negatives: {number_true_negatives}\\n'\n",
    "                   f'False Negatives: {number_false_negatives}\\n\\n'\n",
    "                   f'Sensitivity: {sensitivity:.4f}\\n'\n",
    "                   f'Specificity: {specificity:.4f}\\n'\n",
    "                   f'Precision: {precision:.4f}\\n'\n",
    "                   f'Miss Rate: {miss_rate:.4f}\\n'\n",
    "                   f'F1 Score: {f1_score:.4f}')\n",
    "    plt.text(0, 0.5, metrics_text, fontsize=10, \n",
    "            verticalalignment='center')\n",
    "    \n",
    "    plt.suptitle('Photography Detection: Confusion Matrix and Performance Metrics Based on is_photo Label as Ground Truth', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    output_path = data_path / 'conf_matrix_metrics.pdf'\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc1130-c8c9-41c9-a4a4-a24d09a13753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conf_matrix_tag(labels_results, label, prediction, cases, filename_tag):\n",
    "    true_positives, false_positives, true_negatives, false_negatives, positives, negatives = cases\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(labels_results[label], labels_results[prediction])\n",
    "    \n",
    "    number_true_positives = true_positives.shape[0]\n",
    "    number_false_positives = false_positives.shape[0]\n",
    "    number_true_negatives = true_negatives.shape[0]\n",
    "    number_false_negatives = false_negatives.shape[0]\n",
    "    \n",
    "    sensitivity = number_true_positives / positives.shape[0]\n",
    "    specificity = number_true_negatives / negatives.shape[0]\n",
    "    \n",
    "    if (number_true_positives + number_false_positives) > 0:\n",
    "        precision = number_true_positives / (number_true_positives + number_false_positives)\n",
    "        f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
    "    else:\n",
    "        precision = None\n",
    "        f1_score = None\n",
    "        \n",
    "    if positives.shape[0] > 0:\n",
    "        miss_rate = number_false_negatives / positives.shape[0]\n",
    "    else:\n",
    "        miss_rate = None\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15,8))\n",
    "    gs = plt.GridSpec(1, 2, width_ratios=[2, 1])\n",
    "    \n",
    "    plt.subplot(gs[0])\n",
    "    confusion_matrix_data = [[number_true_negatives, number_false_positives], \n",
    "                             [number_false_negatives, number_true_positives]]\n",
    "    heatmap = sns.heatmap(confusion_matrix_data, annot=True, fmt='d', \n",
    "               xticklabels=['Predicted Negative', 'Predicted Positive'], \n",
    "               yticklabels=['Actual Negative', 'Actual Positive'],\n",
    "               cbar_kws={'label': 'Number of Instances'})\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    plt.subplot(gs[1])\n",
    "    plt.axis('off')\n",
    "    if precision is not None:\n",
    "        metrics_text = (f'Performance Metrics:\\n\\n'\n",
    "                       f'True Positives: {number_true_positives}\\n'\n",
    "                       f'False Positives: {number_false_positives}\\n'\n",
    "                       f'True Negatives: {number_true_negatives}\\n'\n",
    "                       f'False Negatives: {number_false_negatives}\\n\\n'\n",
    "                       f'Sensitivity: {sensitivity:.4f}\\n'\n",
    "                       f'Specificity: {specificity:.4f}\\n'\n",
    "                       f'Precision: {precision:.4f}\\n'\n",
    "                       f'Miss Rate: {miss_rate:.4f}\\n'\n",
    "                       f'F1 Score: {f1_score:.4f}')\n",
    "    else:\n",
    "        metrics_text = (f'Performance Metrics:\\n\\n'\n",
    "                       f'True Positives: {number_true_positives}\\n'\n",
    "                       f'False Positives: {number_false_positives}\\n'\n",
    "                       f'True Negatives: {number_true_negatives}\\n'\n",
    "                       f'False Negatives: {number_false_negatives}\\n\\n'\n",
    "                       f'Sensitivity: {sensitivity:.4f}\\n'\n",
    "                       f'Specificity: {specificity:.4f}\\n')\n",
    "        \n",
    "    plt.text(0, 0.5, metrics_text, fontsize=10, \n",
    "            verticalalignment='center')\n",
    "    \n",
    "    plt.suptitle('Photography Detection: Confusion Matrix and Performance Metrics Based on is_photo Label as Ground Truth', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    filename = 'conf_matrix_metrics_' + filename_tag + '.pdf'\n",
    "    output_path = data_path / filename\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "783f9949-382a-4636-b05a-9844446141f3",
   "metadata": {},
   "source": [
    "def store_duration(time_analysis_dict, time_analysis_for_df_dict, analysis_name, duration):\n",
    "    time_analysis_dict[analysis_name] = {}\n",
    "    time_analysis_dict[analysis_name]['duration_str'] = f\"Analysis took: {duration}\"\n",
    "    time_analysis_dict[analysis_name]['duration_seconds'] = total_seconds\n",
    "    time_analysis_dict[analysis_name]['duration_seconds_str'] = f\"Analysis took: {total_seconds:.2f} seconds\"\n",
    "    time_analysis_dict[analysis_name]['duration_minutes'] = total_seconds/60\n",
    "    time_analysis_dict[analysis_name]['duration_minutes_str'] = f\"Analysis took: {total_seconds/60:.2f} minutes\"\n",
    "    time_analysis_dict[analysis_name]['time_stamp_start'] = timestamp_start_is_photo_analysis\n",
    "    time_analysis_dict[analysis_name]['time_stamp_end'] = timestamp_end_is_photo_analysis\n",
    "\n",
    "    time_analysis_for_df_dict['analysis_name'].append(analysis_name)\n",
    "    time_analysis_for_df_dict['duration_str'].append(f\"Analysis took: {duration}\")\n",
    "    time_analysis_for_df_dict['duration_seconds'].append(total_seconds)\n",
    "    time_analysis_for_df_dict['duration_seconds_str'].append(f\"Analysis took: {total_seconds:.2f} seconds\")\n",
    "    time_analysis_for_df_dict['duration_minutes'].append(total_seconds/60)\n",
    "    time_analysis_for_df_dict['duration_minutes_str'].append(f\"Analysis took: {total_seconds/60:.2f} minutes\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f96db-560b-4442-8adc-6a3dd069dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_duration(time_analysis_dict, time_analysis_for_df_dict, analysis_name, duration,\n",
    "                  timestamp_start_is_photo_analysis,\n",
    "                  timestamp_end_is_photo_analysis):\n",
    "    time_analysis_dict[analysis_name] = {}\n",
    "    time_analysis_dict[analysis_name]['duration_str'] = f\"Analysis took: {duration}\"\n",
    "    time_analysis_dict[analysis_name]['duration_seconds'] = total_seconds\n",
    "    time_analysis_dict[analysis_name]['duration_seconds_str'] = f\"Analysis took: {total_seconds:.2f} seconds\"\n",
    "    time_analysis_dict[analysis_name]['duration_minutes'] = total_seconds/60\n",
    "    time_analysis_dict[analysis_name]['duration_minutes_str'] = f\"Analysis took: {total_seconds/60:.2f} minutes\"\n",
    "    time_analysis_dict[analysis_name]['time_stamp_start'] = timestamp_start_is_photo_analysis\n",
    "    time_analysis_dict[analysis_name]['time_stamp_end'] = timestamp_end_is_photo_analysis\n",
    "\n",
    "    time_analysis_for_df_dict['analysis_name'].append(analysis_name)\n",
    "    time_analysis_for_df_dict['time_stamp_start'].append(timestamp_start_is_photo_analysis)\n",
    "    time_analysis_for_df_dict['duration_str'].append(f\"Analysis took: {duration}\")\n",
    "    time_analysis_for_df_dict['duration_seconds'].append(total_seconds)\n",
    "    time_analysis_for_df_dict['duration_seconds_str'].append(f\"Analysis took: {total_seconds:.2f} seconds\")\n",
    "    time_analysis_for_df_dict['duration_minutes'].append(total_seconds/60)\n",
    "    time_analysis_for_df_dict['duration_minutes_str'].append(f\"Analysis took: {total_seconds/60:.2f} minutes\")\n",
    "\n",
    "    return time_analysis_dict, time_analysis_for_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2d784-e1c0-4603-a50b-6868f5079e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_giub_img_dir_llm_yn_text(jpg_data_path, create_analysis_prompt, model_function):\n",
    "    # Get time stamp:\n",
    "    timestamp_start_is_photo_analysis = pd.Timestamp.now()\n",
    "    \n",
    "    # Get list of image files to analyse: \n",
    "    image_files = os.listdir(jpg_data_path)\n",
    "    img_ids = [image_file.split('Oberland')[1].split('.')[0] for image_file in image_files]\n",
    "    \n",
    "    # Make empty dictionary to store results:\n",
    "    image_descr = {}\n",
    "    \n",
    "    # Loop through images to get answers: \n",
    "    for image_file in image_files:\n",
    "        image_path = jpg_data_path / image_file\n",
    "        path_str = str(image_path)\n",
    "        #print('\\n')\n",
    "        #print(path_str)\n",
    "        parts = path_str.split('.jpg')\n",
    "        img_id = parts[-2][-3:]\n",
    "    \n",
    "        # Analyse image, get values for each of the categorical variables specified above:\n",
    "        #image_description = analyze_image_structured(image_path)\n",
    "        #image_description = llm_o.analyze_image_structured(image_path, create_analysis_prompt)\n",
    "        #image_description = llm_o.analyze_image_structured(image_path, create_analysis_prompt, model_function)\n",
    "        image_description = llm_o.analyze_image_yes_no(image_path, create_analysis_prompt, model_function)\n",
    "        \n",
    "        #dict_type_bool = type(image_description) == dict\n",
    "            \n",
    "        #print(image_description)\n",
    "        image_descr[img_id] = image_description\n",
    "    \n",
    "    timestamp_end_is_photo_analysis = pd.Timestamp.now()\n",
    "\n",
    "    return timestamp_start_is_photo_analysis, timestamp_end_is_photo_analysis, image_descr\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379cca7-718b-4783-bb97-b6d6c1fe177a",
   "metadata": {},
   "source": [
    "### Choose LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbdf7cd-470d-4209-ae65-fa0e9a94b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_function = llm_i.call_minicpm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baecdaa-3eb8-4b84-a687-6d79a2b8dc6b",
   "metadata": {},
   "source": [
    "### Prepare empty dictionary for time analyses and get time stamp for overall workflow duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b873fb2-76d4-45a3-845c-cfe3581d5a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_analyses = {}\n",
    "time_analyses_for_df = {}\n",
    "time_analyses_for_df['analysis_name'] = []\n",
    "time_analyses_for_df['time_stamp_start'] = []\n",
    "time_analyses_for_df['duration_str'] = []\n",
    "time_analyses_for_df['duration_seconds'] = []\n",
    "time_analyses_for_df['duration_seconds_str'] = []\n",
    "time_analyses_for_df['duration_minutes'] = []\n",
    "time_analyses_for_df['duration_minutes_str'] = []\n",
    "\n",
    "timestamp_start_workflow = pd.Timestamp.now()\n",
    "timestamp_start_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a948d-ff0a-47f6-b0d9-26a1b858b621",
   "metadata": {},
   "source": [
    "### Prepare empty dictionary to store the different response dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d6356-c39e-4543-ab7a-904f9faf9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dictionaries = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944fe6c-cbf4-44a5-9a60-2161ea174666",
   "metadata": {},
   "source": [
    "### Prepare empty dictionary for cases with unstructured answers for visual inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c71661-cc6a-4a0f-aec2-372111668756",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_closer_inspection = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376dce6f-6efc-4190-8302-212c4840a5bf",
   "metadata": {},
   "source": [
    "### Prepare empty dictionary for result dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0502d312-4363-420e-8ba2-4e897d9bd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tabular = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f50665f-eccc-46bb-aa22-b66055f62c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cddf2c0-ee5c-477e-8097-35ddbdbb67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_metrics = pd.DataFrame({'positives': [],\n",
    "              'negatives': [], \n",
    "              'true_positives': [], \n",
    "              'true_negatives': [],\n",
    "              'false_negatives': [], \n",
    "              'false_positives': [], \n",
    "              'sensitivity': [], \n",
    "              'specificity': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96499d-706e-43f5-9ce5-905f9f24381a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "585626b0-743e-49a1-b0a5-ce2dc292831c",
   "metadata": {},
   "source": [
    "### Set paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4a90a-ef5d-4305-be02-bc10a9abf0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_path = Path('/Users/stephanehess/Documents/CAS_AML/dias_digit_project')\n",
    "#root_path = Path('/Users/stephanehess/Documents/CAS_AML/dias_digit_project/test_yolo_object_train')\n",
    "\n",
    "project_path = Path.cwd()\n",
    "root_path = (project_path / 'test_LLM_prompt_experiments').resolve()\n",
    "#root_path = project_path\n",
    "data_path = root_path / 'data'\n",
    "tif_data_path = root_path / 'data_1'\n",
    "#data_path = root_path / 'visual_genome_data_all'\n",
    "jpg_data_path = root_path / 'data_jpg'\n",
    "#yolo_path = root_path / 'visual_genome_yolo_all'\n",
    "output_dir_not_photo = root_path / 'not_photo'\n",
    "output_dir_with_person = root_path / 'with_person'\n",
    "output_dir_without_person = root_path / 'without_person'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffceee-2d3e-4720-82fc-e53af17efd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e570215b-ddd7-45bb-99f5-adbbd6e80afe",
   "metadata": {},
   "source": [
    "### Create directories for sorting the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd239889-c0d0-4290-96bd-c0f6dcdad172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "#os.chdir(root_path/'..')\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(tif_data_path, exist_ok=True)\n",
    "os.makedirs(jpg_data_path, exist_ok=True)\n",
    "#os.makedirs(output_dir_not_photo, exist_ok=True)\n",
    "#os.makedirs(output_dir_with_person, exist_ok=True)\n",
    "#os.makedirs(output_dir_without_person, exist_ok=True)\n",
    "#os.chdir('root_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2fe3ae-d0e8-4101-b740-fde42409b7aa",
   "metadata": {},
   "source": [
    "### Copy and convert image files from tif_data_path to jpg_data_path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32204a-0c76-46d2-ae33-d1b517769bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_folder = tif_data_path\n",
    "destination_folder = jpg_data_path\n",
    "\n",
    "llm_i.convert_tif_to_jpg(source_folder, destination_folder, quality=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bd1c1-a1d1-4be9-9cd9-052b058d3576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f0fea0f-6a59-46eb-b057-12cb5672d4b2",
   "metadata": {},
   "source": [
    "### Load person label data (ground truth) to compare to LLM responses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c7411-e309-4f9f-8051-63c7f4867514",
   "metadata": {},
   "source": [
    "The file with_without_person.csv contains labels added by (human) visual inspection that represent the ground truth. \n",
    " * Column with_person: whether or not any person is in the image.\n",
    " * Column recognisable: whether any person that would be recognisable to a human familiar with said person is in the image.\n",
    " * Column church: whether the image contains a church.\n",
    " * Column is_photo: whether the image is a photography or something else. (this formulation is, I guess, unprecise, as most dias can probably be called a photography of sorts (if a dia shows a painting, I assume a photograph of the painting has been taken), so, to be precise: whether or not the image is showing anything that exists in the real world or is showing a representation of anything that exists in the real world or aspects thereof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15bba5d-201d-470c-95c1-578906246062",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = pd.read_csv(data_path/'labels_mod.csv')\n",
    "label_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6aa73-8348-4a71-9f83-b606e5107f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconvert image ids to integers (e.g. '234') as strings from the form they were saved in (e.g. 'id234' \n",
    "# to ensure string data type to deal with duck typing): \n",
    "img_ids = list(label_data.image_id)\n",
    "label_data['image_id'] = img_idc.reconvert_image_ids(img_ids)\n",
    "label_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399cc27e-f350-4fd0-8d56-c2a28310e6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72cc5225-b73b-4cc3-a1d7-c9ab2cbea829",
   "metadata": {},
   "source": [
    "### The following cell is only required for the test run on the test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e760fae-8f9c-45b6-8eeb-9b50fd715313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only rows referring to images in the smaller data set (test run):\n",
    "\n",
    "# Make sure no .DS_Store file is included in jpg_data_path: \n",
    "import os\n",
    "ds_file_path = jpg_data_path / '.DS_Store'\n",
    "\n",
    "# Remove a specific .DS_Store file\n",
    "if os.path.exists(ds_file_path):\n",
    "    os.remove(ds_file_path)\n",
    "    print(\"Removed .DS_Store\")\n",
    "else:\n",
    "    print(\".DS_Store not found\")\n",
    "\n",
    "# Get list of image files present:\n",
    "image_files = os.listdir(jpg_data_path)\n",
    "\n",
    "# Extract image ids from image file names:\n",
    "img_ids = [image_file.split('Oberland')[1].split('.')[0] for image_file in image_files]\n",
    "img_ids.sort()\n",
    "print(img_ids)\n",
    "\n",
    "# Select relevant rows from label_data data frame by id list: \n",
    "select_bools = [img_id in img_ids for img_id in label_data.image_id]\n",
    "\n",
    "label_data = label_data[select_bools].copy()\n",
    "label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853a5ba-1fff-412b-8861-171e54e6fa6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "747bfce4-a523-4307-acc9-59960f2bc00e",
   "metadata": {},
   "source": [
    "### Prepare variations of LLM prompt functions to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce2beb-05bf-4767-990b-f182518ab33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_prompt_yes_no_basic():\n",
    "    \"\"\"Create the structured prompt for image analysis.\"\"\"\n",
    "    return \"\"\"\n",
    "    Is this image a photography or is it something else?\n",
    "\n",
    "    Please, answwer yes if it is a photography, answer no if it is something else.\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d8d08-2ea4-4815-ac7b-634fddc96eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_prompt_yes_no_precise():\n",
    "    \"\"\"Create the structured prompt for image analysis.\"\"\"\n",
    "    return \"\"\"\n",
    "    Is this image a photography or is it something else?\n",
    "\n",
    "    Please, answwer yes if the image is a direct representation of something, no if it is an indirect representation (i.e. a representation of a representation of something).\n",
    "    \n",
    "    A representation can represent anything that exists or aspects thereof; the represantation can be concrete or abstract. \n",
    "    The image at hand can either be such a direct representation \n",
    "    or else a representation of a representation of anything that exists or of aspects thereof (concrete or abstract). \n",
    "    In other words, you have to determine if the image is a direct (answer yes) or an indirect representation (answer no).\n",
    "    Return ONLY the dictionary, no other text.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7ddaf-fb89-4ce7-804b-799f1b5405c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_prompt_yes_no_intuitive():\n",
    "    \"\"\"Create the structured prompt for image analysis.\"\"\"\n",
    "    return \"\"\"\n",
    "    Is this image a photography or is it something else (e.g. if the image is a map or a painting)?\n",
    "\n",
    "    Please, answwer yes if it is a photography, answer no if it is something else.\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497cfce9-386f-438e-8fef-355fa110215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_prompt_yes_no_alternatives():\n",
    "    \"\"\"Create the structured prompt for image analysis.\"\"\"\n",
    "    return \"\"\"\n",
    "    Is this image a photography or is it something else (map, a painting, a drawing, a scheme, a statistics figure, or other)?\n",
    "\n",
    "    Please, answwer yes if it is a photography, answer no if it is something else.\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd19d9-0463-44f9-8339-d7d65fb8b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_prompt_map_yes_no_alternatives():\n",
    "    \"\"\"Create the structured prompt for image analysis.\"\"\"\n",
    "    return \"\"\"\n",
    "    Is this image a map or is it something else (photography, a painting, a drawing, a scheme, a statistics figure, or other)?\n",
    "\n",
    "    Please, answwer yes if it is a map, answer no if it is something else.\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb1c28-55c3-488e-97fc-2eeb78389331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_prompt_person_yes_no():\n",
    "    \"\"\"Create the structured prompt for image analysis.\"\"\"\n",
    "    return \"\"\"\n",
    "    Are there any persons in this image?\n",
    "    Please, answwer yes if there is one or multiple persons in the image, answer no if there is no person in the image.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dfcfa9-9b40-4c38-aa7a-013785a08952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "805092b5-17dc-49b7-9987-3fc5355497cd",
   "metadata": {},
   "source": [
    "## Identify non-photo images with basic prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a83c0f-fab5-4993-bfee-34f92a23da91",
   "metadata": {},
   "source": [
    "### Set parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2855bbb-73c8-4249-9f1d-02751bc7b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = 'is_photo_basic_nat_minicpm'\n",
    "create_analysis_prompt = create_analysis_prompt_yes_no_basic\n",
    "\n",
    "label_name = 'is_photo'\n",
    "prediction_name = 'is_photo_pred'\n",
    "keys_list_expected = ['answer', 'additional_comments']\n",
    "#response_variable = 'image_is_photography'\n",
    "response_variable = 'answer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f5a6a-e727-4709-9a46-bcae7d45093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_analysis_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0051afb-2016-43a2-bc39-a1a5a337c8d7",
   "metadata": {},
   "source": [
    "### Prepare data objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbbf9c-80c9-4caf-952c-8c7601c12aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data objects: \n",
    "labels_results_repetitions = []\n",
    "response_dictionaries[analysis_name] = {}\n",
    "\n",
    "ml_metrics_analysis_name = []\n",
    "ml_metrics_time_stamp = []\n",
    "ml_metrics_positives = []\n",
    "ml_metrics_negatives = []\n",
    "ml_metrics_true_positives = []\n",
    "ml_metrics_false_positives = []\n",
    "ml_metrics_true_negatives = []\n",
    "ml_metrics_false_negatives = []\n",
    "ml_metrics_sensitivity = []\n",
    "ml_metrics_specificity = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee20cb47-b260-4a76-ad0a-4ee6d5c35501",
   "metadata": {},
   "source": [
    "### Extract and store results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c7beb-9897-4cd7-9147-41748cd4fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "itercount = 0\n",
    "\n",
    "while itercount < 1:\n",
    "\n",
    "    # Analysis with LLM: \n",
    "    timestamp_start_is_photo_analysis, timestamp_end_is_photo_analysis, image_descr = analyse_giub_img_dir_llm_yn_text(jpg_data_path, create_analysis_prompt, model_function)\n",
    "\n",
    "    # Calculate duration of analysis: \n",
    "    duration = timestamp_end_is_photo_analysis - timestamp_start_is_photo_analysis\n",
    "    total_seconds = duration.total_seconds()\n",
    "    print(total_seconds)\n",
    "\n",
    "    # Store information about duration: \n",
    "    time_analyses, time_analyses_for_df = store_duration(time_analyses, time_analyses_for_df, analysis_name, \n",
    "                   duration,timestamp_start_is_photo_analysis,\n",
    "                  timestamp_end_is_photo_analysis)\n",
    "    \n",
    "    # Store dictionary with LLM responses:\n",
    "    response_dictionaries[analysis_name][timestamp_start_is_photo_analysis] = image_descr\n",
    "    \n",
    "    # Extract LLM responses from dictionary:\n",
    "    #img_ids, is_photo, img_ids_closer_inspection = \\\n",
    "    #llm_o.extract_vals_from_response_dict(img_ids, image_descr, keys_list_expected, response_variable)\n",
    "\n",
    "    img_ids, is_photo, img_ids_closer_inspection = \\\n",
    "    extract_vals_from_yes_no_response(img_ids, image_descr, keys_list_expected, response_variable)\n",
    "    \n",
    "    # Check if the response variable lists has the same length as id list:\n",
    "    # print('Length of img_ids:')\n",
    "    # print(len(img_ids))\n",
    "    # print('Length of is_photo:')\n",
    "    # print(len(is_photo))\n",
    "    \n",
    "    # Put response variables into data frame: \n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids)\n",
    "    predictions = pd.DataFrame({'image_id': img_ids, \n",
    "                               prediction_name: is_photo,\n",
    "                               'time_stamp': timestamp_ids})\n",
    "    \n",
    "    # Check for missing values:\n",
    "    # print(predictions.isnull().any().any())\n",
    "    # print(predictions.isna().any().any())\n",
    "    # print(has_missing_comprehensive(predictions))\n",
    "    \n",
    "    # Merge label data with the predictions:\n",
    "    label_data_c = label_data.copy()\n",
    "    labels_results = label_data_c.merge(predictions, how='inner', on='image_id')\n",
    "    print(labels_results.shape)\n",
    "    labels_results_repetitions.append(labels_results)\n",
    "    \n",
    "    # Save labels and predictions in dictionary: \n",
    "    #results_tabular[analysis_name] = {}\n",
    "    #results_tabular[analysis_name][timestamp_start_is_photo_analysis] = labels_results\n",
    "    \n",
    "    # Save image list for closer inspection:\n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids_closer_inspection)\n",
    "    imgs_closer_inspection = pd.DataFrame({'image_id': img_ids_closer_inspection,\n",
    "    'time_stamp': timestamp_ids})\n",
    "    images_closer_inspection[analysis_name] = imgs_closer_inspection\n",
    "    \n",
    "    \n",
    "    # Calculate sensitivity and specificity for photography predictions and get lists images with positive photography predictions:\n",
    "    subsets_and_metrics = llm_o.get_classification_subsets_metrics(labels_results, label_name, prediction_name)\n",
    "    positives, negatives, true_positives, true_negatives, \\\n",
    "    false_negatives, false_positives, sensitivity, specificity = subsets_and_metrics\n",
    "    # ml_metrics['analysis_name'] = analysis_name\n",
    "    # ml_metrics['time_stamp'] = timestamp_start_is_photo_analysis\n",
    "    # ml_metrics['positives'] = positives.shape[0]\n",
    "    # ml_metrics['negatives'] = negatives.shape[0]\n",
    "    # ml_metrics['true_positives'] = true_positives.shape[0]\n",
    "    # ml_metrics['false_positives'] = false_positives.shape[0]\n",
    "    # ml_metrics['true_negatives'] = true_negatives.shape[0]\n",
    "    # ml_metrics['false_negatives'] = false_negatives.shape[0]\n",
    "    # ml_metrics['sensitivity'] = sensitivity\n",
    "    # ml_metrics['specificity'] = specificity\n",
    "\n",
    "\n",
    "    ml_metrics_analysis_name.append(analysis_name)\n",
    "    ml_metrics_time_stamp.append(timestamp_start_is_photo_analysis)\n",
    "    ml_metrics_positives.append(positives.shape[0])\n",
    "    ml_metrics_negatives.append(negatives.shape[0])\n",
    "    ml_metrics_true_positives.append(true_positives.shape[0])\n",
    "    ml_metrics_false_positives.append(false_positives.shape[0])\n",
    "    ml_metrics_true_negatives.append(true_negatives.shape[0])\n",
    "    ml_metrics_false_negatives.append(false_negatives.shape[0])\n",
    "    ml_metrics_sensitivity.append(sensitivity)\n",
    "    ml_metrics_specificity.append(specificity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'True Positives: {true_positives.shape[0]}')\n",
    "    print(f'False Positives: {false_positives.shape[0]}')\n",
    "    print(f'True Negatives: {true_negatives.shape[0]}')\n",
    "    print(f'False Negatives: {false_negatives.shape[0]}')\n",
    "    \n",
    "    itercount += 1\n",
    "\n",
    "results_tabular[analysis_name] = pd.concat(labels_results_repetitions, ignore_index=True)\n",
    "\n",
    "\n",
    "ml_metrics_one_analysis = pd.DataFrame({'positives': [],\n",
    "              'negatives': [], \n",
    "              'true_positives': [], \n",
    "              'true_negatives': [],\n",
    "              'false_negatives': [], \n",
    "              'false_positives': [], \n",
    "              'sensitivity': [], \n",
    "              'specificity': []})\n",
    "\n",
    "ml_metrics_one_analysis['analysis_name'] = ml_metrics_analysis_name\n",
    "ml_metrics_one_analysis['time_stamp'] = ml_metrics_time_stamp\n",
    "ml_metrics_one_analysis['positives'] = ml_metrics_positives\n",
    "ml_metrics_one_analysis['negatives'] = ml_metrics_negatives\n",
    "ml_metrics_one_analysis['true_positives'] = ml_metrics_true_positives\n",
    "ml_metrics_one_analysis['false_positives'] = ml_metrics_false_positives\n",
    "ml_metrics_one_analysis['true_negatives'] = ml_metrics_true_negatives\n",
    "ml_metrics_one_analysis['false_negatives'] = ml_metrics_false_negatives\n",
    "ml_metrics_one_analysis['sensitivity'] = ml_metrics_sensitivity\n",
    "ml_metrics_one_analysis['specificity'] = ml_metrics_specificity\n",
    "\n",
    "ml_metrics = pd.concat([ml_metrics, ml_metrics_one_analysis], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1cf395-dae8-4e81-8120-24d0c16727e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e13b6-d20b-4083-b460-322d37ac7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dictionaries[analysis_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e4e2d-e6ae-437b-9048-ccb9840806ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26467347-fdd1-49a6-8ea4-2985b74b4646",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tabular[analysis_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351bf48e-344f-416f-9753-691e50de3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0303f-f38d-4910-8010-a3df0b305fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a6205-2b16-4b64-b6e9-cfb8f6117250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93dad657-2cbe-4da7-a384-3c6716d8a2aa",
   "metadata": {},
   "source": [
    "## Identify non-photo images with precise prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d55c60e-57f4-434f-b46c-a006251e36f5",
   "metadata": {},
   "source": [
    "### Set parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e52242-17d4-4d05-8533-62b8c53d0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = 'is_photo_precise_nat_minicpm'\n",
    "create_analysis_prompt = create_analysis_prompt_yes_no_precise\n",
    "\n",
    "label_name = 'is_photo'\n",
    "prediction_name = 'is_photo_pred'\n",
    "keys_list_expected = ['answer', 'additional_comments']\n",
    "#response_variable = 'image_is_photography'\n",
    "response_variable = 'answer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4965498-9f88-4afc-b017-be794e1e94e4",
   "metadata": {},
   "source": [
    "### Prepare data objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b1514-e074-423b-9000-b13949635e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data objects: \n",
    "labels_results_repetitions = []\n",
    "response_dictionaries[analysis_name] = {}\n",
    "\n",
    "ml_metrics_analysis_name = []\n",
    "ml_metrics_time_stamp = []\n",
    "ml_metrics_positives = []\n",
    "ml_metrics_negatives = []\n",
    "ml_metrics_true_positives = []\n",
    "ml_metrics_false_positives = []\n",
    "ml_metrics_true_negatives = []\n",
    "ml_metrics_false_negatives = []\n",
    "ml_metrics_sensitivity = []\n",
    "ml_metrics_specificity = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86efb386-60db-4ac2-b0de-2ef453b69447",
   "metadata": {},
   "source": [
    "### Extract and store results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae4fc4-0fe7-4e25-b1f9-1482d23cb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "itercount = 0\n",
    "\n",
    "while itercount < 1:\n",
    "\n",
    "    # Analysis with LLM: \n",
    "    timestamp_start_is_photo_analysis, timestamp_end_is_photo_analysis, image_descr = analyse_giub_img_dir_llm_yn_text(jpg_data_path, create_analysis_prompt, model_function)\n",
    "\n",
    "    # Calculate duration of analysis: \n",
    "    duration = timestamp_end_is_photo_analysis - timestamp_start_is_photo_analysis\n",
    "    total_seconds = duration.total_seconds()\n",
    "    print(total_seconds)\n",
    "\n",
    "    # Store information about duration: \n",
    "    time_analyses, time_analyses_for_df = store_duration(time_analyses, time_analyses_for_df, analysis_name, \n",
    "                   duration,timestamp_start_is_photo_analysis,\n",
    "                  timestamp_end_is_photo_analysis)\n",
    "    \n",
    "    # Store dictionary with LLM responses:\n",
    "    response_dictionaries[analysis_name][timestamp_start_is_photo_analysis] = image_descr\n",
    "    \n",
    "    # Extract LLM responses from dictionary:\n",
    "    #img_ids, is_photo, img_ids_closer_inspection = \\\n",
    "    #llm_o.extract_vals_from_response_dict(img_ids, image_descr, keys_list_expected, response_variable)\n",
    "\n",
    "    img_ids, is_photo, img_ids_closer_inspection = \\\n",
    "    extract_vals_from_yes_no_response(img_ids, image_descr, keys_list_expected, response_variable)\n",
    "    \n",
    "    # Check if the response variable lists has the same length as id list:\n",
    "    # print('Length of img_ids:')\n",
    "    # print(len(img_ids))\n",
    "    # print('Length of is_photo:')\n",
    "    # print(len(is_photo))\n",
    "    \n",
    "    # Put response variables into data frame: \n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids)\n",
    "    predictions = pd.DataFrame({'image_id': img_ids, \n",
    "                               prediction_name: is_photo,\n",
    "                               'time_stamp': timestamp_ids})\n",
    "    \n",
    "    # Check for missing values:\n",
    "    # print(predictions.isnull().any().any())\n",
    "    # print(predictions.isna().any().any())\n",
    "    # print(has_missing_comprehensive(predictions))\n",
    "    \n",
    "    # Merge label data with the predictions:\n",
    "    label_data_c = label_data.copy()\n",
    "    labels_results = label_data_c.merge(predictions, how='inner', on='image_id')\n",
    "    print(labels_results.shape)\n",
    "    labels_results_repetitions.append(labels_results)\n",
    "    \n",
    "    # Save labels and predictions in dictionary: \n",
    "    #results_tabular[analysis_name] = {}\n",
    "    #results_tabular[analysis_name][timestamp_start_is_photo_analysis] = labels_results\n",
    "    \n",
    "    # Save image list for closer inspection:\n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids_closer_inspection)\n",
    "    imgs_closer_inspection = pd.DataFrame({'image_id': img_ids_closer_inspection,\n",
    "    'time_stamp': timestamp_ids})\n",
    "    images_closer_inspection[analysis_name] = imgs_closer_inspection\n",
    "    \n",
    "    \n",
    "    # Calculate sensitivity and specificity for photography predictions and get lists images with positive photography predictions:\n",
    "    subsets_and_metrics = llm_o.get_classification_subsets_metrics(labels_results, label_name, prediction_name)\n",
    "    positives, negatives, true_positives, true_negatives, \\\n",
    "    false_negatives, false_positives, sensitivity, specificity = subsets_and_metrics\n",
    "    # ml_metrics['analysis_name'] = analysis_name\n",
    "    # ml_metrics['time_stamp'] = timestamp_start_is_photo_analysis\n",
    "    # ml_metrics['positives'] = positives.shape[0]\n",
    "    # ml_metrics['negatives'] = negatives.shape[0]\n",
    "    # ml_metrics['true_positives'] = true_positives.shape[0]\n",
    "    # ml_metrics['false_positives'] = false_positives.shape[0]\n",
    "    # ml_metrics['true_negatives'] = true_negatives.shape[0]\n",
    "    # ml_metrics['false_negatives'] = false_negatives.shape[0]\n",
    "    # ml_metrics['sensitivity'] = sensitivity\n",
    "    # ml_metrics['specificity'] = specificity\n",
    "\n",
    "\n",
    "    ml_metrics_analysis_name.append(analysis_name)\n",
    "    ml_metrics_time_stamp.append(timestamp_start_is_photo_analysis)\n",
    "    ml_metrics_positives.append(positives.shape[0])\n",
    "    ml_metrics_negatives.append(negatives.shape[0])\n",
    "    ml_metrics_true_positives.append(true_positives.shape[0])\n",
    "    ml_metrics_false_positives.append(false_positives.shape[0])\n",
    "    ml_metrics_true_negatives.append(true_negatives.shape[0])\n",
    "    ml_metrics_false_negatives.append(false_negatives.shape[0])\n",
    "    ml_metrics_sensitivity.append(sensitivity)\n",
    "    ml_metrics_specificity.append(specificity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'True Positives: {true_positives.shape[0]}')\n",
    "    print(f'False Positives: {false_positives.shape[0]}')\n",
    "    print(f'True Negatives: {true_negatives.shape[0]}')\n",
    "    print(f'False Negatives: {false_negatives.shape[0]}')\n",
    "    \n",
    "    itercount += 1\n",
    "\n",
    "results_tabular[analysis_name] = pd.concat(labels_results_repetitions, ignore_index=True)\n",
    "\n",
    "\n",
    "ml_metrics_one_analysis = pd.DataFrame({'positives': [],\n",
    "              'negatives': [], \n",
    "              'true_positives': [], \n",
    "              'true_negatives': [],\n",
    "              'false_negatives': [], \n",
    "              'false_positives': [], \n",
    "              'sensitivity': [], \n",
    "              'specificity': []})\n",
    "\n",
    "ml_metrics_one_analysis['analysis_name'] = ml_metrics_analysis_name\n",
    "ml_metrics_one_analysis['time_stamp'] = ml_metrics_time_stamp\n",
    "ml_metrics_one_analysis['positives'] = ml_metrics_positives\n",
    "ml_metrics_one_analysis['negatives'] = ml_metrics_negatives\n",
    "ml_metrics_one_analysis['true_positives'] = ml_metrics_true_positives\n",
    "ml_metrics_one_analysis['false_positives'] = ml_metrics_false_positives\n",
    "ml_metrics_one_analysis['true_negatives'] = ml_metrics_true_negatives\n",
    "ml_metrics_one_analysis['false_negatives'] = ml_metrics_false_negatives\n",
    "ml_metrics_one_analysis['sensitivity'] = ml_metrics_sensitivity\n",
    "ml_metrics_one_analysis['specificity'] = ml_metrics_specificity\n",
    "\n",
    "ml_metrics = pd.concat([ml_metrics, ml_metrics_one_analysis], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f27a70-2827-422a-9f2f-2265c9f36048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05525549-b125-4c2c-ac77-6a767ace01d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3483a37-7ef4-4f30-9d23-ce3a632b8a22",
   "metadata": {},
   "source": [
    "## Identify non-photo images with intuitive prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2322d-096c-4334-af36-cb457c929aee",
   "metadata": {},
   "source": [
    "### Set parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1c3b5-5c1a-4d9c-a7e4-7f90d8a2b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analysis_name = 'is_photo_intuitive_nat_minicpm'\n",
    "create_analysis_prompt = create_analysis_prompt_yes_no_intuitive\n",
    "\n",
    "label_name = 'is_photo'\n",
    "prediction_name = 'is_photo_pred'\n",
    "keys_list_expected = ['answer', 'additional_comments']\n",
    "#response_variable = 'image_is_photography'\n",
    "response_variable = 'answer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ea085-d7ea-4a8d-94b2-5b2d768c4e4c",
   "metadata": {},
   "source": [
    "### Prepare data objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8299dcc-2e32-42d2-9ae9-e7dcbd717617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data objects: \n",
    "labels_results_repetitions = []\n",
    "response_dictionaries[analysis_name] = {}\n",
    "\n",
    "ml_metrics_analysis_name = []\n",
    "ml_metrics_time_stamp = []\n",
    "ml_metrics_positives = []\n",
    "ml_metrics_negatives = []\n",
    "ml_metrics_true_positives = []\n",
    "ml_metrics_false_positives = []\n",
    "ml_metrics_true_negatives = []\n",
    "ml_metrics_false_negatives = []\n",
    "ml_metrics_sensitivity = []\n",
    "ml_metrics_specificity = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee677e08-0144-4607-b62a-1de52d96c2e8",
   "metadata": {},
   "source": [
    "### Extract and store results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e61234-196f-4c0b-9ac6-3e922b976ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "itercount = 0\n",
    "\n",
    "while itercount < 1:\n",
    "\n",
    "    # Analysis with LLM: \n",
    "    timestamp_start_is_photo_analysis, timestamp_end_is_photo_analysis, image_descr = analyse_giub_img_dir_llm_yn_text(jpg_data_path, create_analysis_prompt, model_function)\n",
    "\n",
    "    # Calculate duration of analysis: \n",
    "    duration = timestamp_end_is_photo_analysis - timestamp_start_is_photo_analysis\n",
    "    total_seconds = duration.total_seconds()\n",
    "    print(total_seconds)\n",
    "\n",
    "    # Store information about duration: \n",
    "    time_analyses, time_analyses_for_df = store_duration(time_analyses, time_analyses_for_df, analysis_name, \n",
    "                   duration,timestamp_start_is_photo_analysis,\n",
    "                  timestamp_end_is_photo_analysis)\n",
    "    \n",
    "    # Store dictionary with LLM responses:\n",
    "    response_dictionaries[analysis_name][timestamp_start_is_photo_analysis] = image_descr\n",
    "    \n",
    "    # Extract LLM responses from dictionary:\n",
    "    #img_ids, is_photo, img_ids_closer_inspection = \\\n",
    "    #llm_o.extract_vals_from_response_dict(img_ids, image_descr, keys_list_expected, response_variable)\n",
    "\n",
    "    img_ids, is_photo, img_ids_closer_inspection = \\\n",
    "    extract_vals_from_yes_no_response(img_ids, image_descr, keys_list_expected, response_variable)\n",
    "    \n",
    "    # Check if the response variable lists has the same length as id list:\n",
    "    # print('Length of img_ids:')\n",
    "    # print(len(img_ids))\n",
    "    # print('Length of is_photo:')\n",
    "    # print(len(is_photo))\n",
    "    \n",
    "    # Put response variables into data frame: \n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids)\n",
    "    predictions = pd.DataFrame({'image_id': img_ids, \n",
    "                               prediction_name: is_photo,\n",
    "                               'time_stamp': timestamp_ids})\n",
    "    \n",
    "    # Check for missing values:\n",
    "    # print(predictions.isnull().any().any())\n",
    "    # print(predictions.isna().any().any())\n",
    "    # print(has_missing_comprehensive(predictions))\n",
    "    \n",
    "    # Merge label data with the predictions:\n",
    "    label_data_c = label_data.copy()\n",
    "    labels_results = label_data_c.merge(predictions, how='inner', on='image_id')\n",
    "    print(labels_results.shape)\n",
    "    labels_results_repetitions.append(labels_results)\n",
    "    \n",
    "    # Save labels and predictions in dictionary: \n",
    "    #results_tabular[analysis_name] = {}\n",
    "    #results_tabular[analysis_name][timestamp_start_is_photo_analysis] = labels_results\n",
    "    \n",
    "    # Save image list for closer inspection:\n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids_closer_inspection)\n",
    "    imgs_closer_inspection = pd.DataFrame({'image_id': img_ids_closer_inspection,\n",
    "    'time_stamp': timestamp_ids})\n",
    "    images_closer_inspection[analysis_name] = imgs_closer_inspection\n",
    "    \n",
    "    \n",
    "    # Calculate sensitivity and specificity for photography predictions and get lists images with positive photography predictions:\n",
    "    subsets_and_metrics = llm_o.get_classification_subsets_metrics(labels_results, label_name, prediction_name)\n",
    "    positives, negatives, true_positives, true_negatives, \\\n",
    "    false_negatives, false_positives, sensitivity, specificity = subsets_and_metrics\n",
    "    # ml_metrics['analysis_name'] = analysis_name\n",
    "    # ml_metrics['time_stamp'] = timestamp_start_is_photo_analysis\n",
    "    # ml_metrics['positives'] = positives.shape[0]\n",
    "    # ml_metrics['negatives'] = negatives.shape[0]\n",
    "    # ml_metrics['true_positives'] = true_positives.shape[0]\n",
    "    # ml_metrics['false_positives'] = false_positives.shape[0]\n",
    "    # ml_metrics['true_negatives'] = true_negatives.shape[0]\n",
    "    # ml_metrics['false_negatives'] = false_negatives.shape[0]\n",
    "    # ml_metrics['sensitivity'] = sensitivity\n",
    "    # ml_metrics['specificity'] = specificity\n",
    "\n",
    "\n",
    "    ml_metrics_analysis_name.append(analysis_name)\n",
    "    ml_metrics_time_stamp.append(timestamp_start_is_photo_analysis)\n",
    "    ml_metrics_positives.append(positives.shape[0])\n",
    "    ml_metrics_negatives.append(negatives.shape[0])\n",
    "    ml_metrics_true_positives.append(true_positives.shape[0])\n",
    "    ml_metrics_false_positives.append(false_positives.shape[0])\n",
    "    ml_metrics_true_negatives.append(true_negatives.shape[0])\n",
    "    ml_metrics_false_negatives.append(false_negatives.shape[0])\n",
    "    ml_metrics_sensitivity.append(sensitivity)\n",
    "    ml_metrics_specificity.append(specificity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'True Positives: {true_positives.shape[0]}')\n",
    "    print(f'False Positives: {false_positives.shape[0]}')\n",
    "    print(f'True Negatives: {true_negatives.shape[0]}')\n",
    "    print(f'False Negatives: {false_negatives.shape[0]}')\n",
    "    \n",
    "    itercount += 1\n",
    "\n",
    "results_tabular[analysis_name] = pd.concat(labels_results_repetitions, ignore_index=True)\n",
    "\n",
    "\n",
    "ml_metrics_one_analysis = pd.DataFrame({'positives': [],\n",
    "              'negatives': [], \n",
    "              'true_positives': [], \n",
    "              'true_negatives': [],\n",
    "              'false_negatives': [], \n",
    "              'false_positives': [], \n",
    "              'sensitivity': [], \n",
    "              'specificity': []})\n",
    "\n",
    "ml_metrics_one_analysis['analysis_name'] = ml_metrics_analysis_name\n",
    "ml_metrics_one_analysis['time_stamp'] = ml_metrics_time_stamp\n",
    "ml_metrics_one_analysis['positives'] = ml_metrics_positives\n",
    "ml_metrics_one_analysis['negatives'] = ml_metrics_negatives\n",
    "ml_metrics_one_analysis['true_positives'] = ml_metrics_true_positives\n",
    "ml_metrics_one_analysis['false_positives'] = ml_metrics_false_positives\n",
    "ml_metrics_one_analysis['true_negatives'] = ml_metrics_true_negatives\n",
    "ml_metrics_one_analysis['false_negatives'] = ml_metrics_false_negatives\n",
    "ml_metrics_one_analysis['sensitivity'] = ml_metrics_sensitivity\n",
    "ml_metrics_one_analysis['specificity'] = ml_metrics_specificity\n",
    "\n",
    "ml_metrics = pd.concat([ml_metrics, ml_metrics_one_analysis], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f7539-e86c-4aab-9ccb-10eba9cf7c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009ff72-e802-4dc3-9742-9bda9afcc51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2bf3d0-7237-4579-8a28-603590220bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0c75d-d603-48f4-8eef-28a4cf0781af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b589af-62f1-4f49-a24c-593bad8e31a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f274d0dc-d7a9-4685-9993-2585e6e7bbb5",
   "metadata": {},
   "source": [
    "## Identify non-photo images with alternatives prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c66709-dd66-40d1-81b8-9401d3475ed7",
   "metadata": {},
   "source": [
    "### Set parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013351ec-1dbd-42d4-a775-c75e973c9205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analysis_name = 'is_photo_alternatives_nat_minicpm'\n",
    "create_analysis_prompt = create_analysis_prompt_yes_no_alternatives\n",
    "\n",
    "label_name = 'is_photo'\n",
    "prediction_name = 'is_photo_pred'\n",
    "keys_list_expected = ['answer', 'additional_comments']\n",
    "#response_variable = 'image_is_photography'\n",
    "response_variable = 'answer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe4316-1608-4f07-8723-e2294814117b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3574f4b-2563-49f6-8d59-15cb79f8d71a",
   "metadata": {},
   "source": [
    "### Prepare data objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c80da-c85f-4568-8451-5a2264f1a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data objects: \n",
    "labels_results_repetitions = []\n",
    "response_dictionaries[analysis_name] = {}\n",
    "\n",
    "ml_metrics_analysis_name = []\n",
    "ml_metrics_time_stamp = []\n",
    "ml_metrics_positives = []\n",
    "ml_metrics_negatives = []\n",
    "ml_metrics_true_positives = []\n",
    "ml_metrics_false_positives = []\n",
    "ml_metrics_true_negatives = []\n",
    "ml_metrics_false_negatives = []\n",
    "ml_metrics_sensitivity = []\n",
    "ml_metrics_specificity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8891aa6-37a1-472a-a8f7-dfb748a380d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba59ddcc-5bc0-42db-a508-c30b5d3c653e",
   "metadata": {},
   "source": [
    "### Extract and store results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a456e77-31e1-406d-a2d6-f1f9108f26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "itercount = 0\n",
    "\n",
    "while itercount < 1:\n",
    "\n",
    "    # Analysis with LLM: \n",
    "    timestamp_start_is_photo_analysis, timestamp_end_is_photo_analysis, image_descr = analyse_giub_img_dir_llm_yn_text(jpg_data_path, create_analysis_prompt, model_function)\n",
    "\n",
    "    # Calculate duration of analysis: \n",
    "    duration = timestamp_end_is_photo_analysis - timestamp_start_is_photo_analysis\n",
    "    total_seconds = duration.total_seconds()\n",
    "    print(total_seconds)\n",
    "\n",
    "    # Store information about duration: \n",
    "    time_analyses, time_analyses_for_df = store_duration(time_analyses, time_analyses_for_df, analysis_name, \n",
    "                   duration,timestamp_start_is_photo_analysis,\n",
    "                  timestamp_end_is_photo_analysis)\n",
    "    \n",
    "    # Store dictionary with LLM responses:\n",
    "    response_dictionaries[analysis_name][timestamp_start_is_photo_analysis] = image_descr\n",
    "    \n",
    "    # Extract LLM responses from dictionary:\n",
    "    #img_ids, is_photo, img_ids_closer_inspection = \\\n",
    "    #llm_o.extract_vals_from_response_dict(img_ids, image_descr, keys_list_expected, response_variable)\n",
    "\n",
    "    img_ids, is_photo, img_ids_closer_inspection = \\\n",
    "    extract_vals_from_yes_no_response(img_ids, image_descr, keys_list_expected, response_variable)\n",
    "    \n",
    "    # Check if the response variable lists has the same length as id list:\n",
    "    # print('Length of img_ids:')\n",
    "    # print(len(img_ids))\n",
    "    # print('Length of is_photo:')\n",
    "    # print(len(is_photo))\n",
    "    \n",
    "    # Put response variables into data frame: \n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids)\n",
    "    predictions = pd.DataFrame({'image_id': img_ids, \n",
    "                               prediction_name: is_photo,\n",
    "                               'time_stamp': timestamp_ids})\n",
    "    \n",
    "    # Check for missing values:\n",
    "    # print(predictions.isnull().any().any())\n",
    "    # print(predictions.isna().any().any())\n",
    "    # print(has_missing_comprehensive(predictions))\n",
    "    \n",
    "    # Merge label data with the predictions:\n",
    "    label_data_c = label_data.copy()\n",
    "    labels_results = label_data_c.merge(predictions, how='inner', on='image_id')\n",
    "    print(labels_results.shape)\n",
    "    labels_results_repetitions.append(labels_results)\n",
    "    \n",
    "    # Save labels and predictions in dictionary: \n",
    "    #results_tabular[analysis_name] = {}\n",
    "    #results_tabular[analysis_name][timestamp_start_is_photo_analysis] = labels_results\n",
    "    \n",
    "    # Save image list for closer inspection:\n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids_closer_inspection)\n",
    "    imgs_closer_inspection = pd.DataFrame({'image_id': img_ids_closer_inspection,\n",
    "    'time_stamp': timestamp_ids})\n",
    "    images_closer_inspection[analysis_name] = imgs_closer_inspection\n",
    "    \n",
    "    \n",
    "    # Calculate sensitivity and specificity for photography predictions and get lists images with positive photography predictions:\n",
    "    subsets_and_metrics = llm_o.get_classification_subsets_metrics(labels_results, label_name, prediction_name)\n",
    "    positives, negatives, true_positives, true_negatives, \\\n",
    "    false_negatives, false_positives, sensitivity, specificity = subsets_and_metrics\n",
    "    # ml_metrics['analysis_name'] = analysis_name\n",
    "    # ml_metrics['time_stamp'] = timestamp_start_is_photo_analysis\n",
    "    # ml_metrics['positives'] = positives.shape[0]\n",
    "    # ml_metrics['negatives'] = negatives.shape[0]\n",
    "    # ml_metrics['true_positives'] = true_positives.shape[0]\n",
    "    # ml_metrics['false_positives'] = false_positives.shape[0]\n",
    "    # ml_metrics['true_negatives'] = true_negatives.shape[0]\n",
    "    # ml_metrics['false_negatives'] = false_negatives.shape[0]\n",
    "    # ml_metrics['sensitivity'] = sensitivity\n",
    "    # ml_metrics['specificity'] = specificity\n",
    "\n",
    "\n",
    "    ml_metrics_analysis_name.append(analysis_name)\n",
    "    ml_metrics_time_stamp.append(timestamp_start_is_photo_analysis)\n",
    "    ml_metrics_positives.append(positives.shape[0])\n",
    "    ml_metrics_negatives.append(negatives.shape[0])\n",
    "    ml_metrics_true_positives.append(true_positives.shape[0])\n",
    "    ml_metrics_false_positives.append(false_positives.shape[0])\n",
    "    ml_metrics_true_negatives.append(true_negatives.shape[0])\n",
    "    ml_metrics_false_negatives.append(false_negatives.shape[0])\n",
    "    ml_metrics_sensitivity.append(sensitivity)\n",
    "    ml_metrics_specificity.append(specificity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'True Positives: {true_positives.shape[0]}')\n",
    "    print(f'False Positives: {false_positives.shape[0]}')\n",
    "    print(f'True Negatives: {true_negatives.shape[0]}')\n",
    "    print(f'False Negatives: {false_negatives.shape[0]}')\n",
    "    \n",
    "    itercount += 1\n",
    "\n",
    "results_tabular[analysis_name] = pd.concat(labels_results_repetitions, ignore_index=True)\n",
    "\n",
    "\n",
    "ml_metrics_one_analysis = pd.DataFrame({'positives': [],\n",
    "              'negatives': [], \n",
    "              'true_positives': [], \n",
    "              'true_negatives': [],\n",
    "              'false_negatives': [], \n",
    "              'false_positives': [], \n",
    "              'sensitivity': [], \n",
    "              'specificity': []})\n",
    "\n",
    "ml_metrics_one_analysis['analysis_name'] = ml_metrics_analysis_name\n",
    "ml_metrics_one_analysis['time_stamp'] = ml_metrics_time_stamp\n",
    "ml_metrics_one_analysis['positives'] = ml_metrics_positives\n",
    "ml_metrics_one_analysis['negatives'] = ml_metrics_negatives\n",
    "ml_metrics_one_analysis['true_positives'] = ml_metrics_true_positives\n",
    "ml_metrics_one_analysis['false_positives'] = ml_metrics_false_positives\n",
    "ml_metrics_one_analysis['true_negatives'] = ml_metrics_true_negatives\n",
    "ml_metrics_one_analysis['false_negatives'] = ml_metrics_false_negatives\n",
    "ml_metrics_one_analysis['sensitivity'] = ml_metrics_sensitivity\n",
    "ml_metrics_one_analysis['specificity'] = ml_metrics_specificity\n",
    "\n",
    "ml_metrics = pd.concat([ml_metrics, ml_metrics_one_analysis], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baae927-ad72-48e4-9d43-a95a2d0ea5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "274c3f06-274a-47a1-ac6e-3c7287f69b75",
   "metadata": {},
   "source": [
    "## Save ml-metrics data frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d21010-357d-4ff5-9277-116c9c3da7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file name: \n",
    "date = str(timestamp_end_is_photo_analysis).split('.')[0][0:10]\n",
    "filename = 'ml_metrics_prompt_exp_struct_minicpm_' + date + '.csv'\n",
    "ml_metrics_output_path = os.path.join(data_path, filename)\n",
    "\n",
    "# Save csv-file: \n",
    "ml_metrics.to_csv(ml_metrics_output_path, index=False)\n",
    "\n",
    "# Reload saved csv table to check if saving worked:\n",
    "ml_metrics_reloaded = pd.read_csv(ml_metrics_output_path, parse_dates=['time_stamp'])\n",
    "ml_metrics_reloaded.head()\n",
    "\n",
    "ml_metrics.equals(ml_metrics_reloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a42f0-bffa-4b2c-b3a4-2c82a693fad7",
   "metadata": {},
   "source": [
    "## Save response dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b3f1d-fd66-45f3-9105-7717b6a6aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define file name: \n",
    "date = str(timestamp_end_is_photo_analysis).split('.')[0][0:10]\n",
    "filename = 'results_prompt_exp_struct_minicpm_' + date + '.pkl'\n",
    "\n",
    "# Save dictionary with LLM responses:\n",
    "img_analysis_output_path = os.path.join(data_path, filename)\n",
    "with open(img_analysis_output_path, 'wb') as f:\n",
    "   pickle.dump(response_dictionaries, f)\n",
    "\n",
    "# Reload saved dictionary to check if saving worked:\n",
    "with open(img_analysis_output_path, 'rb') as f:\n",
    "   reloaded_image_descr = pickle.load(f)\n",
    "\n",
    "# Check if original and reloaded dictionary are the same:\n",
    "print(len(image_descr))\n",
    "print(type(image_descr))\n",
    "print(type(reloaded_image_descr))\n",
    "print(len(reloaded_image_descr))\n",
    "\n",
    "print(image_descr.keys() == reloaded_image_descr.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f9f140-46a0-4597-a7e8-947fe8e51fc9",
   "metadata": {},
   "source": [
    "## Save labels and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be394aa6-5b9e-4050-b684-d5cff2ca6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define file name: \n",
    "current_timestamp = pd.Timestamp.now()\n",
    "current_date_time = current_timestamp.strftime('%Y-%m-%d %H:%M')\n",
    "results_file_name = 'results_table_prompt_exp_struct_minicpm_' + current_date_time + '.pkl'\n",
    "\n",
    "# Save dictionary with LLM responses:\n",
    "results_tabular_output_path = os.path.join(data_path, results_file_name)\n",
    "with open(results_tabular_output_path, 'wb') as f:\n",
    "   pickle.dump(results_tabular, f)\n",
    "\n",
    "# Reload saved dictionary to check if saving worked:\n",
    "with open(results_tabular_output_path, 'rb') as f:\n",
    "   reloaded_results_tabular = pickle.load(f)\n",
    "\n",
    "# Check if original and reloaded dictionary are the same:\n",
    "print(len(results_tabular))\n",
    "print(type(results_tabular))\n",
    "print(type(reloaded_results_tabular))\n",
    "print(len(reloaded_results_tabular))\n",
    "\n",
    "print(results_tabular.keys() == reloaded_results_tabular.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8437c1e-cc8f-4cb7-99ae-de3f4bfb2ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1676f6ef-f0cc-42fe-94aa-88f0cda7e895",
   "metadata": {},
   "source": [
    "## Calculate duration of analysis overall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6671fb-d2e6-4b18-bf58-7ebb322d0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_end_workflow = pd.Timestamp.now()\n",
    "timestamp_end_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e892f-cdf1-4e60-b9e7-8b73f992cb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7ad7598-9f98-43ef-8929-92bb47df74f4",
   "metadata": {},
   "source": [
    "## Save time analyses: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704780fe-5e1c-4d3e-a3ac-a4e1080fe59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39f470-ad16-496b-904e-21fbae7ba317",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define file name: \n",
    "current_date_time = current_timestamp.strftime('%Y-%m-%d %H:%M')\n",
    "time_analyses_df_file_name = 'times_df_prompt_exp_struct_minicpm_' + current_date_time + '.pkl'\n",
    "\n",
    "# Save dictionary:\n",
    "time_analyses_df_output_path = os.path.join(data_path, time_analyses_df_file_name)\n",
    "with open(time_analyses_df_output_path, 'wb') as f:\n",
    "   pickle.dump(time_analyses_for_df, f)\n",
    "\n",
    "# Reload saved dictionary to check if saving worked:\n",
    "with open(time_analyses_df_output_path, 'rb') as f:\n",
    "   reloaded_time_analyses_for_df = pickle.load(f)\n",
    "\n",
    "# Check if original and reloaded dictionary are the same:\n",
    "print(len(time_analyses_for_df))\n",
    "print(type(time_analyses_for_df))\n",
    "print(type(reloaded_time_analyses_for_df))\n",
    "print(len(reloaded_time_analyses_for_df))\n",
    "\n",
    "print(time_analyses_for_df.keys() == reloaded_time_analyses_for_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072c414-46be-4d07-8561-a470ae6e2779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d7c7d6-929f-46bb-8772-9624e3eac807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7877bf5c-9423-4a54-bd83-80acce77f4bd",
   "metadata": {},
   "source": [
    "### I am here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d3ac3-f2df-4d8d-9aed-e5c149f42c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a4485f-09c0-4edd-bbc8-673b5eb17a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa7fb15e-d347-4871-b060-6007962e4548",
   "metadata": {},
   "source": [
    "## Save response dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define file name: \n",
    "date = str(timestamp_end_is_photo_analysis).split('.')[0][0:10]\n",
    "filename = 'results_prompt_exp_nat_minicpm_' + date + '.pkl'\n",
    "\n",
    "# Save dictionary with LLM responses:\n",
    "img_analysis_output_path = os.path.join(data_path, filename)\n",
    "with open(img_analysis_output_path, 'wb') as f:\n",
    "   pickle.dump(response_dictionaries, f)\n",
    "\n",
    "# Reload saved dictionary to check if saving worked:\n",
    "with open(img_analysis_output_path, 'rb') as f:\n",
    "   reloaded_image_descr = pickle.load(f)\n",
    "\n",
    "# Check if original and reloaded dictionary are the same:\n",
    "print(len(image_descr))\n",
    "print(type(image_descr))\n",
    "print(type(reloaded_image_descr))\n",
    "print(len(reloaded_image_descr))\n",
    "\n",
    "print(image_descr.keys() == reloaded_image_descr.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1319be-905b-471f-b9fd-706b0408a780",
   "metadata": {},
   "source": [
    "## Save labels and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241de369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define file name: \n",
    "current_timestamp = pd.Timestamp.now()\n",
    "current_date_time = current_timestamp.strftime('%Y-%m-%d %H:%M')\n",
    "results_file_name = 'results_table_prompt_exp_nat_minicpm_' + current_date_time + '.pkl'\n",
    "\n",
    "# Save dictionary with LLM responses:\n",
    "results_tabular_output_path = os.path.join(data_path, results_file_name)\n",
    "with open(results_tabular_output_path, 'wb') as f:\n",
    "   pickle.dump(results_tabular, f)\n",
    "\n",
    "# Reload saved dictionary to check if saving worked:\n",
    "with open(results_tabular_output_path, 'rb') as f:\n",
    "   reloaded_results_tabular = pickle.load(f)\n",
    "\n",
    "# Check if original and reloaded dictionary are the same:\n",
    "print(len(results_tabular))\n",
    "print(type(results_tabular))\n",
    "print(type(reloaded_results_tabular))\n",
    "print(len(reloaded_results_tabular))\n",
    "\n",
    "print(results_tabular.keys() == reloaded_results_tabular.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fcfb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc9d2ac2-2eba-4d5a-a128-27c055043b46",
   "metadata": {},
   "source": [
    "## Calculate duration of analysis overall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_end_workflow = pd.Timestamp.now()\n",
    "timestamp_end_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb31b9f-5004-413a-86f9-e492fd20c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = timestamp_end_workflow - timestamp_start_workflow\n",
    "total_seconds = duration.total_seconds()\n",
    "store_duration(time_analyses, time_analyses_for_df, analysis_name, duration)\n",
    "print(f\"Analysis took: {duration}\")\n",
    "print(f\"Analysis took: {total_seconds:.2f} seconds\")\n",
    "print(f\"Analysis took: {total_seconds/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07345b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93d8c4bc-ec18-4a6d-b59a-2656f485274c",
   "metadata": {},
   "source": [
    "## Save time analyses: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b9b61-1df5-4ee6-999c-f2c2f302a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define file name: \n",
    "current_date_time = current_timestamp.strftime('%Y-%m-%d %H:%M')\n",
    "time_analyses_file_name = 'times_prompt_exp_nat_minicpm_' + current_date_time + '.pkl'\n",
    "\n",
    "# Save dictionary with LLM responses:\n",
    "time_analyses_output_path = os.path.join(data_path, time_analyses_file_name)\n",
    "with open(time_analyses_output_path, 'wb') as f:\n",
    "   pickle.dump(time_analyses, f)\n",
    "\n",
    "# Reload saved dictionary to check if saving worked:\n",
    "with open(time_analyses_output_path, 'rb') as f:\n",
    "   reloaded_time_analyses = pickle.load(f)\n",
    "\n",
    "# Check if original and reloaded dictionary are the same:\n",
    "print(len(time_analyses))\n",
    "print(type(time_analyses))\n",
    "print(type(reloaded_time_analyses))\n",
    "print(len(reloaded_time_analyses))\n",
    "\n",
    "print(time_analyses.keys() == reloaded_time_analyses.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d753bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b36253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define file name: \n",
    "current_date_time = current_timestamp.strftime('%Y-%m-%d %H:%M')\n",
    "time_analyses_df_file_name = 'times_df_prompt_exp_nat_minicpm_' + current_date_time + '.pkl'\n",
    "\n",
    "# Save dictionary with LLM responses:\n",
    "time_analyses_df_output_path = os.path.join(data_path, time_analyses_df_file_name)\n",
    "with open(time_analyses_df_output_path, 'wb') as f:\n",
    "   pickle.dump(time_analyses_for_df, f)\n",
    "\n",
    "# Reload saved dictionary to check if saving worked:\n",
    "with open(time_analyses_df_output_path, 'rb') as f:\n",
    "   reloaded_time_analyses_for_df = pickle.load(f)\n",
    "\n",
    "# Check if original and reloaded dictionary are the same:\n",
    "print(len(time_analyses_for_df))\n",
    "print(type(time_analyses_for_df))\n",
    "print(type(reloaded_time_analyses_for_df))\n",
    "print(len(reloaded_time_analyses_for_df))\n",
    "\n",
    "print(time_analyses_for_df.keys() == reloaded_time_analyses_for_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c26f4-cc59-491d-a3d2-dc45ef03cf9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a5430-02fe-42d1-ad28-241c5edf48dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9704fd8-042c-437f-b552-5bd05b686f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ea35c-0969-4df8-932c-9dbe13ec236a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9304483-e32e-472e-accb-87f03c2faca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288875be-4770-42a5-b553-24f4e88343c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2075db43-973d-4e17-a4a0-aba0a1ce6d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07354e8c-7afd-4176-9afe-b35125ed2235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a87b5d-3119-4d38-8efe-05b7635a89a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-test-clean)",
   "language": "python",
   "name": "venv-test-clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
