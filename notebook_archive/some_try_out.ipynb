{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d21bd-449f-4791-864c-c68bdea727f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from source import image_id_converter as img_idc\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad71769-089c-4014-b2b5-7a73e318385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def load_image_paths(image_directory, extensions=('.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "    \"\"\"\n",
    "    Load all image file paths from a directory.\n",
    "    \n",
    "    Args:\n",
    "        image_directory (str): Path to directory containing images\n",
    "        extensions (tuple): Allowed file extensions\n",
    "        \n",
    "    Returns:\n",
    "        list: Sorted list of image file paths\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    image_dir = Path(image_directory)\n",
    "    \n",
    "    if not image_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {image_directory} does not exist\")\n",
    "    \n",
    "    for ext in extensions:\n",
    "        # Find files with current extension (case insensitive)\n",
    "        image_paths.extend(image_dir.glob(f\"*{ext}\"))\n",
    "        image_paths.extend(image_dir.glob(f\"*{ext.upper()}\"))\n",
    "    \n",
    "    # Sort paths to ensure consistent ordering\n",
    "    image_paths = sorted([str(path) for path in image_paths])\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images in {image_directory}\")\n",
    "    return image_paths\n",
    "\n",
    "def analyze_image_sizes(image_directory, extensions=('.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "    \"\"\"\n",
    "    Analyze image sizes in a directory to help determine appropriate target_size.\n",
    "    \n",
    "    Args:\n",
    "        image_directory (str): Path to directory containing images\n",
    "        extensions (tuple): Allowed file extensions\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing size analysis results\n",
    "    \"\"\"\n",
    "    # Get all image paths\n",
    "    image_paths = load_image_paths(image_directory, extensions)\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(\"No images found in directory\")\n",
    "        return None\n",
    "    \n",
    "    sizes = []\n",
    "    widths = []\n",
    "    heights = []\n",
    "    failed_images = []\n",
    "    \n",
    "    print(f\"Analyzing {len(image_paths)} images...\")\n",
    "    \n",
    "    # Analyze each image\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "                sizes.append((width, height))\n",
    "                widths.append(width)\n",
    "                heights.append(height)\n",
    "        except Exception as e:\n",
    "            failed_images.append((image_path, str(e)))\n",
    "            print(f\"Failed to read {image_path}: {e}\")\n",
    "        \n",
    "        # Progress indicator for large datasets\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(image_paths)} images...\")\n",
    "    \n",
    "    if not sizes:\n",
    "        print(\"No valid images found\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate statistics\n",
    "    unique_sizes = list(set(sizes))\n",
    "    all_same_size = len(unique_sizes) == 1\n",
    "    \n",
    "    min_width = min(widths)\n",
    "    max_width = max(widths)\n",
    "    avg_width = sum(widths) / len(widths)\n",
    "    \n",
    "    min_height = min(heights)\n",
    "    max_height = max(heights)\n",
    "    avg_height = sum(heights) / len(heights)\n",
    "    \n",
    "    min_size = (min_width, min_height)\n",
    "    max_size = (max_width, max_height)\n",
    "    avg_size = (avg_width, avg_height)\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'total_images': len(image_paths),\n",
    "        'valid_images': len(sizes),\n",
    "        'failed_images': len(failed_images),\n",
    "        'all_same_size': all_same_size,\n",
    "        'unique_sizes_count': len(unique_sizes),\n",
    "        'min_size': min_size,\n",
    "        'max_size': max_size,\n",
    "        'avg_size': avg_size,\n",
    "        'min_width': min_width,\n",
    "        'max_width': max_width,\n",
    "        'avg_width': avg_width,\n",
    "        'min_height': min_height,\n",
    "        'max_height': max_height,\n",
    "        'avg_height': avg_height,\n",
    "        'failed_images': failed_images\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"IMAGE SIZE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total images found: {results['total_images']}\")\n",
    "    print(f\"Valid images: {results['valid_images']}\")\n",
    "    print(f\"Failed to read: {results['failed_images']}\")\n",
    "    print(f\"\\nAll images same size: {'Yes' if all_same_size else 'No'}\")\n",
    "    print(f\"Number of unique sizes: {results['unique_sizes_count']}\")\n",
    "    \n",
    "    print(f\"\\nSize ranges:\")\n",
    "    print(f\"  Minimum size: {min_size[0]} x {min_size[1]}\")\n",
    "    print(f\"  Maximum size: {max_size[0]} x {max_size[1]}\")\n",
    "    print(f\"  Average size: {avg_size[0]:.1f} x {avg_size[1]:.1f}\")\n",
    "    \n",
    "    print(f\"\\nWidth range: {min_width} - {max_width} (avg: {avg_width:.1f})\")\n",
    "    print(f\"Height range: {min_height} - {max_height} (avg: {avg_height:.1f})\")\n",
    "    \n",
    "    if results['failed_images']:\n",
    "        print(f\"\\nFailed images:\")\n",
    "        for path, error in results['failed_images'][:5]:  # Show first 5 failures\n",
    "            print(f\"  {path}: {error}\")\n",
    "        if len(results['failed_images']) > 5:\n",
    "            print(f\"  ... and {len(results['failed_images']) - 5} more\")\n",
    "    \n",
    "    # Suggest target size\n",
    "    if all_same_size:\n",
    "        print(f\"\\nRecommendation: Use target_size={min_size} (all images are the same size)\")\n",
    "    else:\n",
    "        # Suggest a reasonable target size based on minimum dimensions\n",
    "        suggested_size = min(min_width, min_height)\n",
    "        # Round to common sizes\n",
    "        common_sizes = [28, 32, 64, 128, 224, 256, 512]\n",
    "        suggested_size = min(common_sizes, key=lambda x: abs(x - suggested_size))\n",
    "        print(f\"\\nRecommendation: Consider target_size=({suggested_size}, {suggested_size})\")\n",
    "        print(f\"  (Based on minimum dimension and common image sizes)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def convert_to_grayscale(image):\n",
    "    \"\"\"\n",
    "    Convert PIL image to grayscale.\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Grayscale image\n",
    "    \"\"\"\n",
    "    return image.convert('L')\n",
    "\n",
    "\n",
    "def apply_aging_effect(image):\n",
    "    \"\"\"\n",
    "    Apply aging effects to PIL image (adapted from your existing function).\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Aged image\n",
    "    \"\"\"\n",
    "    # Ensure image is in RGB mode for aging effects\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Heavy JPEG compression using temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:\n",
    "        temp_path = temp_file.name\n",
    "        image.save(temp_path, 'JPEG', quality=15)\n",
    "        image = Image.open(temp_path)\n",
    "        os.unlink(temp_path)  # Clean up temp file\n",
    "    \n",
    "    # Significant brightness reduction\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    image = enhancer.enhance(0.8)\n",
    "    \n",
    "    # Low contrast\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    image = enhancer.enhance(0.9)\n",
    "    \n",
    "    # Add significant noise\n",
    "    img_array = np.array(image)\n",
    "    noise = np.random.normal(0, 0.1, img_array.shape).astype(np.uint8)\n",
    "    img_array = np.clip(img_array.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "    image = Image.fromarray(img_array)\n",
    "    \n",
    "    # Strong blur\n",
    "    image = image.filter(ImageFilter.GaussianBlur(radius=1.5))\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(28, 28), convert_grayscale=True, apply_aging=False):\n",
    "    \"\"\"\n",
    "    Load and preprocess a single image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to image file\n",
    "        target_size (tuple): Target size for resizing (width, height)\n",
    "        convert_grayscale (bool): Whether to convert to grayscale\n",
    "        apply_aging (bool): Whether to apply aging effects\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Preprocessed image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Apply aging effects first (works best on RGB images)\n",
    "        if apply_aging:\n",
    "            image = apply_aging_effect(image)\n",
    "        \n",
    "        # Convert to grayscale if requested\n",
    "        if convert_grayscale:\n",
    "            image = convert_to_grayscale(image)\n",
    "        \n",
    "        # Resize to target size\n",
    "        image = image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_transforms(mean=0.5, std=1.0):\n",
    "    \"\"\"\n",
    "    Create image transforms matching your MNIST setup.\n",
    "    \n",
    "    Args:\n",
    "        mean (float): Normalization mean\n",
    "        std (float): Normalization standard deviation\n",
    "        \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: Transform pipeline\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean,), (std,))\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def create_label_transform():\n",
    "    \"\"\"\n",
    "    Create label transform matching your MNIST setup.\n",
    "    \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: Label transform pipeline\n",
    "    \"\"\"\n",
    "    label_transform = transforms.Compose([\n",
    "        lambda x: torch.LongTensor([x])\n",
    "    ])\n",
    "    return label_transform\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class that mimics torchvision.datasets.MNIST structure.\n",
    "    \n",
    "    This dataset loads images lazily (on-demand) and applies preprocessing\n",
    "    and transforms similar to how MNIST dataset works.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, target_size=(28, 28), \n",
    "                 convert_grayscale=True, apply_aging=False,\n",
    "                 transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            image_paths (list): List of image file paths\n",
    "            labels (list): List of integer labels corresponding to images\n",
    "            target_size (tuple): Target size for resizing images\n",
    "            convert_grayscale (bool): Whether to convert images to grayscale\n",
    "            apply_aging (bool): Whether to apply aging effects\n",
    "            transform (callable): Transform to apply to images\n",
    "            target_transform (callable): Transform to apply to labels\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.target_size = target_size\n",
    "        self.convert_grayscale = convert_grayscale\n",
    "        self.apply_aging = apply_aging\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Validate that we have equal number of images and labels\n",
    "        if len(image_paths) != len(labels):\n",
    "            raise ValueError(f\"Number of images ({len(image_paths)}) must match number of labels ({len(labels)})\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, label) where image is a tensor and label is a tensor\n",
    "        \"\"\"\n",
    "        # Load and preprocess image (lazy loading)\n",
    "        image = load_and_preprocess_image(\n",
    "            self.image_paths[idx],\n",
    "            target_size=self.target_size,\n",
    "            convert_grayscale=self.convert_grayscale,\n",
    "            apply_aging=self.apply_aging\n",
    "        )\n",
    "        \n",
    "        # Handle case where image loading failed\n",
    "        if image is None:\n",
    "            # Return a black image as fallback\n",
    "            if self.convert_grayscale:\n",
    "                image = Image.new('L', self.target_size, 0)\n",
    "            else:\n",
    "                image = Image.new('RGB', self.target_size, (0, 0, 0))\n",
    "        \n",
    "        # Get label\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "def create_image_dataset(image_paths, labels, target_size=(28, 28),\n",
    "                        convert_grayscale=True, apply_aging=False,\n",
    "                        mean=0.5, std=1.0):\n",
    "    \"\"\"\n",
    "    Create a complete image dataset ready for use with DataLoader.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of image file paths\n",
    "        labels (list): List of integer labels for the images\n",
    "        target_size (tuple): Target size for resizing images\n",
    "        convert_grayscale (bool): Whether to convert images to grayscale\n",
    "        apply_aging (bool): Whether to apply aging effects\n",
    "        mean (float): Normalization mean\n",
    "        std (float): Normalization standard deviation\n",
    "        \n",
    "    Returns:\n",
    "        CustomImageDataset: Dataset ready for use with DataLoader\n",
    "    \"\"\"\n",
    "    # Step 1: Create transforms\n",
    "    transform = create_transforms(mean=mean, std=std)\n",
    "    target_transform = create_label_transform()\n",
    "    \n",
    "    # Step 2: Create dataset\n",
    "    dataset = CustomImageDataset(\n",
    "        image_paths=image_paths,\n",
    "        labels=labels,\n",
    "        target_size=target_size,\n",
    "        convert_grayscale=convert_grayscale,\n",
    "        apply_aging=apply_aging,\n",
    "        transform=transform,\n",
    "        target_transform=target_transform\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b864d1b-137e-459d-b759-d5e33205fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(valid_loader):\n",
    "  # 1. get numpy array of all validation images:\n",
    "  val_images_noisy = []\n",
    "  val_images = []\n",
    "  val_labels = []\n",
    "\n",
    "  for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "      val_images_noisy.append(noisy_data.detach().cpu().numpy())\n",
    "      val_images.append(data.detach().cpu().numpy())\n",
    "      val_labels.append(target.detach().cpu().numpy())\n",
    "\n",
    "  val_images_noisy = np.concatenate(val_images_noisy, axis=0)\n",
    "  val_images = np.concatenate(val_images, axis=0)\n",
    "  val_labels = np.concatenate(val_labels, axis=0)\n",
    "\n",
    "  # 2. get numpy array of balanced validation samples for visualization:\n",
    "  sample_images_noisy = []\n",
    "  sample_images = []\n",
    "  sample_labels = []\n",
    "  single_el_idx = []  # indexes of single element per class for visualization\n",
    "\n",
    "  n_class = np.max(val_labels) + 1\n",
    "  # Determines the number of classes (for MNIST, this would be 10, representing digits 0-9).\n",
    "  for class_idx in range(n_class):\n",
    "    map_c = val_labels == class_idx\n",
    "\n",
    "    ims_c_noisy = val_images_noisy[map_c]\n",
    "    ims_c = val_images[map_c]\n",
    "    # For each class:\n",
    "       # Creates a boolean mask map_c identifying all samples of the current class.\n",
    "       # Extracts noisy and clean images for just this class.\n",
    "      \n",
    "\n",
    "    samples_idx = np.random.choice(len(ims_c), N_SAMPLE, replace=False)\n",
    "\n",
    "    ims_c_noisy_samples = ims_c_noisy[samples_idx]\n",
    "    ims_c_samples = ims_c[samples_idx]\n",
    "    # Randomly selects N_SAMPLE images from the current class.\n",
    "    # replace=False ensures no duplicates are selected.\n",
    "    # Extracts both noisy and clean versions of these sampled images.\n",
    "      \n",
    "\n",
    "    sample_images_noisy.append(ims_c_noisy_samples)\n",
    "    sample_images.append(ims_c_samples)\n",
    "\n",
    "    sample_labels.append([class_idx]*N_SAMPLE)\n",
    "\n",
    "    # Adds the sampled noisy images, clean images, and labels to their respective lists.\n",
    "    # Creates an array of N_SAMPLE repeated labels for this class.\n",
    "\n",
    "    start_idx = N_SAMPLE*class_idx\n",
    "    single_el_idx.extend([start_idx + i for i in range(min(N_VIS_SAMPLE, N_SAMPLE))])\n",
    "    # Calculates the indices for the first N_VIS_SAMPLE elements of this class in the final concatenated array.\n",
    "    # These indices will be used to extract a smaller subset for visualization.\n",
    "\n",
    "    \n",
    "  sample_images_noisy = np.concatenate(sample_images_noisy, axis=0)\n",
    "  sample_images = np.concatenate(sample_images, axis=0)\n",
    "  sample_labels = np.concatenate(sample_labels, axis=0)\n",
    "  single_el_idx = np.array(single_el_idx)\n",
    "  #Combines all class samples into single arrays.\n",
    "  #Converts the index list to a NumPy array.\n",
    "\n",
    "  samples = {\n",
    "      'images_noisy': sample_images_noisy,\n",
    "      'images': sample_images,\n",
    "      'labels': sample_labels,\n",
    "      'single_el_idx': single_el_idx\n",
    "\n",
    "  }\n",
    "  return samples\n",
    "# Creates and returns a dictionary with all collected samples.\n",
    "\n",
    "\n",
    "# This function ensures we have:\n",
    "# \n",
    "# A balanced number of samples for each class (equal representation)\n",
    "# Both noisy and clean versions of each image\n",
    "# A mapping between the noisy and clean versions\n",
    "# A subset of indices for visualization purposes\n",
    "# This is particularly useful for creating visualizations that show how the model behaves across different classes, or for comparing reconstruction quality across digits.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5cb334-e7b1-43b0-92a4-f9ed72f9e1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e883e2c4-2091-4f34-9f2f-2597fe18ee05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853c23f6-0eb8-445d-8f5c-1bf20d3bca78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d377878-e6d0-4295-a61e-3cc373cfe9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = 7\n",
    "b = 1\n",
    "x = 0\n",
    "1 / (1 + np.exp(-(a + b*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb83494-2595-4611-83d2-f799d0b76ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27b6c4-f40f-49e3-9a99-7c6a2827fd86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "842da123-a7df-4d7c-b690-423d34cbc58b",
   "metadata": {},
   "source": [
    "## Define file paths: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15792160-d2cf-4e75-8c30-1db400d894cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project_path = Path.cwd()\n",
    "root_path = (project_path / '..').resolve()\n",
    "\n",
    "# Define paths\n",
    "image_dir = root_path/'data'  # Replace with your directory containing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4f92e-d459-4aa1-b06b-169feed9c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analyze_image_sizes(image_dir, extensions=('.jpg', '.jpeg', '.tif', '.tiff'))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd8491-c9c6-447b-a786-1759cb4d1004",
   "metadata": {},
   "source": [
    "## Load image ids and labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fd3471-3051-492c-bcdd-daf0b871f1af",
   "metadata": {},
   "source": [
    "#### Using 'recognisable' as a label (indicating if a person or several persons recognisable as such are in the image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f14f4-2bb0-456e-b839-b50c3e77211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_without_person = pd.read_csv(image_dir/'with_without_person_mod.csv')\n",
    "with_without_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268431c-6369-49ef-92bc-f7f42cc5ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = list(with_without_person.image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca30034-f389-4cf1-a0e2-ec39ad420b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_without_person['image_id'] = img_idc.reconvert_image_ids(img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda925fe-2c6b-4ea6-acf2-0ad91dd1a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_without_person.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8fbbe-2d90-41af-be9a-5f7ac8c588d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5082d22d-d2e5-4888-93a7-ef907443e360",
   "metadata": {},
   "source": [
    "## Load image paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29479be-ea3b-46da-93c1-1012b9d413ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = load_image_paths(image_dir)\n",
    "image_paths[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d392c84-2982-408e-82bc-4c56e8b1affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edffc41-d4ce-42b4-8c32-b361db1227f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = []\n",
    "for image_path in image_paths:\n",
    "    path_str = str(image_path)\n",
    "    parts = path_str.split('.tif')\n",
    "    img_id = parts[-2][-3:]\n",
    "    img_ids.append(img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cec064-3e9e-49ae-956f-28a906497275",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de3fd8-4dce-4262-910a-71319137f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_for_mapping = pd.DataFrame({'image_id': img_ids, 'image_paths': image_paths})\n",
    "image_paths_for_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfcc6b8-f476-4bd6-bede-7ff54209b4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db35ac35-7129-413e-a609-7f1766ca212b",
   "metadata": {},
   "source": [
    "## Map image paths to labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786ddb3-00c8-4bb0-8f37-d2c54cfc44b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_labels_mapping = with_without_person.merge(image_paths_for_mapping, how='inner', on='image_id')\n",
    "ids_labels_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ddaf2e-826a-4613-86dd-9ed27fdf2a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(ids_labels_mapping.recognisable)\n",
    "print(type(labels))\n",
    "print(labels[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85665c35-7c26-47c1-8058-6bb2eb4a7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = list(ids_labels_mapping.image_paths)\n",
    "print(type(image_paths))\n",
    "print(image_paths[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f0003-3e8f-4881-81be-58df32c27fc2",
   "metadata": {},
   "source": [
    "## Test convert_to_grayscale function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4bdb18-2c29-49d8-b372-cd2dffb013d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and process image\n",
    "image_path = image_paths[100]\n",
    "original = Image.open(image_path)\n",
    "processed = convert_to_grayscale(original)  # Or any other function\n",
    "\n",
    "# Plot side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(original)\n",
    "ax1.set_title('Original')\n",
    "ax1.axis('off')\n",
    "\n",
    "#ax2.imshow(processed, cmap='gray' if processed.mode == 'L' else None)\n",
    "ax2.imshow(processed)\n",
    "ax2.set_title('After convert_to_grayscale')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79137d6b-bd44-49ee-ae94-00dc66cef0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c237f77d-5e90-47f2-bf65-f9778a02186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = convert_to_grayscale(original)\n",
    "original_array = np.array(original)\n",
    "processed_array = np.array(processed)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6806a7-16e7-4e96-b404-0902c81bf829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values\n",
    "print(f\"Shape: {original_array.shape}\")\n",
    "print(f\"Data type: {original_array.dtype}\")\n",
    "print(f\"Min value: {original_array.min()}\")\n",
    "print(f\"Max value: {original_array.max()}\")\n",
    "print(f\"Mean value: {original_array.mean():.2f}\")\n",
    "print(f\"Unique values (first 10): {np.unique(original_array)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14048f-c339-4095-8c16-37c2c4106ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values\n",
    "print(f\"Shape: {processed_array.shape}\")\n",
    "print(f\"Data type: {processed_array.dtype}\")\n",
    "print(f\"Min value: {processed_array.min()}\")\n",
    "print(f\"Max value: {processed_array.max()}\")\n",
    "print(f\"Mean value: {processed_array.mean():.2f}\")\n",
    "print(f\"Unique values (first 10): {np.unique(processed_array)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303a8f2-bb24-44e1-8cf6-aabf058dfe99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9740b09-8164-4e35-b7e6-ee12abc424ef",
   "metadata": {},
   "source": [
    "## Test the apply_aging_effect function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca5c66-07be-425e-8835-eb7a3099c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and process image\n",
    "image_path = image_paths[0]\n",
    "original = Image.open(image_path)\n",
    "processed = apply_aging_effect(original)  # Or any other function\n",
    "\n",
    "# Plot side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(original)\n",
    "ax1.set_title('Original')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(processed, cmap='gray' if processed.mode == 'L' else None)\n",
    "ax2.set_title('After aging effect')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45159172-8d06-4135-8a45-69075e11d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_array = np.array(original)\n",
    "processed_array = np.array(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefe340-051a-432b-8ad7-971bf7475dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(original_array.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Histogram of Pixel Values')\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735043c6-b1ca-415e-9866-7d2bed542897",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(processed_array.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Histogram of Pixel Values')\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9fb687-e6ce-495b-8f46-6e50debf1db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbae1d4-ec61-4b45-843f-8cd820bfea15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22207e-ac0a-4875-b4d8-0fe322c935fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = load_and_preprocess_image(image_paths[0], target_size=(512, 512), convert_grayscale=True, apply_aging=False)\n",
    "type(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f503ea3b-4702-4143-bf9d-780d0a160637",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e5928-0b1f-4380-b2b2-6348f24f3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_2 = load_and_preprocess_image(image_paths[0], target_size=(512, 512), convert_grayscale=True, apply_aging=True)\n",
    "type(test_image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8f795-9ed0-4321-ab2e-477d907f343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image_2, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5713c1-2d02-48e7-ac4b-1a91cf89f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(test_image, cmap='gray')\n",
    "ax1.set_title('Original')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(test_image_2, cmap='gray')\n",
    "ax2.set_title('After aging')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e42531e-da59-4605-8406-10a862fde0dc",
   "metadata": {},
   "source": [
    "## Test target_size parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4780f5c3-e258-49a8-b1bd-d2e63f97c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_3 = load_and_preprocess_image(image_paths[0], target_size=(50, 50), convert_grayscale=True, apply_aging=True)\n",
    "type(test_image_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24264f8e-416c-410a-a9bf-4c36e22c24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image_3, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee6a64-169c-479c-ae2f-83ad7200c855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec38dd-1b69-4e1c-b7cb-5ba48aa1024e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65748adf-a58f-44e0-9496-2dfe1415c74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf40f4-3525-4577-84a3-e79d0633d472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5612cd-adcd-43c3-b2f8-3e87ea11fb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a8eac4-7e84-448d-95c4-1237b5856c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the pipeline\n",
    "\n",
    "# Assuming you have:\n",
    "# - Images in './my_images/' directory\n",
    "# - Labels as a list of integers\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = create_image_dataset(\n",
    "    image_paths=image_paths,\n",
    "    labels=labels,\n",
    "    target_size=(512, 512),\n",
    "    convert_grayscale=True,\n",
    "    apply_aging=False,\n",
    "    mean=0.5,\n",
    "    std=1.0\n",
    ")\n",
    "\n",
    "# Test the dataset\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "\n",
    "# Get first sample\n",
    "if len(dataset) > 0:\n",
    "    image, label = dataset[0]\n",
    "    print(f\"Image shape: {image.shape}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Image dtype: {image.dtype}\")\n",
    "    print(f\"Label dtype: {label.dtype}\")\n",
    "\n",
    "# Create DataLoader (same as your MNIST setup)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# You can use your existing collate function\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# Test with one batch\n",
    "for batch in train_loader:\n",
    "    images, labels = batch\n",
    "    print(f\"Batch images shape: {images.shape}\")\n",
    "    print(f\"Batch labels shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e67fb4-aab9-450b-abc5-a6525e98e396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf9f81-06d7-4412-8b36-6e1a269c20bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code compares the pixel value distributions of an input image and its reconstruction. \n",
    "# Here's what each line does:\n",
    "\n",
    "x = xns[0]# - y[1]\n",
    "# Selects the first image from the batch of noisy inputs.\n",
    "# Note that there's a commented-out subtraction (# - y[1])\n",
    "\n",
    "d = y[0]# - y[1]\n",
    "# Selects the first image from the batch of reconstructed outputs.\n",
    "# Again, there's a commented-out subtraction\n",
    "\n",
    "im0 = x[0].detach().cpu().numpy()\n",
    "# Takes the first channel of the selected input image\n",
    "# Detaches it from the computation graph (no gradients needed)\n",
    "# Moves it to CPU if it was on GPU\n",
    "# Converts it to a NumPy array\n",
    "\n",
    "im1 = d[0].detach().cpu().numpy()\n",
    "# Does the same conversion process for the reconstructed image\n",
    "\n",
    "# plt.imshow(im, cmap='gray', vmin=-1, vmax=1)\n",
    "# This is a commented-out visualization that would display the image\n",
    "\n",
    "bins = np.linspace(-1, 1, 100)\n",
    "# Creates 100 evenly spaced histogram bins from -1 to 1\n",
    "# This range is chosen to match the expected range of pixel values\n",
    "\n",
    "plt.hist(im0.flatten(), bins, alpha=0.3);\n",
    "# Creates a histogram of all pixel values in the input image\n",
    "# flatten() converts the 2D image to a 1D array\n",
    "# alpha=0.3 makes the histogram semi-transparent\n",
    "\n",
    "plt.hist(im1.flatten(), bins, alpha=0.3);\n",
    "# Creates a histogram of all pixel values in the reconstructed image\n",
    "# Using the same bins and transparency\n",
    "# Overlaid on the same plot as the input image histogram\n",
    "\n",
    "\n",
    "# This visualization allows comparing the distribution of pixel values between \n",
    "# the noisy input and the reconstruction. It helps assess how well the autoencoder \n",
    "# is preserving the overall pixel value distribution and whether \n",
    "# it's correctly mapping values from the input distribution to the expected output distribution.\n",
    "# The semi-transparent overlapping histograms make it easy to see differences \n",
    "# in how pixel values are distributed between the original and reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d5fc9-61b0-49c7-b2df-fbb68918712d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a34d4-082a-416c-9d52-4dfccd51e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_samples(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a63a5-debb-4dda-a54d-9805f25dfbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
