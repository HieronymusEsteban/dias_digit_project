{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea044f75-a73a-4195-b899-10c09f1866b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from source.visual_genome_meta_data import read_json_to_dict\n",
    "from source.visual_genome_meta_data import get_image_meta_data\n",
    "from source.visual_genome_data import count_occurrences\n",
    "from source.visual_genome_to_yolo import create_class_mapping_from_list\n",
    "from source.visual_genome_to_yolo import save_class_map_to_yaml\n",
    "from source.visual_genome_to_yolo import convert_single_image_to_yolo\n",
    "from source.visual_genome_to_yolo import read_yaml_to_class_map\n",
    "from source.visual_genome_to_yolo import read_yolo_metadata\n",
    "from source.visual_genome_to_yolo import visual_genome_to_yolo_data_n\n",
    "from source.visual_genome_meta_data import plot_image_with_multiple_bboxes\n",
    "from source.visual_genome_data import get_image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea629f9-2244-4086-aa49-7927fc224d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5c511-24a1-4891-a104-25674a4bdc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62daa60f-ce53-4b52-a71b-950fb050c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e1a3ca0-d4bc-48a4-9d11-9779425d0193",
   "metadata": {},
   "source": [
    "\n",
    "def train_yolo_on_single_class(\n",
    "    data_dir,          # Directory with your dataset\n",
    "    class_yaml_path,   # Path to your class_map.yaml file\n",
    "    target_class,      # The single class to train on (e.g., 'mountain')\n",
    "    output_dir,        # Directory to save results\n",
    "    epochs=50,         # Number of training epochs\n",
    "    img_size=640,      # Input image size\n",
    "    batch_size=16      # Batch size\n",
    "):\n",
    "    \"\"\"\n",
    "    Train YOLO v11 on a single class from your dataset.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Load the full class map\n",
    "    with open(class_yaml_path, 'r') as f:\n",
    "        class_data = yaml.safe_load(f)\n",
    "    \n",
    "    # Find the index of the target class\n",
    "    all_classes = class_data['names']\n",
    "    if target_class not in all_classes:\n",
    "        raise ValueError(f\"Class '{target_class}' not found in class map. Available classes: {all_classes}\")\n",
    "    \n",
    "    target_class_id = all_classes.index(target_class)\n",
    "    print(f\"Training on class: {target_class} (class_id: {target_class_id})\")\n",
    "    \n",
    "    # 2. Create a new YAML file for single-class training\n",
    "    single_class_yaml = os.path.join(output_dir, f\"single_class_{target_class}.yaml\")\n",
    "    \n",
    "    # Update dataset configuration\n",
    "    train_path = os.path.join(data_dir, 'train/images')\n",
    "    val_path = os.path.join(data_dir, 'val/images')\n",
    "    \n",
    "    single_class_config = {\n",
    "        'path': os.path.abspath(data_dir),\n",
    "        'train': 'train/images',\n",
    "        'val': 'val/images',\n",
    "        'nc': 1,  # Just one class\n",
    "        'names': [target_class]  # Only the target class\n",
    "    }\n",
    "    \n",
    "    with open(single_class_yaml, 'w') as f:\n",
    "        yaml.dump(single_class_config, f)\n",
    "    \n",
    "    print(f\"Created single-class configuration at: {single_class_yaml}\")\n",
    "    \n",
    "    # 3. Initialize YOLO model\n",
    "    model = YOLO(\"yolo11n.pt\")  # Using YOLOv11 nano (smallest model)\n",
    "    \n",
    "    # 4. Train the model\n",
    "    results = model.train(\n",
    "        data=single_class_yaml,\n",
    "        epochs=epochs,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        project=output_dir,\n",
    "        name=f\"yolov11_{target_class}\",\n",
    "        exist_ok=True,\n",
    "        patience=15,  # Early stopping\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Training completed. Model saved to {output_dir}/yolov11_{target_class}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900b860-c9d9-449d-851f-cfa9ded17f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo_on_single_class(\n",
    "    data_dir,          # Directory with your dataset\n",
    "    class_yaml_path,   # Path to your class_map.yaml file\n",
    "    target_class,      # The single class to train on (e.g., 'mountain')\n",
    "    output_dir,        # Directory to save results\n",
    "    epochs=50,         # Number of training epochs\n",
    "    img_size=640,      # Input image size\n",
    "    batch_size=16,     # Batch size\n",
    "    device=None        # Device for training - will auto-select MPS if available\n",
    "):\n",
    "    \"\"\"\n",
    "    Train YOLO v11 on a single class from your dataset using M1 Mac's MPS acceleration.\n",
    "    \"\"\"\n",
    "    # import os\n",
    "    # import yaml\n",
    "    # import torch\n",
    "    # from ultralytics import YOLO  # Assuming this is the import for YOLOv11\n",
    "    \n",
    "    # Auto-select the best available device with priority for MPS on Mac\n",
    "    if device is None:\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = 'mps'\n",
    "            print(f\"Using MPS acceleration on Apple Silicon\")\n",
    "        elif torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "            print(f\"Using CUDA acceleration\")\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "            print(f\"Using CPU for training (this will be slow)\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Load the full class map\n",
    "    with open(class_yaml_path, 'r') as f:\n",
    "        class_data = yaml.safe_load(f)\n",
    "    \n",
    "    # Find the index of the target class\n",
    "    all_classes = class_data['names']\n",
    "    if target_class not in all_classes:\n",
    "        raise ValueError(f\"Class '{target_class}' not found in class map. Available classes: {all_classes}\")\n",
    "    \n",
    "    target_class_id = all_classes.index(target_class)\n",
    "    print(f\"Training on class: {target_class} (class_id: {target_class_id})\")\n",
    "    \n",
    "    # 2. Create a new YAML file for single-class training\n",
    "    single_class_yaml = os.path.join(output_dir, f\"single_class_{target_class}.yaml\")\n",
    "    \n",
    "    # Update dataset configuration\n",
    "    train_path = os.path.join(data_dir, 'train/images')\n",
    "    val_path = os.path.join(data_dir, 'val/images')\n",
    "    \n",
    "    single_class_config = {\n",
    "        'path': os.path.abspath(data_dir),\n",
    "        'train': 'train/images',\n",
    "        'val': 'val/images',\n",
    "        'nc': 1,  # Just one class\n",
    "        'names': [target_class]  # Only the target class\n",
    "    }\n",
    "    \n",
    "    with open(single_class_yaml, 'w') as f:\n",
    "        yaml.dump(single_class_config, f)\n",
    "    \n",
    "    print(f\"Created single-class configuration at: {single_class_yaml}\")\n",
    "    \n",
    "    # 3. Initialize YOLO model\n",
    "    model = YOLO(\"yolo11n.pt\")  # Using YOLOv11 nano (smallest model)\n",
    "    \n",
    "    # 4. Train the model with MPS-specific configurations\n",
    "    results = model.train(\n",
    "        data=single_class_yaml,\n",
    "        epochs=epochs,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        project=output_dir,\n",
    "        name=f\"yolov11_{target_class}\",\n",
    "        exist_ok=True,\n",
    "        patience=0,  # Early stopping\n",
    "        verbose=True,\n",
    "        device=device,  # Specify device for training\n",
    "        amp=True       # Enable mixed precision training (improves performance on M1/M2)\n",
    "    )\n",
    "    \n",
    "    print(f\"Training completed. Model saved to {output_dir}/yolov11_{target_class}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6b27d-5dc0-45dc-a6d1-f429dbcfc8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path('/Users/stephanehess/Documents/CAS_AML/dias_digit_project')\n",
    "data_dir = root_path / 'yolo_object_train'\n",
    "class_yaml_path = data_dir / 'class_map.yaml'\n",
    "target_class = 'mountain'\n",
    "#target_class = 'church'\n",
    "output_dir = data_dir / 'output'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c002319-0e0a-41ec-a45b-e1f461d27099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def85e76-4209-4cd2-81e8-9732b19452f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "training_results = train_yolo_on_single_class(\n",
    "    data_dir=data_dir,\n",
    "    class_yaml_path=class_yaml_path,\n",
    "    target_class=target_class,\n",
    "    output_dir=output_dir,\n",
    "    epochs=30,\n",
    "    img_size=640,\n",
    "    batch_size=26  # Adjust based on your GPU memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd7f87-add4-49bd-8935-6ca873dc2dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e316569-c015-4466-b753-98c486297d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3afbce-1fd8-428d-b8c0-ffb798595966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dbaf3b-f8cf-4db0-851a-fbcf56c11c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b9afd-77a6-4bbd-9b53-5b80783663ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = data_dir / 'output/yolov11_mountain/weights/best.pt'\n",
    "#model_path = data_dir / 'output/yolov11_church/weights/best.pt'\n",
    "model_path = data_dir / 'output/yolov11_church/weights/last.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a58b19-419a-4e45-a4ef-032c5cb3a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe104c-51fd-4e6c-bdf0-23b76450e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = data_dir / 'test'\n",
    "test_images_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35934f2-0e1a-4062-bcbb-545c7d50f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(test_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b414bf-8d47-4d18-ac34-605972207067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972cc1ba-a86b-445e-8168-7b7d2245e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_yolo_model(model_path, test_images_dir, conf_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Test a trained YOLO model on a directory of test images.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the trained model weights (.pt file)\n",
    "        test_images_dir: Directory containing test images\n",
    "        conf_threshold: Confidence threshold for detections\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Get list of test images\n",
    "    test_images = [f for f in os.listdir(test_images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tif'))]\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_images = len(test_images)\n",
    "    images_with_detections = 0\n",
    "    total_detections = 0\n",
    "    \n",
    "    # Process each image\n",
    "    for i, img_file in enumerate(test_images):\n",
    "        img_path = os.path.join(test_images_dir, img_file)\n",
    "        \n",
    "        # Run inference\n",
    "        results = model(img_path, conf=conf_threshold)\n",
    "        \n",
    "        # Get detection results\n",
    "        detections = results[0].boxes\n",
    "        \n",
    "        # Count detections\n",
    "        if len(detections) > 0:\n",
    "            images_with_detections += 1\n",
    "            total_detections += len(detections)\n",
    "        \n",
    "        # Display the image with detections (every 5th image)\n",
    "        if i % 5 == 0:\n",
    "            # Get the image\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            # Create figure and axis\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            ax.imshow(np.array(img))\n",
    "            \n",
    "            # Add detections to the plot\n",
    "            for det in detections:\n",
    "                # Get bounding box coordinates (x1, y1, x2, y2)\n",
    "                box = det.xyxy[0].cpu().numpy()\n",
    "                \n",
    "                # Create rectangle patch\n",
    "                rect = patches.Rectangle(\n",
    "                    (box[0], box[1]), \n",
    "                    box[2] - box[0], \n",
    "                    box[3] - box[1], \n",
    "                    linewidth=2, \n",
    "                    edgecolor='red', \n",
    "                    facecolor='none'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add class name and confidence\n",
    "                cls_id = int(det.cls[0])\n",
    "                conf = float(det.conf[0])\n",
    "                class_name = model.names[cls_id]\n",
    "                ax.text(\n",
    "                    box[0], box[1] - 5, \n",
    "                    f\"{class_name}: {conf:.2f}\", \n",
    "                    color='red', \n",
    "                    fontsize=12, \n",
    "                    bbox=dict(facecolor='white', alpha=0.7)\n",
    "                )\n",
    "            \n",
    "            # Set title and display\n",
    "            ax.set_title(f\"Image {i+1}/{total_images}: {len(detections)} detections\")\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Processed {i+1}/{total_images} images\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nTest Results Summary:\")\n",
    "    print(f\"Total images: {total_images}\")\n",
    "    print(f\"Images with detections: {images_with_detections} ({images_with_detections/total_images*100:.1f}%)\")\n",
    "    print(f\"Total detections: {total_detections}\")\n",
    "    print(f\"Average detections per image: {total_detections/total_images:.2f}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f0066-6556-4e24-8ffa-f24e4ce83838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "confidence = 0.1                         # Confidence threshold\n",
    "\n",
    "# Run the test\n",
    "test_results = test_yolo_model(model_path, test_images_dir, confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f7ee4-ccfc-4786-80fc-c108ba4bc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295d132-b82b-4475-9533-54255bd7dad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78544d9-582a-403c-9d38-743b05a70b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10483e31-6178-483b-a82b-58cab3955526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detections_on_test_images(\n",
    "    trained_model_path,  # Path to the trained model weights (best.pt)\n",
    "    test_images_dir,     # Directory containing test images\n",
    "    output_dir,          # Directory to save visualized results\n",
    "    conf_threshold=0.25, # Confidence threshold for detections\n",
    "    device=None,         # Device to run inference on (will use MPS if available)\n",
    "    img_size=640         # Size for inference\n",
    "):\n",
    "    \"\"\"\n",
    "    Use trained YOLOv11 model to detect objects in test images and visualize results.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import glob\n",
    "    import torch\n",
    "    from ultralytics import YOLO\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Auto-select device\n",
    "    if device is None:\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = 'mps'\n",
    "            print(f\"Using MPS acceleration on Apple Silicon\")\n",
    "        elif torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "            print(f\"Using CUDA acceleration\")\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "            print(f\"Using CPU for inference\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = YOLO(trained_model_path)\n",
    "    \n",
    "    # Get list of test images\n",
    "    test_images = []\n",
    "    for ext in ['jpg', 'jpeg', 'png', 'bmp', 'tif']:\n",
    "        test_images.extend(glob.glob(os.path.join(test_images_dir, f'*.{ext}')))\n",
    "    \n",
    "    print(f\"Found {len(test_images)} test images\")\n",
    "    \n",
    "    # Process each test image\n",
    "    for img_path in test_images:\n",
    "        # Get filename without extension\n",
    "        img_name = Path(img_path).stem\n",
    "        \n",
    "        # Run inference on image\n",
    "        results = model.predict(\n",
    "            source=img_path,\n",
    "            conf=conf_threshold,\n",
    "            device=device,\n",
    "            imgsz=img_size,\n",
    "            save=True,       # Save images with detections\n",
    "            save_txt=True,   # Save results as .txt files (optional)\n",
    "            project=output_dir,\n",
    "            name='detections',\n",
    "            exist_ok=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Processed {img_name}: {len(results[0].boxes)} detections\")\n",
    "    \n",
    "    print(f\"Detection results saved to {os.path.join(output_dir, 'detections')}\")\n",
    "    print(f\"- Images with bounding boxes saved in {os.path.join(output_dir, 'detections')}\")\n",
    "    print(f\"- Detection data saved in {os.path.join(output_dir, 'detections/labels')}\")\n",
    "\n",
    "    return os.path.join(output_dir, 'detections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8b131-144f-4d86-8e82-84f4df66cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = data_dir / 'output_test'\n",
    "confidence = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556d325-cc1b-4436-b873-11287a0f2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589681e-1aef-43c6-8d9f-c8bb769a360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727452f3-9e3d-4530-a565-e4df5250c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff12008a-ace7-48f4-97cd-c40e8d902899",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = plot_detections_on_test_images(model_path, test_images_dir, output_dir, confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48522479-14b9-4297-9216-bc08d45f00ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed2717-6619-4ba8-906e-20f4e4780859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c5711-b0de-4fdd-92cf-e9e6b851c183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa6eb2-c5d8-4a56-ad37-f102af2f7c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac833d03-6be8-4f20-981a-580a749ada85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36b25be-96ca-4a1a-88ed-8a0c00d36219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
