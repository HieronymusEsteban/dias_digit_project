{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90991bc-d3a5-48a0-a815-15f359081594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from source import image_id_converter as img_idc\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f2ee2-9479-4eb2-8529-370a87703ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import einops as eo\n",
    "import pathlib as pl\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import collections  as mc\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from time import time as timer\n",
    "#import umap\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Audio\n",
    "import IPython\n",
    "\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import sys\n",
    "import einops as eo\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd076d1b-79dc-4323-b5cb-77122e68cd2f",
   "metadata": {},
   "source": [
    "### Function to Check Image Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34285644-4da5-4375-ba5e-271304d4f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_sizes(image_directory, extensions=('.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "   \"\"\"\n",
    "   Analyze image sizes in a directory to help determine appropriate target_size.\n",
    "   \n",
    "   Args:\n",
    "       image_directory (str): Path to directory containing images\n",
    "       extensions (tuple): Allowed file extensions\n",
    "       \n",
    "   Returns:\n",
    "       dict: Dictionary containing size analysis results\n",
    "   \"\"\"\n",
    "   # Get all image paths\n",
    "   image_paths = load_image_paths(image_directory, extensions)\n",
    "   \n",
    "   if not image_paths:\n",
    "       print(\"No images found in directory\")\n",
    "       return None\n",
    "   \n",
    "   sizes = []\n",
    "   widths = []\n",
    "   heights = []\n",
    "   failed_images = []\n",
    "   \n",
    "   print(f\"Analyzing {len(image_paths)} images...\")\n",
    "   \n",
    "   # Analyze each image\n",
    "   for i, image_path in enumerate(image_paths):\n",
    "       try:\n",
    "           with Image.open(image_path) as img:\n",
    "               width, height = img.size\n",
    "               sizes.append((width, height))\n",
    "               widths.append(width)\n",
    "               heights.append(height)\n",
    "       except Exception as e:\n",
    "           failed_images.append((image_path, str(e)))\n",
    "           print(f\"Failed to read {image_path}: {e}\")\n",
    "       \n",
    "       # Progress indicator for large datasets\n",
    "       if (i + 1) % 100 == 0:\n",
    "           print(f\"Processed {i + 1}/{len(image_paths)} images...\")\n",
    "   \n",
    "   if not sizes:\n",
    "       print(\"No valid images found\")\n",
    "       return None\n",
    "   \n",
    "   # Calculate statistics\n",
    "   unique_sizes = list(set(sizes))\n",
    "   all_same_size = len(unique_sizes) == 1\n",
    "   \n",
    "   min_width = min(widths)\n",
    "   max_width = max(widths)\n",
    "   avg_width = sum(widths) / len(widths)\n",
    "   \n",
    "   min_height = min(heights)\n",
    "   max_height = max(heights)\n",
    "   avg_height = sum(heights) / len(heights)\n",
    "   \n",
    "   # Calculate quartiles\n",
    "   import numpy as np\n",
    "   width_q25, width_median, width_q75 = np.percentile(widths, [25, 50, 75])\n",
    "   height_q25, height_median, height_q75 = np.percentile(heights, [25, 50, 75])\n",
    "   \n",
    "   min_size = (min_width, min_height)\n",
    "   max_size = (max_width, max_height)\n",
    "   avg_size = (avg_width, avg_height)\n",
    "   \n",
    "   # Create results dictionary\n",
    "   results = {\n",
    "       'total_images': len(image_paths),\n",
    "       'valid_images': len(sizes),\n",
    "       'failed_images': len(failed_images),\n",
    "       'all_same_size': all_same_size,\n",
    "       'unique_sizes_count': len(unique_sizes),\n",
    "       'min_size': min_size,\n",
    "       'max_size': max_size,\n",
    "       'avg_size': avg_size,\n",
    "       'min_width': min_width,\n",
    "       'max_width': max_width,\n",
    "       'avg_width': avg_width,\n",
    "       'min_height': min_height,\n",
    "       'max_height': max_height,\n",
    "       'avg_height': avg_height,\n",
    "       'width_q25': width_q25,\n",
    "       'width_median': width_median,\n",
    "       'width_q75': width_q75,\n",
    "       'height_q25': height_q25,\n",
    "       'height_median': height_median,\n",
    "       'height_q75': height_q75,\n",
    "       'failed_images': failed_images\n",
    "   }\n",
    "   \n",
    "   # Print summary\n",
    "   print(\"\\n\" + \"=\"*50)\n",
    "   print(\"IMAGE SIZE ANALYSIS SUMMARY\")\n",
    "   print(\"=\"*50)\n",
    "   print(f\"Total images found: {results['total_images']}\")\n",
    "   print(f\"Valid images: {results['valid_images']}\")\n",
    "   print(f\"Failed to read: {results['failed_images']}\")\n",
    "   print(f\"\\nAll images same size: {'Yes' if all_same_size else 'No'}\")\n",
    "   print(f\"Number of unique sizes: {results['unique_sizes_count']}\")\n",
    "   \n",
    "   print(f\"\\nSize ranges:\")\n",
    "   print(f\"  Minimum size: {min_size[0]} x {min_size[1]}\")\n",
    "   print(f\"  Maximum size: {max_size[0]} x {max_size[1]}\")\n",
    "   print(f\"  Average size: {avg_size[0]:.1f} x {avg_size[1]:.1f}\")\n",
    "   \n",
    "   print(f\"\\nWidth range: {min_width} - {max_width} (avg: {avg_width:.1f})\")\n",
    "   print(f\"Width quartiles: Q25={width_q25:.1f}, Median={width_median:.1f}, Q75={width_q75:.1f}\")\n",
    "   print(f\"Height range: {min_height} - {max_height} (avg: {avg_height:.1f})\")\n",
    "   print(f\"Height quartiles: Q25={height_q25:.1f}, Median={height_median:.1f}, Q75={height_q75:.1f}\")\n",
    "   \n",
    "   if results['failed_images']:\n",
    "       print(f\"\\nFailed images:\")\n",
    "       for path, error in results['failed_images'][:5]:  # Show first 5 failures\n",
    "           print(f\"  {path}: {error}\")\n",
    "       if len(results['failed_images']) > 5:\n",
    "           print(f\"  ... and {len(results['failed_images']) - 5} more\")\n",
    "   \n",
    "   # Suggest target size\n",
    "   if all_same_size:\n",
    "       print(f\"\\nRecommendation: Use target_size={min_size} (all images are the same size)\")\n",
    "   else:\n",
    "       # Suggest a reasonable target size based on minimum dimensions\n",
    "       suggested_size = min(min_width, min_height)\n",
    "       # Round to common sizes\n",
    "       common_sizes = [28, 32, 64, 128, 224, 256, 512]\n",
    "       suggested_size = min(common_sizes, key=lambda x: abs(x - suggested_size))\n",
    "       print(f\"\\nRecommendation: Consider target_size=({suggested_size}, {suggested_size})\")\n",
    "       print(f\"  (Based on minimum dimension and common image sizes)\")\n",
    "   \n",
    "   return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8611c3b6-2c3b-47ce-a7ea-c28f858fd1df",
   "metadata": {},
   "source": [
    "### Functions to Get Image File Paths and Split them into Training and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07443d29-c3fe-4a29-b68d-ae9d27f3844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_paths(image_directory, extensions=('.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "    \"\"\"\n",
    "    Load all image file paths from a directory.\n",
    "    \n",
    "    Args:\n",
    "        image_directory (str): Path to directory containing images\n",
    "        extensions (tuple): Allowed file extensions\n",
    "        \n",
    "    Returns:\n",
    "        list: Sorted list of image file paths\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    image_dir = Path(image_directory)\n",
    "    \n",
    "    if not image_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {image_directory} does not exist\")\n",
    "    \n",
    "    for ext in extensions:\n",
    "        # Find files with current extension (case insensitive)\n",
    "        image_paths.extend(image_dir.glob(f\"*{ext}\"))\n",
    "        image_paths.extend(image_dir.glob(f\"*{ext.upper()}\"))\n",
    "    \n",
    "    # Sort paths to ensure consistent ordering\n",
    "    image_paths = sorted([str(path) for path in image_paths])\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images in {image_directory}\")\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999202c2-9e1a-413e-88af-9694298c701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_train_validation(image_paths, labels, train_ratio=0.8, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split image paths into training and validation sets with balanced class distribution.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of image file paths\n",
    "        labels (list): List of class labels corresponding to image paths\n",
    "        train_ratio (float): Proportion for training set (default 0.8)\n",
    "        random_seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_paths, train_labels, val_paths, val_labels)\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # Group paths by class\n",
    "    class_groups = defaultdict(list)\n",
    "    for path, label in zip(image_paths, labels):\n",
    "        class_groups[label].append(path)\n",
    "    \n",
    "    train_paths, train_labels = [], []\n",
    "    val_paths, val_labels = [], []\n",
    "    \n",
    "    # Split each class maintaining proportion\n",
    "    for class_label, paths in class_groups.items():\n",
    "        # Shuffle paths for this class\n",
    "        shuffled_paths = paths.copy()\n",
    "        random.shuffle(shuffled_paths)\n",
    "        \n",
    "        # Calculate split point\n",
    "        n_train = int(len(shuffled_paths) * train_ratio)\n",
    "        \n",
    "        # Split paths\n",
    "        class_train_paths = shuffled_paths[:n_train]\n",
    "        class_val_paths = shuffled_paths[n_train:]\n",
    "        \n",
    "        # Add to final lists\n",
    "        train_paths.extend(class_train_paths)\n",
    "        train_labels.extend([class_label] * len(class_train_paths))\n",
    "        val_paths.extend(class_val_paths)\n",
    "        val_labels.extend([class_label] * len(class_val_paths))\n",
    "    \n",
    "    # Final shuffle to mix classes\n",
    "    combined_train = list(zip(train_paths, train_labels))\n",
    "    combined_val = list(zip(val_paths, val_labels))\n",
    "    random.shuffle(combined_train)\n",
    "    random.shuffle(combined_val)\n",
    "    \n",
    "    train_paths, train_labels = zip(*combined_train) if combined_train else ([], [])\n",
    "    val_paths, val_labels = zip(*combined_val) if combined_val else ([], [])\n",
    "    \n",
    "    print(f\"Training set: {len(train_paths)} images\")\n",
    "    print(f\"Validation set: {len(val_paths)} images\")\n",
    "    \n",
    "    return list(train_paths), list(train_labels), list(val_paths), list(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8d93a-c8b2-4277-8e59-d668c77c4e39",
   "metadata": {},
   "source": [
    "### Functions Data Loading Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc60fea-0551-492d-922f-59ffed17f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayscale(image):\n",
    "    \"\"\"\n",
    "    Convert PIL image to grayscale.\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Grayscale image\n",
    "    \"\"\"\n",
    "    return image.convert('L')\n",
    "\n",
    "\n",
    "def apply_aging_effect(image):\n",
    "    \"\"\"\n",
    "    Apply aging effects to PIL image (adapted from your existing function).\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Aged image\n",
    "    \"\"\"\n",
    "    # Ensure image is in RGB mode for aging effects\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Heavy JPEG compression using temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:\n",
    "        temp_path = temp_file.name\n",
    "        image.save(temp_path, 'JPEG', quality=15)\n",
    "        image = Image.open(temp_path)\n",
    "        os.unlink(temp_path)  # Clean up temp file\n",
    "    \n",
    "    # Significant brightness reduction\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    image = enhancer.enhance(0.8)\n",
    "    \n",
    "    # Low contrast\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    image = enhancer.enhance(0.9)\n",
    "    \n",
    "    # Add significant noise\n",
    "    img_array = np.array(image)\n",
    "    noise = np.random.normal(0, 0.1, img_array.shape).astype(np.uint8)\n",
    "    img_array = np.clip(img_array.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "    image = Image.fromarray(img_array)\n",
    "    \n",
    "    # Strong blur\n",
    "    image = image.filter(ImageFilter.GaussianBlur(radius=1.5))\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(28, 28), convert_grayscale=True, apply_aging=False):\n",
    "    \"\"\"\n",
    "    Load and preprocess a single image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to image file\n",
    "        target_size (tuple): Target size for resizing (width, height)\n",
    "        convert_grayscale (bool): Whether to convert to grayscale\n",
    "        apply_aging (bool): Whether to apply aging effects\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Preprocessed image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Apply aging effects first (works best on RGB images)\n",
    "        if apply_aging:\n",
    "            image = apply_aging_effect(image)\n",
    "        \n",
    "        # Convert to grayscale if requested\n",
    "        if convert_grayscale:\n",
    "            image = convert_to_grayscale(image)\n",
    "        \n",
    "        # Resize to target size\n",
    "        image = image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_transforms(mean=0.5, std=1.0):\n",
    "    \"\"\"\n",
    "    Create image transforms matching your MNIST setup.\n",
    "    \n",
    "    Args:\n",
    "        mean (float): Normalization mean\n",
    "        std (float): Normalization standard deviation\n",
    "        \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: Transform pipeline\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean,), (std,))\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def create_label_transform():\n",
    "    \"\"\"\n",
    "    Create label transform matching your MNIST setup.\n",
    "    \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: Label transform pipeline\n",
    "    \"\"\"\n",
    "    label_transform = transforms.Compose([\n",
    "        lambda x: torch.LongTensor([x])\n",
    "    ])\n",
    "    return label_transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class that exactly mimics torchvision.datasets.MNIST structure.\n",
    "    \n",
    "    This dataset replicates ALL MNIST properties including .data and .targets tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, target_size=(28, 28), \n",
    "                 convert_grayscale=True, apply_aging=False,\n",
    "                 transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with MNIST-compatible structure.\n",
    "        \n",
    "        Args:\n",
    "            image_paths (list): List of image file paths\n",
    "            labels (list): List of integer labels corresponding to images\n",
    "            target_size (tuple): Target size for resizing images\n",
    "            convert_grayscale (bool): Whether to convert images to grayscale\n",
    "            apply_aging (bool): Whether to apply aging effects\n",
    "            transform (callable): Transform to apply to images\n",
    "            target_transform (callable): Transform to apply to labels\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.target_size = target_size\n",
    "        self.convert_grayscale = convert_grayscale\n",
    "        self.apply_aging = apply_aging\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Validate that we have equal number of images and labels\n",
    "        if len(image_paths) != len(labels):\n",
    "            raise ValueError(f\"Number of images ({len(image_paths)}) must match number of labels ({len(labels)})\")\n",
    "        \n",
    "        # Create MNIST-compatible attributes\n",
    "        self.targets = torch.tensor(labels, dtype=torch.long)  # Shape: [N]\n",
    "        self._data = None  # Will be loaded when first accessed\n",
    "        \n",
    "        print(f\"Initializing dataset with {len(image_paths)} images...\")\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"\n",
    "        MNIST-compatible data property that returns all images as a tensor.\n",
    "        Shape: [N, H, W] for grayscale or [N, H, W, C] for color\n",
    "        Dtype: torch.uint8 (same as MNIST)\n",
    "        \n",
    "        Images are loaded and cached on first access.\n",
    "        \"\"\"\n",
    "        if self._data is None:\n",
    "            print(\"Loading all images into .data tensor (this may take a moment)...\")\n",
    "            self._load_all_images()\n",
    "        return self._data\n",
    "    \n",
    "    def _load_all_images(self):\n",
    "        \"\"\"Load all images into the .data tensor.\"\"\"\n",
    "        all_images = []\n",
    "        \n",
    "        for i, image_path in enumerate(self.image_paths):\n",
    "            # Load and preprocess image\n",
    "            image = load_and_preprocess_image(\n",
    "                image_path,\n",
    "                target_size=self.target_size,\n",
    "                convert_grayscale=self.convert_grayscale,\n",
    "                apply_aging=self.apply_aging\n",
    "            )\n",
    "            \n",
    "            # Handle failed loading\n",
    "            if image is None:\n",
    "                if self.convert_grayscale:\n",
    "                    image = Image.new('L', self.target_size, 0)\n",
    "                else:\n",
    "                    image = Image.new('RGB', self.target_size, (0, 0, 0))\n",
    "            \n",
    "            # Convert to numpy array with uint8 dtype (like MNIST)\n",
    "            img_array = np.array(image, dtype=np.uint8)\n",
    "            all_images.append(img_array)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Loaded {i + 1}/{len(self.image_paths)} images...\")\n",
    "        \n",
    "        # Stack all images and convert to tensor\n",
    "        all_images = np.stack(all_images, axis=0)\n",
    "        self._data = torch.from_numpy(all_images)\n",
    "        \n",
    "        print(f\"Loaded .data tensor with shape: {self._data.shape}, dtype: {self._data.dtype}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, label) where image is a tensor and label is a tensor\n",
    "            \n",
    "        Note: This applies transforms to the raw data, just like MNIST\n",
    "        \"\"\"\n",
    "        # Get raw image from .data tensor (this will load all images if needed)\n",
    "        raw_image = self.data[idx]  # Shape: [H, W] or [H, W, C]\n",
    "        \n",
    "        # Convert tensor back to PIL Image for transforms\n",
    "        if raw_image.dim() == 2:  # Grayscale\n",
    "            image = Image.fromarray(raw_image.numpy(), mode='L')\n",
    "        else:  # Color\n",
    "            image = Image.fromarray(raw_image.numpy(), mode='RGB')\n",
    "        \n",
    "        # Get label from targets tensor\n",
    "        label = self.targets[idx].item()  # Convert to Python int\n",
    "        \n",
    "        # Apply transforms (same as MNIST)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_image_dataset(image_paths, labels, target_size=(28, 28),\n",
    "                        convert_grayscale=True, apply_aging=False,\n",
    "                        mean=0.5, std=1.0):\n",
    "    \"\"\"\n",
    "    Create a complete image dataset ready for use with DataLoader.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of image file paths\n",
    "        labels (list): List of integer labels for the images\n",
    "        target_size (tuple): Target size for resizing images\n",
    "        convert_grayscale (bool): Whether to convert images to grayscale\n",
    "        apply_aging (bool): Whether to apply aging effects\n",
    "        mean (float): Normalization mean\n",
    "        std (float): Normalization standard deviation\n",
    "        \n",
    "    Returns:\n",
    "        CustomImageDataset: Dataset ready for use with DataLoader\n",
    "    \"\"\"\n",
    "    # Step 1: Create transforms\n",
    "    transform = create_transforms(mean=mean, std=std)\n",
    "    target_transform = create_label_transform()\n",
    "    \n",
    "    # Step 2: Create dataset\n",
    "    dataset = CustomImageDataset(\n",
    "        image_paths=image_paths,\n",
    "        labels=labels,\n",
    "        target_size=target_size,\n",
    "        convert_grayscale=convert_grayscale,\n",
    "        apply_aging=apply_aging,\n",
    "        transform=transform,\n",
    "        target_transform=target_transform\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2a01e-a8a1-4511-b110-554f37549158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_ae_dataset(samples):\n",
    "    \"\"\"\n",
    "    The function collates sampels into a batch, and creates noisy samples if DENOISING is True\n",
    "    for the denoising autoencoder.\n",
    "    \"\"\"\n",
    "    xs = [s[0] for s in samples]\n",
    "    ys = [s[1] for s in samples]\n",
    "    #Extracts the first element (input data) from each sample into list xs.\n",
    "    #Extracts the second element (labels or targets) from each sample into list ys.\n",
    "    #This assumes each sample is a tuple or list with at least two elements.\n",
    "    \n",
    "    xs = torch.stack(xs)\n",
    "    ys = torch.concat(ys)\n",
    "    #torch.stack(xs) combines the list of input tensors into a single \n",
    "    #tensor along a new dimension (creating a batch dimension).\n",
    "    #torch.concat(ys) concatenates the label tensors along \n",
    "    #the existing first dimension. This suggests the labels might have \n",
    "    #variable lengths or already include a batch-like dimension.\n",
    "\n",
    "    add_noise = NOISE_RATE > 0.\n",
    "    #Checks if noise should be added based on a global variable NOISE_RATE.\n",
    "    #If NOISE_RATE is greater than 0, noise will be added to the inputs.\n",
    "    \n",
    "    if add_noise:\n",
    "      sh = xs.shape\n",
    "      noise_mask = torch.bernoulli(torch.full(sh, NOISE_RATE))  # 0 (keep) or 1 (replace with noise)\n",
    "      #Gets the shape of the input tensor batch.\n",
    "      #Creates a binary mask using Bernoulli sampling, where each element has NOISE_RATE probability of being 1 \n",
    "      #(indicating where noise will be applied) and 1-NOISE_RATE probability of being 0.\n",
    "            \n",
    "      sp_noise = torch.bernoulli(torch.full(sh, 0.5))-0.5  # -1 or 1\n",
    "      #Generates the actual noise values as either -0.5 or 0.5.\n",
    "      #First creates a tensor of the same shape filled with 0.5, then applies Bernoulli \n",
    "      #sampling to get 0s or 1s.\n",
    "      #Subtracts 0.5 to convert to -0.5 or 0.5 (this creates salt and pepper noise).\n",
    "        \n",
    "      xns = xs * (1-noise_mask) + sp_noise * noise_mask\n",
    "      #Creates the noisy input xns by:\n",
    "          #Keeping original values where the mask is 0: xs * (1-noise_mask)\n",
    "          #Adding noise values where the mask is 1: sp_noise * noise_mask\n",
    "          #The result is a tensor where some values are preserved \n",
    "          #from the original input and others are replaced with noise.\n",
    "      \n",
    "      # sp = sp_noise\n",
    "    else:\n",
    "       xns = xs\n",
    "    #If no noise is to be added, the noisy input is the same as the original input.\n",
    "\n",
    "    return xns.to(device), xs.to(device), ys.to(device)\n",
    "    #Returns three tensors, all moved to the specified device (likely GPU):\n",
    "\n",
    "    #xns: The inputs with noise added (or original inputs if no noise)\n",
    "    #xs: The original clean inputs\n",
    "    #ys: The labels or targets\n",
    "    #\n",
    "    #\n",
    "    #This return structure is typical for denoising autoencoders, where you need \n",
    "    #both the noisy input (fed to the encoder) and the clean target \n",
    "    #(used to compute the reconstruction loss).\n",
    "    #\n",
    "    #This function is specifically designed for training denoising autoencoders, \n",
    "    #where the model learns to remove noise from corrupted inputs by trying \n",
    "    #to reconstruct the original clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce7055-18c9-449b-ae35-306174e84f89",
   "metadata": {},
   "source": [
    "### Functions to Prepare and Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af426c-1e58-44b9-9189-1542edda4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(valid_loader):\n",
    "  # 1. get numpy array of all validation images:\n",
    "  val_images_noisy = []\n",
    "  val_images = []\n",
    "  val_labels = []\n",
    "\n",
    "  for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "      val_images_noisy.append(noisy_data.detach().cpu().numpy())\n",
    "      val_images.append(data.detach().cpu().numpy())\n",
    "      val_labels.append(target.detach().cpu().numpy())\n",
    "\n",
    "  val_images_noisy = np.concatenate(val_images_noisy, axis=0)\n",
    "  val_images = np.concatenate(val_images, axis=0)\n",
    "  val_labels = np.concatenate(val_labels, axis=0)\n",
    "\n",
    "  # 2. get numpy array of balanced validation samples for visualization:\n",
    "  sample_images_noisy = []\n",
    "  sample_images = []\n",
    "  sample_labels = []\n",
    "  single_el_idx = []  # indexes of single element per class for visualization\n",
    "\n",
    "  n_class = np.max(val_labels) + 1\n",
    "  # Determines the number of classes (for MNIST, this would be 10, representing digits 0-9).\n",
    "  for class_idx in range(n_class):\n",
    "    map_c = val_labels == class_idx\n",
    "\n",
    "    ims_c_noisy = val_images_noisy[map_c]\n",
    "    ims_c = val_images[map_c]\n",
    "    print('class label:')\n",
    "    print(class_idx)\n",
    "    print('shape selected class')\n",
    "    print(ims_c.shape)\n",
    "    # For each class:\n",
    "       # Creates a boolean mask map_c identifying all samples of the current class.\n",
    "       # Extracts noisy and clean images for just this class.\n",
    "      \n",
    "\n",
    "    samples_idx = np.random.choice(len(ims_c), N_SAMPLE, replace=False)\n",
    "\n",
    "    ims_c_noisy_samples = ims_c_noisy[samples_idx]\n",
    "    ims_c_samples = ims_c[samples_idx]\n",
    "    # Randomly selects N_SAMPLE images from the current class.\n",
    "    # replace=False ensures no duplicates are selected.\n",
    "    # Extracts both noisy and clean versions of these sampled images.\n",
    "      \n",
    "\n",
    "    sample_images_noisy.append(ims_c_noisy_samples)\n",
    "    sample_images.append(ims_c_samples)\n",
    "\n",
    "    sample_labels.append([class_idx]*N_SAMPLE)\n",
    "\n",
    "    # Adds the sampled noisy images, clean images, and labels to their respective lists.\n",
    "    # Creates an array of N_SAMPLE repeated labels for this class.\n",
    "\n",
    "    start_idx = N_SAMPLE*class_idx\n",
    "    single_el_idx.extend([start_idx + i for i in range(min(N_VIS_SAMPLE, N_SAMPLE))])\n",
    "    # Calculates the indices for the first N_VIS_SAMPLE elements of this class in the final concatenated array.\n",
    "    # These indices will be used to extract a smaller subset for visualization.\n",
    "\n",
    "    \n",
    "  sample_images_noisy = np.concatenate(sample_images_noisy, axis=0)\n",
    "  sample_images = np.concatenate(sample_images, axis=0)\n",
    "  sample_labels = np.concatenate(sample_labels, axis=0)\n",
    "  single_el_idx = np.array(single_el_idx)\n",
    "  #Combines all class samples into single arrays.\n",
    "  #Converts the index list to a NumPy array.\n",
    "\n",
    "  samples = {\n",
    "      'images_noisy': sample_images_noisy,\n",
    "      'images': sample_images,\n",
    "      'labels': sample_labels,\n",
    "      'single_el_idx': single_el_idx\n",
    "\n",
    "  }\n",
    "  return samples\n",
    "# Creates and returns a dictionary with all collected samples.\n",
    "\n",
    "\n",
    "# This function ensures we have:\n",
    "# \n",
    "# A balanced number of samples for each class (equal representation)\n",
    "# Both noisy and clean versions of each image\n",
    "# A mapping between the noisy and clean versions\n",
    "# A subset of indices for visualization purposes\n",
    "# This is particularly useful for creating visualizations that show how the model behaves across different classes, or for comparing reconstruction quality across digits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d87e7-1730-4229-bd34-35f5c8b1f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np_showable(pt_img):\n",
    "  np_im = pt_img.detach().cpu().numpy()\n",
    "  if len(np_im.shape) == 4:\n",
    "    np_im = np_im[0]\n",
    "\n",
    "  if np_im.shape[0] > 3:\n",
    "    np_im = np_im[-3:]\n",
    "\n",
    "  return (eo.rearrange(np_im, 'c h w -> h w c')/2+.5).clip(0., 1.)\n",
    "\n",
    "#This function converts a PyTorch tensor image to a NumPy array suitable for visualization.\n",
    "#pt_img.detach().cpu().numpy() - Detaches the tensor from the computation graph, moves it to CPU if it's on GPU, and converts it to a NumPy array.\n",
    "#if len(np_im.shape) == 4: - Checks if the image has a batch dimension (shape: [batch, channels, height, width]).\n",
    "#np_im = np_im[0] - If there's a batch dimension, takes only the first image in the batch.\n",
    "#if np_im.shape[0] > 3: - Checks if there are more than 3 channels.\n",
    "#np_im = np_im[-3:] - If there are more than 3 channels, keeps only the last 3 channels (useful for handling multi-channel data).\n",
    "#eo.rearrange(np_im, 'c h w -> h w c') - Uses the einops library to rearrange the tensor from PyTorch's [channels, height, width] format to matplotlib's [height, width, channels] format.\n",
    "#/2+.5 - Applies normalization assuming the image data is in the range [-1, 1], converting it to [0, 1].\n",
    "#.clip(0., 1.) - Ensures all values are within the [0, 1] range, clamping any values outside this range.\n",
    "\n",
    "def plot_im(im, is_torch=True):\n",
    "  plt.imshow(to_np_showable(im) if is_torch else im, cmap='gray')\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "#This function plots a single image.\n",
    "#is_torch=True - Default parameter indicating whether the input is a PyTorch tensor.\n",
    "#to_np_showable(im) if is_torch else im - Converts the image to a NumPy array if it's a PyTorch tensor, otherwise uses it directly.\n",
    "#plt.imshow(..., cmap='gray') - Displays the image using matplotlib with a grayscale colormap.\n",
    "#plt.show() - Renders the plot.\n",
    "#plt.close() - Closes the figure to free up memory.\n",
    "\n",
    "def plot_im_samples(ds, n=5, is_torch=False):\n",
    "  fig, axs = plt.subplots(1, n, figsize=(16, n))\n",
    "  for i, image in enumerate(ds[:n]):\n",
    "      axs[i].imshow(to_np_showable(image) if is_torch else image, cmap='gray')\n",
    "      axs[i].set_axis_off()\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "#This function plots multiple images from a dataset in a row.\n",
    "#ds - The dataset or collection of images to sample from.\n",
    "#n=5 - Default number of images to display.\n",
    "#is_torch=False - Default parameter indicating whether the inputs are PyTorch tensors.\n",
    "#plt.subplots(1, n, figsize=(16, n)) - Creates a figure with a single row of n subplots, with a width of 16 inches and height of n inches.\n",
    "#The loop iterates through the first n images in the dataset:\n",
    "#\n",
    "#axs[i].imshow(...) - Displays each image in its corresponding subplot.\n",
    "#axs[i].set_axis_off() - Removes axis labels and ticks for cleaner visualization.\n",
    "#\n",
    "#\n",
    "#plt.show() - Renders the entire plot with all images.\n",
    "#plt.close() - Closes the figure to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f50b9-bb6f-41f9-b91f-51433e65dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and std of an array with numpy:\n",
    "def get_mean_std(x):\n",
    "    x_mean = np.mean(x)\n",
    "    x_std = np.std(x)\n",
    "    return x_mean, x_std\n",
    "\n",
    "# get min and max of an array with numpy:\n",
    "def get_min_max(x):\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    return x_min, x_max\n",
    "\n",
    "def is_iterable(obj):\n",
    "    try:\n",
    "        iter(obj)\n",
    "    except Exception:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "#This function checks if an object is iterable (can be looped over).\n",
    "#It uses a try-except block to attempt to call iter(obj), which will succeed only if obj is iterable.\n",
    "#If calling iter(obj) raises any exception, the function returns False.\n",
    "#If no exception occurs, the function returns True.\n",
    "\n",
    "def type_len(obj):\n",
    "    t = type(obj)\n",
    "    if is_iterable(obj):\n",
    "        sfx = f', shape: {obj.shape}' if t == np.ndarray else ''\n",
    "        print(f'type: {t}, len: {len(obj)}{sfx}')\n",
    "    else:\n",
    "        print(f'type: {t}, len: {len(obj)}')\n",
    "\n",
    "#This is a utility function for debugging that prints information about an object.\n",
    "#t = type(obj) - Gets the type of the provided object.\n",
    "#It checks if the object is iterable using the is_iterable function defined earlier.\n",
    "#If the object is iterable:\n",
    "#\n",
    "#It checks if the object is a NumPy array (t == np.ndarray).\n",
    "#If it's a NumPy array, it adds shape information to the output string.\n",
    "#It prints the type and length of the object, along with shape information if applicable.\n",
    "#\n",
    "#\n",
    "#If the object is not iterable, it still attempts to print the type and length (though this might raise an error if len() isn't applicable to the object).\n",
    "#\n",
    "#Note: There seems to be an issue with the type_len function - it tries to call len() on non-iterable objects in the else clause, \n",
    "#which would typically cause an error. This might be a bug in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33cc4e0-c874-4eaa-950a-8cd3a9ae9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging 2d matrix of images in 1 image\n",
    "def mosaic(mtr_of_ims):\n",
    "  ny = len(mtr_of_ims)\n",
    "  assert(ny != 0)\n",
    "  #Gets the number of rows in the matrix and asserts that it's not empty.\n",
    "\n",
    "  nx = len(mtr_of_ims[0])\n",
    "  assert(nx != 0)\n",
    "  #Gets the number of columns in the first row and asserts that it's not empty.\n",
    "\n",
    "  im_sh = mtr_of_ims[0][0].shape\n",
    "\n",
    "  assert (2 <= len(im_sh) <= 3)\n",
    "  #Gets the shape of the first image in the matrix.\n",
    "  #Verifies that the image is either 2D (grayscale) or 3D (with channels).\n",
    "    \n",
    "  multichannel = len(im_sh) == 3\n",
    "\n",
    "  if multichannel:\n",
    "    h, w, c = im_sh\n",
    "  else:\n",
    "    h, w = im_sh\n",
    "  #Determines if the images have multiple channels.\n",
    "  #If multichannel, unpacks height, width, and channels. Otherwise, just height and width.\n",
    "\n",
    "  h_c = h * ny + 1 * (ny-1)\n",
    "  w_c = w * nx + 1 * (nx-1)\n",
    "  #Calculates the total height and width of the canvas.\n",
    "  #Adds 1 pixel spacing between images (both horizontally and vertically).\n",
    "\n",
    "  canv_sh = (h_c, w_c, c) if multichannel else (h_c, w_c)\n",
    "  canvas = np.ones(shape=canv_sh, dtype=np.float32)*0.5\n",
    "  #Defines the shape of the canvas based on whether images are multichannel.\n",
    "  #Creates a canvas filled with gray (0.5) values, assuming image values are in [0,1] range.\n",
    "\n",
    "  for iy, row in enumerate(mtr_of_ims):\n",
    "    y_ofs = iy * (h + 1)\n",
    "    #Loops through each row of images.\n",
    "    #Calculates the vertical offset for the current row.\n",
    "    for ix, im in enumerate(row):\n",
    "      x_ofs = ix * (w + 1)\n",
    "      #Loops through each image in the current row.\n",
    "      #Calculates the horizontal offset for the current image.\n",
    "      canvas[y_ofs:y_ofs + h, x_ofs:x_ofs + w] = im\n",
    "      #Copies the current image to the appropriate position in the canvas.\n",
    "      #This uses NumPy's array slicing to place the image at the correct location.\n",
    "  return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc172ef0-00f3-4744-84af-00b9877cde59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def split_train_validation(image_paths, labels, train_ratio=0.8, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split image paths into training and validation sets with balanced class distribution.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of image file paths\n",
    "        labels (list): List of class labels corresponding to image paths\n",
    "        train_ratio (float): Proportion for training set (default 0.8)\n",
    "        random_seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_paths, train_labels, val_paths, val_labels)\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # Group paths by class\n",
    "    class_groups = defaultdict(list)\n",
    "    for path, label in zip(image_paths, labels):\n",
    "        class_groups[label].append(path)\n",
    "    \n",
    "    train_paths, train_labels = [], []\n",
    "    val_paths, val_labels = [], []\n",
    "    \n",
    "    # Split each class maintaining proportion\n",
    "    for class_label, paths in class_groups.items():\n",
    "        # Shuffle paths for this class\n",
    "        shuffled_paths = paths.copy()\n",
    "        random.shuffle(shuffled_paths)\n",
    "        \n",
    "        # Calculate split point\n",
    "        n_train = int(len(shuffled_paths) * train_ratio)\n",
    "        \n",
    "        # Split paths\n",
    "        class_train_paths = shuffled_paths[:n_train]\n",
    "        class_val_paths = shuffled_paths[n_train:]\n",
    "        \n",
    "        # Add to final lists\n",
    "        train_paths.extend(class_train_paths)\n",
    "        train_labels.extend([class_label] * len(class_train_paths))\n",
    "        val_paths.extend(class_val_paths)\n",
    "        val_labels.extend([class_label] * len(class_val_paths))\n",
    "    \n",
    "    # Final shuffle to mix classes\n",
    "    combined_train = list(zip(train_paths, train_labels))\n",
    "    combined_val = list(zip(val_paths, val_labels))\n",
    "    random.shuffle(combined_train)\n",
    "    random.shuffle(combined_val)\n",
    "    \n",
    "    train_paths, train_labels = zip(*combined_train) if combined_train else ([], [])\n",
    "    val_paths, val_labels = zip(*combined_val) if combined_val else ([], [])\n",
    "    \n",
    "    print(f\"Training set: {len(train_paths)} images\")\n",
    "    print(f\"Validation set: {len(val_paths)} images\")\n",
    "    \n",
    "    return list(train_paths), list(train_labels), list(val_paths), list(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcbe952-8c63-42e9-8b4b-8bc3a58f5141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c077319-4fd6-4ace-8aa8-20333f7b7c66",
   "metadata": {},
   "source": [
    "### Functions To Analyse Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101620c5-0a61-416d-9b6e-f9e71a5a83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(history, logscale=True):\n",
    "    \"\"\"\n",
    "    plot training loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = history['loss']\n",
    "    v_loss = history['val_loss']\n",
    "    epochs = history['epoch']\n",
    "\n",
    "    # This function visualizes training history (loss over epochs).\n",
    "    # Extracts training loss, validation loss, and epoch numbers from the history dictionary.\n",
    "\n",
    "    \n",
    "    plot = plt.semilogy if logscale else plt.plot\n",
    "    # Cleverly chooses between logarithmic scale (plt.semilogy) or linear scale (plt.plot) based on the logscale parameter.\n",
    "    # Default is logarithmic scale, which is often better for visualizing loss curves as they typically decrease exponentially.\n",
    "    \n",
    "    plot(epochs, loss, label='training');\n",
    "    plot(epochs, v_loss, label='validation');\n",
    "    # Plots both training and validation loss curves using the selected plotting function.\n",
    "    # Labels each curve for the legend.\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # Adds a legend, axis labels, displays the plot, and then closes the figure.\n",
    "\n",
    "\n",
    "\n",
    "def plot_samples(sample_history, samples, epoch_stride=5, fig_scale=1):\n",
    "    \"\"\"\n",
    "    Plots input, noisy samples (for DAE) and reconstruction.\n",
    "    Each `epoch_stride`-th epoch\n",
    "    \"\"\"\n",
    "    # This function visualizes sample reconstructions over training epochs.\n",
    "    # Shows how the model's reconstruction capability improves over time.\n",
    "\n",
    "    single_el_idx = samples['single_el_idx']\n",
    "    images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
    "    images = samples['images'][single_el_idx, 0]\n",
    "    # Extracts indices for selected samples to visualize.\n",
    "    # Gets the noisy input images and the original clean images for these samples.\n",
    "    # The , 0 indexing suggests selecting the first channel of each image.\n",
    "\n",
    "    last_epoch = np.max(list(sample_history.keys()))\n",
    "    # Determines the last epoch number in the history data.\n",
    "\n",
    "    for epoch_idx, hist_el in sample_history.items():\n",
    "      if epoch_idx % epoch_stride != 0 and epoch_idx != last_epoch:\n",
    "        continue\n",
    "    # Iterates through each epoch's results in the history.\n",
    "    # Uses epoch_stride to select only every nth epoch (to avoid too many visualizations).\n",
    "    # Always includes the last epoch regardless of the stride.\n",
    "\n",
    "      samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
    "    # Creates an array of three sets of images to visualize side by side:\n",
    "       # The noisy input images\n",
    "       # The model's reconstructions for the current epoch\n",
    "       # The original clean images (ground truth)\n",
    "\n",
    "      ny = len(samples_arr)\n",
    "      nx = len(samples_arr[0])\n",
    "\n",
    "      plt.figure(figsize=(fig_scale*nx, fig_scale*ny))\n",
    "      # Calculates the dimensions of the visualization grid.\n",
    "      # Creates a figure with size proportional to the number of samples.\n",
    "\n",
    "        \n",
    "      m = mosaic(samples_arr)\n",
    "      # Uses the previously defined mosaic function to create a grid of all images.\n",
    "\n",
    "      plt.title(f'after epoch {int(epoch_idx)}')\n",
    "      plt.imshow(m, cmap='gray', vmin=-.5, vmax=.5)\n",
    "      # Adds a title showing which epoch this visualization represents.\n",
    "      # Displays the mosaic with a grayscale colormap and fixed value range.\n",
    "      # The vmin=-.5, vmax=.5 matches the normalized data range we've seen before.\n",
    "\n",
    "        \n",
    "      plt.tight_layout(pad=0.1, h_pad=0, w_pad=0)\n",
    "      plt.show()\n",
    "      plt.close()\n",
    "      # Ensures proper spacing in the figure.\n",
    "      # Displays the figure and then closes it to free memory.\n",
    "\n",
    "# This function creates a powerful visualization showing the progression of the model's reconstruction ability across epochs. Each visualization has three rows:\n",
    "# \n",
    "# The noisy inputs\n",
    "# The model's reconstructions\n",
    "# The original clean images (targets)\n",
    "# \n",
    "# This makes it easy to see how the model gradually learns to denoise and reconstruct the images over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9677f0-b2c1-452a-9ff9-1b919838625a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85f40e-d21f-4bfe-881b-0ecc47a7973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are utility functions for working with trained models at different stages of training. Let me break them down:\n",
    "\n",
    "def run_on_trained(model, root_dir, run_fn, ep=None, model_filename=None):\n",
    "    \"\"\"\n",
    "    Helper function to excecute any function on model in state after `ep` training epoch\n",
    "    \"\"\"\n",
    "    # This function loads a model checkpoint and runs a specified function on it.\n",
    "    # Parameters:\n",
    "    # \n",
    "    # model: The neural network model instance\n",
    "    # root_dir: Directory containing saved model checkpoints\n",
    "    # run_fn: The function to run on the loaded model\n",
    "    # ep: Specific epoch to load (optional)\n",
    "    # model_filename: Specific checkpoint file to load (optional)\n",
    "\n",
    "    if model_filename is None:\n",
    "        if ep is not None:\n",
    "            model_filename = root_dir/f'model_{ep:03d}.pth'\n",
    "        else:\n",
    "            model_filename = sorted(list(root_dir.glob('*.pth')))[-1]  # last model state\n",
    "    # Determines which model checkpoint file to load:\n",
    "    # \n",
    "    # If a specific filename is provided, use that (in this case this code block would be skipped)\n",
    "    # If an epoch number is provided, construct the filename using a pattern\n",
    "    # If neither is provided, use the last checkpoint file (by alphabetical sorting)\n",
    "    # The code uses pathlib's Path objects for file handling (using / for path joining)\n",
    "\n",
    "    \n",
    "    model_dict = torch.load(model_filename,weights_only=False)\n",
    "\n",
    "    model.load_state_dict(model_dict['model_state_dict'])\n",
    "\n",
    "    # Loads the saved model state from the specified file\n",
    "    # The weights_only=False parameter indicates to load the full state dictionary (not just weights)\n",
    "    # Restores the model parameters from the saved state dictionary\n",
    "    \n",
    "\n",
    "    run_fn(model)\n",
    "    # Calls the provided function on the loaded model\n",
    "\n",
    "def run_on_all_training_history(model, root_dir, run_fn, n_ep=None):\n",
    "    \"\"\"\n",
    "    Helper function to excecute any function on model state after each of the training epochs\n",
    "    \"\"\"\n",
    "    # This function runs a specified function on multiple model checkpoints from different training epochs.\n",
    "    # Parameters:\n",
    "    # \n",
    "    # model: The neural network model instance\n",
    "    # root_dir: Directory containing saved model checkpoints\n",
    "    # run_fn: The function to run on each loaded model state\n",
    "    # n_ep: Specific number of epochs to process (optional)\n",
    "    \n",
    "    if n_ep is not None:\n",
    "        for ep in range(n_ep):\n",
    "            print(f'running on epoch {ep+1}/{n_ep}...')\n",
    "            run_on_trained(model, root_dir, run_fn, ep=ep)\n",
    "    # If a specific number of epochs is provided:\n",
    "    # \n",
    "    # Iterates through each epoch from 0 to n_ep-1\n",
    "    # Prints progress information\n",
    "    # Calls run_on_trained for each epoch\n",
    "    \n",
    "    else:\n",
    "        for model_filename in sorted(root_dir.glob('*.pth')):\n",
    "            print(f'running on checkpoint {model_filename}...')\n",
    "            run_on_trained(model, root_dir, run_fn, model_filename=model_filename)\n",
    "\n",
    "    # If no specific number of epochs is provided:\n",
    "    # \n",
    "    # Finds all .pth files in the root directory\n",
    "    # Sorts them (presumably by name, which would be by epoch if using the naming pattern)\n",
    "    # Processes each checkpoint file in order\n",
    "    \n",
    "    print(f'done')\n",
    "\n",
    "    # Prints a completion message when all checkpoints have been processed\n",
    "    # \n",
    "    # These utility functions make it easy to:\n",
    "    # \n",
    "    # Analyze a model at a specific point in its training history\n",
    "    # Run the same analysis across multiple stages of training\n",
    "    # Visualize or evaluate how the model's behavior changes over the course of training\n",
    "    # \n",
    "    # They're particularly useful for post-training analysis, debugging, and creating visualizations of model evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ea88f-da6e-4762-a5f2-b9a93ac2e666",
   "metadata": {},
   "source": [
    "### Set Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5cfe18-2f56-491c-a536-b1a95243cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set globals (your existing code)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "N_SAMPLE = 4*32\n",
    "N_VIS_SAMPLE = 2\n",
    "CODE_SIZE = 120\n",
    "NOISE_RATE = 0.1\n",
    "MODEL_NAME = 'cdae_model'\n",
    "SIDE_LENGTH = 320\n",
    "\n",
    "target_size = (SIDE_LENGTH, SIDE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419008bd-3a6d-41f5-a00d-83b97b51e16c",
   "metadata": {},
   "source": [
    "### Define file paths: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddf17f-30b2-4e36-bbbf-75e1ecfe36b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project_path = Path.cwd()\n",
    "root_path = (project_path / '..').resolve()\n",
    "\n",
    "# Define paths\n",
    "image_dir = root_path/'visual_genome_proc_data'  # Replace with your directory containing images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c578de-0f90-468a-ac33-440cae517f7a",
   "metadata": {},
   "source": [
    "### Get information about image size: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4822edc-4de4-4eba-85e6-d1319c7cea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analyze_image_sizes(image_dir, extensions=('.jpg', '.jpeg', '.tif', '.tiff'))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd606ba-3261-4b0c-8a87-bde29feb2dbe",
   "metadata": {},
   "source": [
    "### Load meta data about images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f918ac-856e-4c0d-b964-976235a0bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv(image_dir/'labels.csv')\n",
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa36a9-544a-4dec-b63e-e781a07ebd92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4b98d-8b57-4a72-a602-11c02b073ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = load_image_paths(image_dir)\n",
    "image_paths[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f676af-3aa6-46e7-a465-6caf999cb215",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = list(meta_data.file_paths)\n",
    "#labels = list(meta_data.mountains)\n",
    "labels = list(meta_data.buildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563a563-ec49-4d07-b52f-1ad16f4b4041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb6059-2a94-42ee-b789-536128293c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c0444cd-ab0b-4a7e-a5eb-4b750ce19fdf",
   "metadata": {},
   "source": [
    "### Define Basic Autoencoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcad9d4-c14c-47e4-bbfa-c6c493339510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, code_size):\n",
    "        self.input_size = list(input_size)  # shape of data sample\n",
    "        self.flat_data_size = np.prod(self.input_size)\n",
    "        self.hidden_size = 128\n",
    "\n",
    "        self.code_size = code_size  # code size\n",
    "\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        #Creates an autoencoder neural network that inherits from PyTorch's nn.Module.\n",
    "        #Takes two parameters:\n",
    "        #\n",
    "        #input_size: The shape of input data (e.g., [1, 28, 28] for MNIST)\n",
    "        #code_size: The dimension of the encoded representation (bottleneck)\n",
    "        #\n",
    "        #\n",
    "        #Calculates the flattened input size by multiplying all dimensions.\n",
    "        #Sets an intermediate hidden layer size of 128 neurons.\n",
    "        #Calls the parent class initializer.\n",
    "\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(self.flat_data_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size, self.code_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        #Defines the encoder network as a sequence of operations:\n",
    "            #\n",
    "            #Flattens the input (e.g., converts a 2D image to 1D)\n",
    "            #Linear layer mapping from input size to hidden size\n",
    "            #ReLU activation\n",
    "            #Linear layer mapping from hidden size to code size\n",
    "            #Sigmoid activation (constrains the encoded values to [0, 1])\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.code_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size, self.flat_data_size),\n",
    "            nn.Tanh(),  # Think: why tanh?\n",
    "\n",
    "            nn.Unflatten(1, self.input_size),\n",
    "        )\n",
    "        #Defines the decoder network:\n",
    "            #Linear layer from code size to hidden size\n",
    "            #ReLU activation\n",
    "            #Linear layer from hidden size back to the flattened input size\n",
    "            #Tanh activation (outputs values in [-1, 1], matching the normalized input range)\n",
    "            #Unflattens the output back to the original input shape\n",
    "\n",
    "#Regarding \"why tanh?\": Tanh is used because the input images were normalized to approximately [-0.5, 0.5] \n",
    "    #range (using m=0.5, s=1.0). Tanh outputs values in the range [-1, 1], \n",
    "    #which after scaling by 1.1 in the decode method closely matches the input data range.\n",
    "\n",
    "    def forward(self, x, return_z=False):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return (decoded, encoded) if return_z else decoded\n",
    "    # The forward pass:\n",
    "        #Encodes the input\n",
    "        #Decodes the encoded representation\n",
    "        #If return_z=True, returns both the reconstruction and the encoded values\n",
    "        #Otherwise, just returns the reconstruction\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)*1.1\n",
    "# Helper methods to encode and decode separately\n",
    "# Note the multiplication by 1.1 in the decode method, \n",
    "    # which slightly amplifies the output range to better match the input data distribution\n",
    "\n",
    "        \n",
    "\n",
    "    def get_n_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    # Utility method to count the total number of trainable parameters in the model\n",
    "\n",
    "\n",
    "def eval_on_samples(ae_model, epoch, samples):\n",
    "    # this is called on end of each training epoch\n",
    "    xns = samples['images_noisy']\n",
    "    xns = torch.tensor(xns, dtype=torch.float32).to(device)\n",
    "    #labels = samples['labels']\n",
    "\n",
    "# Function to evaluate the autoencoder on sample data after each epoch\n",
    "# Takes the model, current epoch number, and samples dictionary\n",
    "# Extracts noisy images from the samples and converts them to a PyTorch tensor on the target device\n",
    "# The labels are extracted but commented out (not used)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yz = ae_model(xns, return_z=True)\n",
    "        yz = [el.detach().cpu().numpy() for el in yz]\n",
    "\n",
    "        y = yz[0]\n",
    "        z = yz[1:]\n",
    "    # Uses torch.no_grad() to disable gradient calculation (for efficiency during evaluation)\n",
    "    # Gets both reconstructions and encodings (i.e. latent space!) by calling the model with return_z=True\n",
    "    # Converts all outputs to NumPy arrays\n",
    "    # Separates the reconstructions y and encodings z\n",
    "\n",
    "    res = {'z': z, 'y': y, 'epoch': epoch}\n",
    "    return res\n",
    "\n",
    "# Creates and returns a dictionary containing:\n",
    "\n",
    "# z: The encoded representations\n",
    "# y: The reconstructed images\n",
    "# epoch: The current epoch number\n",
    "# \n",
    "\n",
    "# This evaluation function captures the model's performance at each epoch, allowing for tracking reconstruction quality and analyzing the learned representations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfc7bae-2221-4265-92dc-d4ee1b405ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0849b-c7bb-42f7-af02-c8cae1acd817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7372f062-82b5-47a1-9ba9-fb7f57835526",
   "metadata": {},
   "source": [
    "### Split data in training set and validation set, execute workflow: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d2526-5d3b-4dd5-b476-591490eb4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, train_labels, val_paths, val_labels = split_train_validation(image_paths, labels, train_ratio=0.8, random_seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0cf3bd-096f-41bb-a0d3-9cf462dd45eb",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa7b8b7-aabb-422b-a837-85e7425b0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load image paths\n",
    "# already done\n",
    "\n",
    "# Step 2: Create your labels list (matching image_paths order)\n",
    "# already done\n",
    "\n",
    "# Step 3: Create dataset (equivalent to train_dataset)\n",
    "train_dataset = create_image_dataset(\n",
    "    image_paths=train_paths,\n",
    "    labels=train_labels,\n",
    "    target_size=target_size,\n",
    "    convert_grayscale=True,\n",
    "    apply_aging=False,\n",
    "    mean=0.5,\n",
    "    std=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea54553-dcec-40c6-abb3-2f0d85acf7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d88b2-e016-4d65-a611-76eab0144110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 4: Create DataLoader with your original collate function\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_ae_dataset,  # Your original function using globals\n",
    "    drop_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae0784-7df2-4173-8edb-fb300c049e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f09106-a0a4-4a12-92fb-b5f260756c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = create_image_dataset(\n",
    "    image_paths=val_paths,\n",
    "    labels=val_labels,\n",
    "    target_size=target_size,\n",
    "    convert_grayscale=True,\n",
    "    apply_aging=False,\n",
    "    mean=0.5,\n",
    "    std=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782748f-3b1c-4a5d-b5c2-1d31740d70bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_ae_dataset,  # Your original function using globals\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b784d301-fd8a-4ebd-aea1-c1a6d5d0ffab",
   "metadata": {},
   "source": [
    "hidden_l = 9\n",
    "hidden_fs = 64\n",
    "my_hidden_size = (hidden_l**2)*hidden_fs\n",
    "my_hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb857fd-9664-4fe0-97c1-745c2c82cf61",
   "metadata": {},
   "source": [
    "### Define Convolutional Autoencoder (inherits some properties from basic autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff561e-2694-4bfc-8913-c03d02fef874",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_spatial_size = 320 // (2**6)\n",
    "\n",
    "#in_size = target_size\n",
    "#in_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e4af2-92e9-4cd9-afc7-784356b028ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoEncoder(AutoEncoder):\n",
    "    def __init__(self, input_size, code_size):\n",
    "        self.input_size = list(input_size)\n",
    "        \n",
    "        # For 320x320 input with 6 stride=2 layers: 320÷64 = 5x5 final spatial size\n",
    "        # With hidden_fs=64: final feature map is 64×5×5 = 1600 values\n",
    "        self.hidden_size = 64 * 5 * 5  # = 1600 (not 128!)\n",
    "        \n",
    "        self.code_size = code_size\n",
    "        \n",
    "        super(ConvolutionalAutoEncoder, self).__init__(input_size, code_size)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1, stride=1), nn.LeakyReLU(negative_slope=0.3),      # 320×320×8\n",
    "            nn.Conv2d(8, 8, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),      # 160×160×8\n",
    "            nn.Conv2d(8, 16, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),     # 80×80×16\n",
    "            nn.Conv2d(16, 16, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),    # 40×40×16\n",
    "            nn.Conv2d(16, 32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),    # 20×20×32\n",
    "            nn.Conv2d(32, 32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),    # 10×10×32\n",
    "            nn.Conv2d(32, 64, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),    # 5×5×64\n",
    "            \n",
    "            nn.Flatten(),  # 5×5×64 = 1600 values\n",
    "            \n",
    "            nn.Linear(1600, 200), nn.LeakyReLU(negative_slope=0.3),  # 1600 → 200\n",
    "            nn.Linear(200, code_size),  # 200 → 70\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(code_size, 1600), nn.LeakyReLU(negative_slope=0.3),  # 70 → 1600\n",
    "            \n",
    "            nn.Unflatten(1, (64, 5, 5)),  # 1600 → 64×5×5\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),  # 5→10\n",
    "            nn.ConvTranspose2d(32, 32, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),  # 10→20\n",
    "            nn.ConvTranspose2d(32, 16, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),  # 20→40\n",
    "            nn.ConvTranspose2d(16, 16, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),  # 40→80\n",
    "            nn.ConvTranspose2d(16, 8, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),   # 80→160\n",
    "            nn.ConvTranspose2d(8, 8, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),    # 160→320\n",
    "            \n",
    "            nn.Conv2d(8, 1, 3, padding=1, stride=1), nn.Tanh(),  # Final output: 320×320×1\n",
    "        )\n",
    "\n",
    "    def decode(self, z):\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction  # No cropping needed with correct dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a1cd6-9246-49a0-9b4c-0ca0007022ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = target_size\n",
    "model = ConvolutionalAutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
    "\n",
    "#samples = get_samples(valid_loader)\n",
    "samples = get_samples(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d380e2-88ef-4895-80f9-997a0f489997",
   "metadata": {},
   "outputs": [],
   "source": [
    "xns = torch.tensor(samples['images_noisy']).to(device)\n",
    "print(xns.shape)\n",
    "zs = model.encode(xns)\n",
    "ys = model(xns)\n",
    "print(zs.shape)\n",
    "print(ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2ef24-66e9-42e2-a619-4472c75cdfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the autoencoder model, for N_EPOCHS epochs,\n",
    "# save history of loss values for training and validation sets,\n",
    "# history of validation samples evolution, and model weights history,\n",
    "\n",
    "\n",
    "# This code implements the complete training loop for the autoencoder. \n",
    "# Let me break it down:\n",
    "\n",
    "#N_EPOCHS = 60\n",
    "N_EPOCHS = 6\n",
    "LR = 0.0009\n",
    "# Sets the number of training epochs to 50\n",
    "# Sets the learning rate for the Adam optimizer to 0.0009\n",
    "\n",
    "model_root = pl.Path(MODEL_NAME)\n",
    "model_root.mkdir(exist_ok=True)\n",
    "# Creates a directory path for saving model checkpoints using the MODEL_NAME ('ae_model')\n",
    "# Makes sure the directory exists (creates it if it doesn't)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "# Creates an Adam optimizer to update the model parameters\n",
    "# Adam is an adaptive learning rate optimization algorithm well-suited for deep learning\n",
    "\n",
    "# implement loss explicitly\n",
    "loss = nn.MSELoss()\n",
    "# Defines the loss function as Mean Squared Error (MSE)\n",
    "# This measures the average squared difference between the reconstructed and target images\n",
    "\n",
    "# train the model\n",
    "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
    "sample_history = {}\n",
    "# Creates dictionaries to store training metrics and sample reconstructions\n",
    "# history tracks training and validation losses across epochs\n",
    "# sample_history will store sample reconstruction results at each epoch\n",
    "\n",
    "pbar = tqdm.tqdm(range(N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
    "# Creates a progress bar for tracking the training process\n",
    "# Will show the current epoch and update metrics during training\n",
    "\n",
    "for epoch_idx in pbar:\n",
    "# Starts the main training loop that runs for N_EPOCHS iterations\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # Initializes the epoch loss accumulator\n",
    "    # Sets the model to training mode (enables dropout, batch normalization updates, etc.)\n",
    "    \n",
    "    for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(noisy_data)\n",
    "        loss_value = loss(output, data)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value.detach().cpu().item()\n",
    "    # Iterates through all batches in the training dataset\n",
    "    # For each batch:\n",
    "    # \n",
    "        # Zeros out previous gradients\n",
    "        # Passes the noisy input through the model\n",
    "        # Calculates the MSE loss between the reconstruction and clean data\n",
    "        # Computes gradients via backpropagation\n",
    "        # Updates model parameters using the optimizer\n",
    "        # Accumulates the loss value for epoch-level reporting\n",
    "    \n",
    "    epoch_loss /= len(train_loader)\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['epoch'].append(epoch_idx)\n",
    "    # update progress bar\n",
    "\n",
    "    # Calculates the average loss for the epoch\n",
    "    # Records the loss and epoch number in the history\n",
    "\n",
    "    # evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "            output = model(noisy_data)\n",
    "            loss_value = loss(output, data)\n",
    "            val_loss += loss_value.detach().cpu().item()\n",
    "        val_loss /= len(valid_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "    # Sets the model to evaluation mode (disables dropout, etc.)\n",
    "    # Disables gradient calculation for efficiency\n",
    "    # Computes the validation loss on the entire validation set\n",
    "    # Records the average validation loss in the history\n",
    "    \n",
    "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
    "    # evaluate on samples\n",
    "    # Updates the progress bar with current epoch, training loss, and validation loss\n",
    "    \n",
    "    sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
    "    # This saves the reconstructions and the latent space thanks to\n",
    "    # the eval_on_samples function where in the application of the \n",
    "    # model to the evaluation data the return_z parameter is set \n",
    "    # to true: \n",
    "    # with torch.no_grad():\n",
    "    #     yz = ae_model(xns, return_z=True)\n",
    "    #     yz = [el.detach().cpu().numpy() for el in yz]\n",
    "# \n",
    "    #     y = yz[0]\n",
    "    #     z = yz[1:]\n",
    "    \n",
    "    # The output of eval_on_samples looks like this: \n",
    "\n",
    "    # sample_res = {'z': z, 'y': y, 'epoch': epoch}\n",
    "\n",
    "    \n",
    "    sample_history[epoch_idx] = sample_res\n",
    "    # Evaluates the model on the sample images\n",
    "    # Stores reconstructions for later visualization\n",
    "\n",
    "    # save model weights\n",
    "    torch.save({\n",
    "                'epoch': epoch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "                }, model_root/f'model_{epoch_idx:03d}.pth')\n",
    "\n",
    "    # Saves a checkpoint of the model at each epoch\n",
    "    # The checkpoint includes:\n",
    "    # \n",
    "    # Current epoch number\n",
    "    # Model parameters\n",
    "    # Optimizer state (allows resuming training)\n",
    "    # Loss function\n",
    "    # \n",
    "    # \n",
    "    # Uses a formatted filename with padded epoch number (e.g., 'model_001.pth')\n",
    "# \n",
    "# This is a complete training pipeline that not only trains the model \n",
    "# but also tracks metrics, evaluates on validation data, \n",
    "# and creates visualizations to monitor progress - \n",
    "# all while saving checkpoints for later analysis or resuming training.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49086add-c088-441c-b59a-f3c4cd44e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c368538-6ce0-43a1-adc7-bc03196da90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_history[59].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36564350-d414-4842-bf92-cb98011cd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample_history[59]['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac6fce-0717-4098-800a-32ef5a8cad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_history[59]['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8df96d-e5ff-43ca-95bc-3917629cdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_history[59]['z'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f1a63-d54b-4215-9b53-c9d6a6493040",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(sample_history, samples=samples, epoch_stride=5, fig_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ddf60e-d278-4021-acb3-a5470c23cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Jupyter notebook cell magic to suppress output (useful for code that might produce verbose output)\n",
    "\n",
    "single_el_idx = samples['single_el_idx']\n",
    "images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
    "images = samples['images'][single_el_idx, 0]\n",
    "# Extracts the indices for the selected sample images\n",
    "# Gets the noisy input images and original clean images for these indices\n",
    "# The , 0 selects the first channel (since these are grayscale images)\n",
    "\n",
    "smpl_ims = []\n",
    "for epoch_idx, hist_el in sample_history.items():\n",
    "    samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
    "    smpl_ims.append(samples_arr)\n",
    "# Creates a list to store image arrays for each epoch\n",
    "# For each epoch in the training history:\n",
    "    # \n",
    "    # Creates an array containing [noisy inputs, reconstructions, original images]\n",
    "    # Adds this array to the list\n",
    "\n",
    "ny, nx = len(smpl_ims[0]), len(smpl_ims[0][0])\n",
    "# Determines the number of rows (3: noisy, reconstructed, original) and columns (number of samples)\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above animations use JavaScript\n",
    "# Sets matplotlib to use JavaScript for HTML animations in Jupyter\n",
    "\n",
    "s=1\n",
    "fig = plt.figure(figsize=(s*nx, s*ny))\n",
    "# Creates a figure with size proportional to the number of images\n",
    "\n",
    "m = mosaic(smpl_ims[0])\n",
    "\n",
    "ttl = plt.title(f'after epoch {int(0)}')\n",
    "# plot 0th epoch - 0th frame\n",
    "imsh = plt.imshow(m, cmap='gray', vmin=-0.5, vmax=0.5)\n",
    "# Creates the initial frame of the animation using the first epoch's images\n",
    "# Uses the mosaic function to arrange the images in a grid\n",
    "# Sets grayscale colormap with value range [-0.5, 0.5]\n",
    "\n",
    "# this function will be called to render each of the frames\n",
    "def animate(i):\n",
    "    m = mosaic(smpl_ims[i])\n",
    "    imsh.set_data(m)\n",
    "\n",
    "    ttl.set_text(f'after epoch {i}')\n",
    "\n",
    "    return imsh\n",
    "\n",
    "# Defines a function to update the plot for each frame of the animation\n",
    "# Creates a mosaic of images for the current epoch\n",
    "# Updates the image data and title text\n",
    "# Returns the updated image object\n",
    "\n",
    "# create animation\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(smpl_ims))\n",
    "\n",
    "# Creates an animation that calls the animate function for each epoch\n",
    "# The result is a dynamic visualization showing how reconstructions evolve throughout training\n",
    "\n",
    "# This animation provides an intuitive way to observe the autoencoder's learning \n",
    "# progress, allowing you to see how the model gradually improves at reconstructing \n",
    "# the original images from the noisy inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea019b-2707-4838-ab69-4b77c5959ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display animation\n",
    "ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c91b8a-6587-4fd4-8324-17c1bf0a52de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above, uses JavaScript\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "# Suppresses output with %%capture\n",
    "# Sets matplotlib to use JavaScript for HTML animations\n",
    "# Creates a square figure with size 8×8 inches\n",
    "\n",
    "labels = samples['labels']\n",
    "epochs = sorted(sample_history.keys())\n",
    "z_res = [sample_history[ep]['z'][0] for ep in epochs]\n",
    "# Gets the digit labels from the samples dictionary\n",
    "# Creates a sorted list of all epoch numbers\n",
    "# Extracts the latent space representations from each epoch\n",
    "\n",
    "scat = plt.scatter(z_res[0][:,14], z_res[0][:,6], c=labels, cmap=cm.rainbow)\n",
    "# Creates a scatter plot using the first two dimensions of the latent space from the first epoch\n",
    "# Colors the points according to their digit labels (0-9)\n",
    "# Uses the rainbow colormap to distinguish between different digits\n",
    "\n",
    "plt.xlim(-60.1, 60.1)\n",
    "plt.ylim(-60.1, 60.1)\n",
    "\n",
    "ax = plt.gca()\n",
    "legend1 = ax.legend(*scat.legend_elements(), title=\"digits\")\n",
    "ax.add_artist(legend1)\n",
    "ax.set_aspect('equal')\n",
    "ttl = plt.title(f'after epoch {0}')\n",
    "# Sets fixed axis limits for consistent visualization across frames\n",
    "# Gets the current axis\n",
    "# Adds a legend showing the mapping between colors and digit classes\n",
    "# Sets the aspect ratio to equal so circles appear as circles\n",
    "# Adds a title showing the current epoch\n",
    "\n",
    "def animate(i):\n",
    "    z = z_res[i]\n",
    "    scat.set_offsets(z)\n",
    "    ttl.set_text(f'after epoch {i}')\n",
    "    return scat\n",
    "\n",
    "# Defines a function to update the plot for each animation frame\n",
    "# Updates the scatter plot with the latent representations from the current epoch\n",
    "# Updates the title text with the current epoch number\n",
    "# Returns the updated scatter plot object\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(z_res))\n",
    "\n",
    "# Creates an animation that runs through all epochs\n",
    "\n",
    "# This animation shows how the model progressively learns to organize \n",
    "# the latent space, with points representing the same digit class gradually \n",
    "# clustering together. It's a powerful visualization that helps understand \n",
    "# how the autoencoder is learning meaningful representations and \n",
    "# separating different classes in the latent space, even though it's \n",
    "# trained in an unsupervised manner without using the labels for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ddfd46-1a92-4e85-8338-2210949b498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad05d5fc-253e-4f64-b44b-5872a997f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space = sample_history[59]['z'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f30b08-eeec-4b0d-a256-86b7c56969b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(perplexity= 1, init=\"pca\", learning_rate=\"auto\", random_state=42)\n",
    "X_valid_2D = tsne.fit_transform(latent_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483946a-8469-4aff-88a8-a686768dd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], s=10, cmap=\"tab10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290fb05-c0e3-44ac-a58f-44c88fd9f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example array with shape (10, 2)\n",
    "# Replace this with your actual data\n",
    "data = X_valid_2D.copy()\n",
    "\n",
    "\n",
    "inertia = []\n",
    "k_range = range(1, 20)  # Test from 1 to 9 clusters\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "    kmeans.fit(data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, inertia, 'o-', color='blue')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e77d0f-ddab-48df-95b1-42d2be6ed84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf82148-fed8-498b-9b45-f87e02e638e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b61fde-ec51-4955-b415-6d7068f605ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of clusters (k)\n",
    "# You can adjust this based on your needs\n",
    "k = 12\n",
    "\n",
    "# Initialize and fit the k-means model\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Get cluster assignments for each data point\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Get the coordinates of the cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Print results\n",
    "print(\"Cluster assignments:\", labels)\n",
    "print(\"Cluster centers:\", centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e0cdd-74b1-4018-8b57-4131890650e2",
   "metadata": {},
   "source": [
    "### Link cluster data from latent space to the respective sample images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9de4c9-740a-492b-998e-a77a8f91b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_indices = list(range(len(labels)))\n",
    "\n",
    "cluster_data = pd.DataFrame({'image_index': image_indices,\n",
    "                            'cluster_label': labels, \n",
    "                             'class_label': samples['labels'], \n",
    "                            'feature_1': data[:, 0], \n",
    "                             'feature_2': data[:, 1]})\n",
    "cluster_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f0f54a-6beb-4a67-835c-4d9d0b07fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latent_space.shape)\n",
    "print(data.shape)\n",
    "print(cluster_data.shape)\n",
    "print(sample_history[59]['y'].shape)\n",
    "print(sample_history[59]['z'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd2028f-cbb2-4892-bf2a-2b1ab05a89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_indexed = {}\n",
    "for idx in cluster_data.image_index:\n",
    "    image = samples['images'][idx][0]\n",
    "    #print(image.shape)\n",
    "    samples_indexed[idx] = image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053c6a4-f24e-4110-9f54-57586ae3fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_history[59]['y'][1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4123cc-6eb7-4a87-8ae2-9176b804ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions_indexed = {}\n",
    "for idx in cluster_data.image_index:\n",
    "    reconstruction = sample_history[59]['y'][idx][0]\n",
    "    reconstructions_indexed[idx] = reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddafa513-d3c6-42b7-bfdc-9381fdb7cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(cluster_data.feature_1, cluster_data.feature_2, c=cluster_data.cluster_label, cmap='viridis', s=100, alpha=0.7)\n",
    "\n",
    "# Add cluster labels at cluster centers\n",
    "unique_clusters = np.unique(cluster_data.cluster_label)\n",
    "for cluster_id in unique_clusters:\n",
    "    # Find points belonging to this cluster\n",
    "    cluster_points = cluster_data[cluster_data.cluster_label == cluster_id]\n",
    "    \n",
    "    # Calculate centroid of this cluster\n",
    "    center_x = cluster_points.feature_1.mean()\n",
    "    center_y = cluster_points.feature_2.mean()\n",
    "    \n",
    "    # Add text label at the center\n",
    "    plt.text(center_x, center_y, str(int(cluster_id)), \n",
    "             fontsize=12, fontweight='bold', ha='center', va='center',\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.title('K-means Clustering Results')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Add colorbar to show cluster mapping\n",
    "plt.colorbar(scatter, label='Cluster ID')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the silhouette score to evaluate the clustering quality\n",
    "from sklearn.metrics import silhouette_score\n",
    "if k > 1 and len(np.unique(labels)) > 1:  # Ensure there are at least 2 clusters with data\n",
    "    score = silhouette_score(data, labels)\n",
    "    print(f\"Silhouette Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793ac31-d3e7-408a-869b-19c30cec9778",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a6fa1-25b8-44e2-a5f3-09a2312c7a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68391609-2566-41c8-bae2-33ca592e42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_from_cluster(cluster_data, cluster_label, image_data_indexed):\n",
    "    cluster_selected = cluster_data[cluster_data.cluster_label == cluster_label]\n",
    "    print(cluster_selected.head())\n",
    "    \n",
    "    # Group images into rows of 3\n",
    "    for i in range(0, len(cluster_selected.image_index), 3):\n",
    "       # Get up to 3 images for this row\n",
    "       row_indices = cluster_selected.image_index[i:i+3]\n",
    "       \n",
    "       # Create subplot with 1 row and number of images in this row\n",
    "       fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "       \n",
    "       # Handle case where there's only one image (axes won't be a list)\n",
    "       if len(row_indices) == 1:\n",
    "           axes = [axes]\n",
    "       \n",
    "       for j, image_index in enumerate(row_indices):\n",
    "           #image = samples_indexed[image_index]\n",
    "           image = image_data_indexed[image_index]\n",
    "           print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "           \n",
    "           axes[j].imshow(image, cmap='gray')\n",
    "           axes[j].axis('off')  # Remove axis labels\n",
    "           axes[j].set_title(f'Index: {image_index}')\n",
    "       \n",
    "       plt.tight_layout()\n",
    "       plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701d2fc-34a9-4bd3-919a-8a9b41454eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 0, reconstructions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad5720c-3aa2-4ed9-9105-12bbd7ad9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 1, reconstructions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90953c-d598-43ec-a139-96b0a6b67747",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 2, reconstructions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57debfc-a4f9-47d3-8bb4-4681ccd35b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 3, reconstructions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8565b77-089b-4002-a3f2-752d5ec67b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 4, reconstructions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4ce72-2376-474f-9be3-704757e5a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 5, reconstructions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c285e-9968-4494-8af9-4d66f0f7b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 6, reconstructions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d5719-86a5-4f9d-9053-062b60a554a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 7, reconstructions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164df5d-76e1-4dd9-819f-9a2d48265e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 8, reconstructions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78345542-da16-494a-92e8-21ca0a8966e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 9, reconstructions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df2b56-5e59-4611-9ab0-5796d18a9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 10, reconstructions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c2b7b-cd87-45a7-84c7-4d744275cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 11, reconstructions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a6708-7d2a-42bc-a7fd-73f2fbf9a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 0, samples_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42707b56-4e3d-4861-90b3-860e0f3c7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 1, samples_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3add2-95a1-4af8-8551-541ab604ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 2, samples_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecef39d-dc29-4881-8f65-dc5b0708b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_cluster(cluster_data, 3, samples_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bea026-7eb5-43cc-b05a-23f735909a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983eaa33-ac8b-4dc3-94dd-97f369d698d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ba4aa-624f-4e0d-99cf-8765768f9296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd23ae1f-babd-4c80-aad7-9f4b33082cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cd6244-d7c8-4b09-ade1-3294c9a6ada3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420db745-7f1a-40c0-b439-e63bba4f455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1 = cluster_data[cluster_data.cluster_label == 1]\n",
    "cluster_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34939c-c251-4ece-b1b4-17217e34798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 1]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25f75c-c923-4248-a9d4-4cfa6e8c481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 2]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6750dfa-58e1-4c38-bf2a-4fbe47584004",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 3]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c6b4b-0b72-4251-9080-aa82bfe9e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 4]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69c12d-c377-427c-8b8a-7bc3f8aa03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 5]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8402f41a-56aa-4af4-a6ce-b24f5393343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 6]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f560e52-3087-4c92-a3f4-0b29079ad928",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 7]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1207080-85aa-40af-8574-0d5a9cf99291",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 8]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf05629-0174-4b19-b121-9a9fcb58f42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28496d-c2ce-493f-9658-a2c945925f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 9]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a9e32-2c97-4767-b32f-9ca7c57a572d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47cdc8-d30f-4b2a-b7f3-840baa3b1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 10]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885ddc2-3afc-4095-93ea-0f8f04d8f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 11]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e2fb6-b9b1-44da-acef-c51e7edb60c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617f83c-f609-4f17-830e-18d9809c8f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 12]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57147a7-4621-4c45-a24d-68be7b3a8394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ea2a1-dcb3-4792-a092-a75795112e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 13]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2f206-7aea-4494-8327-88b535b9ba11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1219511a-70d0-40f1-b200-9d141e3ed81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 14]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10344c70-dc8b-4f5d-8e93-bb0641d2c2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81104976-102d-49b3-81c8-ba1e95ea4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 15]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3eadce-3f85-4d46-ad59-40e8d08899d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b12585-6927-4f44-9d8b-2a19e39c9b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 16]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3522e6c-63ae-4df6-939d-4209a2463e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec0a33-0d64-4f20-9313-dd555e1e4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 17]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "# Group images into rows of 3\n",
    "for i in range(0, len(cluster_selected.image_index), 3):\n",
    "   # Get up to 3 images for this row\n",
    "   row_indices = cluster_selected.image_index[i:i+3]\n",
    "   \n",
    "   # Create subplot with 1 row and number of images in this row\n",
    "   fig, axes = plt.subplots(1, len(row_indices), figsize=(15, 5))\n",
    "   \n",
    "   # Handle case where there's only one image (axes won't be a list)\n",
    "   if len(row_indices) == 1:\n",
    "       axes = [axes]\n",
    "   \n",
    "   for j, image_index in enumerate(row_indices):\n",
    "       image = samples_indexed[image_index]\n",
    "       print(f\"Row {i//3 + 1}, Image {j+1}: {image.shape}\")\n",
    "       \n",
    "       axes[j].imshow(image, cmap='gray')\n",
    "       axes[j].axis('off')  # Remove axis labels\n",
    "       axes[j].set_title(f'Index: {image_index}')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f204373-8a2a-455d-a169-0850913b16fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5e04e-3b1a-4374-b085-8fa5e749d753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba2d56-f9a5-45ba-8ce9-5b9f33c1f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_selected = cluster_data[cluster_data.cluster_label == 4]\n",
    "print(cluster_selected.head())\n",
    "\n",
    "label_sum = sum(cluster_selected.class_label)\n",
    "label_proportion = label_sum/cluster_selected.shape[0]\n",
    "label_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a9ecf-e50f-442a-bca1-2b7bfc9a6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_label_proportion(cluster_selected, label_name):\n",
    "    label_sum = sum(cluster_selected[label_name])\n",
    "    label_proportion = label_sum/cluster_selected.shape[0]\n",
    "    return label_proportion\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8d207-8cea-40b2-9d7e-42f94f2dc685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c4659-a7f8-4179-b060-6809f5300687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac66438-fb16-4546-a2a7-2c40014bb14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_proportions_in_clusters = {}\n",
    "\n",
    "for cluster_label in cluster_data.cluster_label:\n",
    "    cluster_selected = cluster_data[cluster_data.cluster_label == cluster_label]\n",
    "    label_proportion = calculate_label_proportion(cluster_selected, 'class_label')\n",
    "    label_proportions_in_clusters[cluster_label] = label_proportion\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd2ebf-b126-4d94-9402-afb41f40c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_proportions_in_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7b57c-c6df-420d-b84d-a1702a5ed2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c8655-9575-4875-a5cd-a7c9475818c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_bar_chart(data_dict, title=\"Bar Chart\", xlabel=\"Keys\", ylabel=\"Values\", figsize=(12, 6)):\n",
    "   \"\"\"\n",
    "   Create a bar plot from a dictionary with sorted keys.\n",
    "   \n",
    "   Args:\n",
    "       data_dict: Dictionary with keys as x-axis labels and values as bar heights\n",
    "       title: Plot title\n",
    "       xlabel: X-axis label\n",
    "       ylabel: Y-axis label\n",
    "       figsize: Figure size tuple\n",
    "   \"\"\"\n",
    "   # Sort the dictionary by keys\n",
    "   sorted_items = sorted(data_dict.items())\n",
    "   keys = [item[0] for item in sorted_items]\n",
    "   values = [item[1] for item in sorted_items]\n",
    "   \n",
    "   # Create the bar plot\n",
    "   plt.figure(figsize=figsize)\n",
    "   plt.bar(keys, values)\n",
    "   plt.title(title)\n",
    "   plt.xlabel(xlabel)\n",
    "   plt.ylabel(ylabel)\n",
    "   plt.grid(axis='y', alpha=0.3)\n",
    "   \n",
    "   # Ensure all keys are shown as tick labels\n",
    "   plt.xticks(keys)\n",
    "   \n",
    "   # Rotate x-axis labels if there are many keys\n",
    "   if len(keys) > 10:\n",
    "       plt.xticks(rotation=45)\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "## Usage with your data:\n",
    "#data = {11: 0.5, 0: 0.2692307692307692, 9: 0.6153846153846154, 2: 0.6666666666666666, \n",
    "#       5: 0.43478260869565216, 14: 0.36666666666666664, 4: 0.6, 8: 0.7083333333333334, \n",
    "#       3: 0.46153846153846156, 15: 0.18181818181818182, 1: 0.5833333333333334, \n",
    "#       13: 0.6666666666666666, 16: 0.5, 7: 0.3333333333333333, 12: 0.8, \n",
    "#       10: 0.5833333333333334, 6: 0.3333333333333333}\n",
    "#\n",
    "#plot_bar_chart(data, title=\"Data Distribution\", xlabel=\"Categories\", ylabel=\"Values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d1467-220e-4b1b-9573-ca4aeb0fc87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_bar_chart(data_dict, title=\"Bar Chart\", xlabel=\"Keys\", ylabel=\"Values\", figsize=(12, 6), \n",
    "                   title_fontsize=16, label_fontsize=14, tick_fontsize=12):\n",
    "    \"\"\"\n",
    "    Create a bar plot from a dictionary with sorted keys.\n",
    "    \n",
    "    Args:\n",
    "        data_dict: Dictionary with keys as x-axis labels and values as bar heights\n",
    "        title: Plot title\n",
    "        xlabel: X-axis label\n",
    "        ylabel: Y-axis label\n",
    "        figsize: Figure size tuple\n",
    "        title_fontsize: Font size for the title\n",
    "        label_fontsize: Font size for axis labels\n",
    "        tick_fontsize: Font size for tick labels\n",
    "    \"\"\"\n",
    "    # Sort the dictionary by keys\n",
    "    sorted_items = sorted(data_dict.items())\n",
    "    keys = [item[0] for item in sorted_items]\n",
    "    values = [item[1] for item in sorted_items]\n",
    "    \n",
    "    # Create the bar plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.bar(range(len(keys)), values)  # Use range for x-positions\n",
    "    plt.title(title, fontsize=title_fontsize)\n",
    "    plt.xlabel(xlabel, fontsize=label_fontsize)\n",
    "    plt.ylabel(ylabel, fontsize=label_fontsize)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Set x-axis ticks and labels properly for categorical data\n",
    "    plt.xticks(range(len(keys)), keys, fontsize=tick_fontsize)\n",
    "    plt.yticks(fontsize=tick_fontsize)\n",
    "    \n",
    "    # Rotate x-axis labels if there are many keys\n",
    "    if len(keys) > 10:\n",
    "        plt.xticks(range(len(keys)), keys, rotation=45, fontsize=tick_fontsize)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da7824-0d47-4353-980f-7d0c186aab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_chart(label_proportions_in_clusters, xlabel='clusters', ylabel='proportion of class label', label_fontsize=18, tick_fontsize=18, figsize=(17, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c387949-e804-41ff-a154-29a40618a198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed302e-53ba-43ce-8c31-39ec0db372ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31cecc-5826-45ed-8257-2512d2612855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec5ea7-c209-4901-adfd-2a45bce3a951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484170a5-c7de-4d9f-af69-ab1e359e478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(sample_history, samples, epoch_stride=5, fig_scale=3, max_images_per_row=None, title_fontsize=16):\n",
    "    \"\"\"\n",
    "    Plots input, noisy samples (for DAE) and reconstruction.\n",
    "    Each `epoch_stride`-th epoch\n",
    "    \n",
    "    Args:\n",
    "        max_images_per_row: Maximum number of images to display per row (None = all)\n",
    "        fig_scale: Scale factor for figure size (increased default from 1 to 3)\n",
    "        title_fontsize: Font size for the plot title\n",
    "    \"\"\"\n",
    "    single_el_idx = samples['single_el_idx']\n",
    "    \n",
    "    # Limit the number of images if specified\n",
    "    if max_images_per_row is not None:\n",
    "        single_el_idx = single_el_idx[:max_images_per_row]\n",
    "    \n",
    "    images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
    "    images = samples['images'][single_el_idx, 0]\n",
    "    \n",
    "    last_epoch = np.max(list(sample_history.keys()))\n",
    "    \n",
    "    for epoch_idx, hist_el in sample_history.items():\n",
    "        if epoch_idx % epoch_stride != 0 and epoch_idx != last_epoch:\n",
    "            continue\n",
    "            \n",
    "        samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
    "        \n",
    "        ny = len(samples_arr)\n",
    "        nx = len(samples_arr[0])\n",
    "        \n",
    "        # Increased figure size calculation for bigger images\n",
    "        plt.figure(figsize=(fig_scale*nx*1.5, fig_scale*ny*1.5))\n",
    "        \n",
    "        m = mosaic(samples_arr)\n",
    "        plt.title(f'after epoch {int(epoch_idx)}', fontsize=title_fontsize)  # Customizable title font\n",
    "        plt.imshow(m, cmap='gray', vmin=-.5, vmax=.5)\n",
    "        \n",
    "        plt.tight_layout(pad=0.1, h_pad=0, w_pad=0)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130be050-b43b-443a-8548-2682bf094874",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLE = 32\n",
    "N_VIS_SAMPLE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f7571-d4e2-4520-9f93-cdec446dfe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d64066-e546-4b4d-bef5-c9898e91e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (1.0,))\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    lambda x: torch.LongTensor([x])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da818de-1858-47f2-8490-42c0b8746480",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Same interface as MNIST\n",
    "fashion_train = datasets.FashionMNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform, \n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "fashion_test = datasets.FashionMNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform, \n",
    "    target_transform=target_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66e9f2-3df7-40e2-9f33-5819cec9f086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379c72f-0390-4cc2-a93f-c570599e576d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ad835-74b4-4ec0-acfc-0fa61f832717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f195407-36f6-410a-adc2-855e9bd120ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 4: Create DataLoader with your original collate function\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    fashion_train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_ae_dataset,  # Your original function using globals\n",
    "    drop_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e88941-bb23-43d9-b676-dd7a00b6ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_train.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36881acc-51e9-4b0c-aa55-9892205ef79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    fashion_test, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_ae_dataset,  # Your original function using globals\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8aa9a-9623-4744-86d0-87c5aaa3d38f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce35c55-f562-4463-a307-e08e5f6ef248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227d538-2ba4-43b0-abe8-8a31b25e891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic code for train_loader object\n",
    "print(\"=== TRAIN_LOADER Properties ===\")\n",
    "print(f\"Type: {type(train_loader)}\")\n",
    "print(f\"Length (number of batches): {len(train_loader)}\")\n",
    "\n",
    "# Check all attributes\n",
    "print(f\"\\nAttributes: {[attr for attr in dir(train_loader) if not attr.startswith('_')]}\")\n",
    "\n",
    "# Check key properties\n",
    "print(f\"batch_size: {train_loader.batch_size}\")\n",
    "#print(f\"shuffle: {train_loader.shuffle}\")\n",
    "print(f\"drop_last: {train_loader.drop_last}\")\n",
    "print(f\"collate_fn: {train_loader.collate_fn}\")\n",
    "print(f\"dataset type: {type(train_loader.dataset)}\")\n",
    "print(f\"dataset length: {len(train_loader.dataset)}\")\n",
    "\n",
    "# Test getting one batch\n",
    "print(f\"\\n=== BATCH SAMPLE ===\")\n",
    "for batch in train_loader:\n",
    "   print(f\"Batch type: {type(batch)}\")\n",
    "   print(f\"Batch length: {len(batch)}\")\n",
    "   \n",
    "   # Unpack the batch (based on your collate function returning 3 items)\n",
    "   noisy_img, clean_img, labels = batch\n",
    "   \n",
    "   print(f\"\\nnoisy_img type: {type(noisy_img)}\")\n",
    "   print(f\"noisy_img shape: {noisy_img.shape}\")\n",
    "   print(f\"noisy_img dtype: {noisy_img.dtype}\")\n",
    "   print(f\"noisy_img device: {noisy_img.device}\")\n",
    "   \n",
    "   print(f\"\\nclean_img type: {type(clean_img)}\")\n",
    "   print(f\"clean_img shape: {clean_img.shape}\")\n",
    "   print(f\"clean_img dtype: {clean_img.dtype}\")\n",
    "   print(f\"clean_img device: {clean_img.device}\")\n",
    "   \n",
    "   print(f\"\\nlabels type: {type(labels)}\")\n",
    "   print(f\"labels shape: {labels.shape}\")\n",
    "   print(f\"labels dtype: {labels.dtype}\")\n",
    "   print(f\"labels device: {labels.device}\")\n",
    "   \n",
    "   break  # Only test first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63820163-8ca0-4741-909b-02cadd3ffc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a4d2a-b62e-4bae-9d90-e9ee14bf3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "    print('batch_idx:')\n",
    "    print(batch_idx)\n",
    "    print('type of noisy_data:')\n",
    "    print(type(noisy_data))\n",
    "    print('type of data:')\n",
    "    print(type(data))\n",
    "    print('dimensions of data:')\n",
    "    print(data.data.shape)\n",
    "    print('type of target:')\n",
    "    print(type(target))\n",
    "    print('dimensions of target:')\n",
    "    print(target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7dc9a-d761-414b-801d-a38319ca10ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc51298-8b06-4f19-a9ec-c0932cbc1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = (28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f672aab-46b4-4e27-8d43-44822e0cc754",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_spatial_size = 320 // (2**6)\n",
    "\n",
    "#in_size = target_size\n",
    "#in_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3679b262-f8b5-46ee-be21-334fa7c1afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoEncoder(AutoEncoder):\n",
    "    def __init__(self, input_size, code_size):\n",
    "        self.input_size = list(input_size)\n",
    "        \n",
    "        # For 28x28 input with 2 stride=2 layers: 28÷4 = 7x7 final spatial size\n",
    "        # With 32 channels: final feature map is 32×7×7 = 1568 values\n",
    "        self.hidden_size = 32 * 7 * 7  # = 1568\n",
    "        \n",
    "        self.code_size = code_size\n",
    "        \n",
    "        super(ConvolutionalAutoEncoder, self).__init__(input_size, code_size)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1, stride=1), nn.LeakyReLU(negative_slope=0.3),   # 28×28×8\n",
    "            nn.Conv2d(8, 16, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),  # 14×14×16\n",
    "            nn.Conv2d(16, 32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3), # 7×7×32\n",
    "            \n",
    "            nn.Flatten(),  # 7×7×32 = 1568 values\n",
    "            \n",
    "            nn.Linear(1568, 128), nn.LeakyReLU(negative_slope=0.3),  # 1568 → 128\n",
    "            nn.Linear(128, code_size),  # 128 → code_size\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(code_size, 1568), nn.LeakyReLU(negative_slope=0.3),  # code_size → 1568\n",
    "            \n",
    "            nn.Unflatten(1, (32, 7, 7)),  # 1568 → 32×7×7\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 16, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),  # 7→14\n",
    "            nn.ConvTranspose2d(16, 8, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),   # 14→28\n",
    "            \n",
    "            nn.Conv2d(8, 1, 3, padding=1, stride=1), nn.Tanh(),  # Final output: 28×28×1\n",
    "        )\n",
    "\n",
    "    def decode(self, z):\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fbb925-70f4-43a5-ab74-0cd6bed32d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = target_size\n",
    "model = ConvolutionalAutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
    "\n",
    "#samples = get_samples(valid_loader)\n",
    "samples = get_samples(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420af69-f40d-4b44-9b2f-b90dd6e6e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "xns = torch.tensor(samples['images_noisy']).to(device)\n",
    "print(xns.shape)\n",
    "zs = model.encode(xns)\n",
    "ys = model(xns)\n",
    "print(zs.shape)\n",
    "print(ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd7501-b698-4d81-9cb7-4390ce7fd089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the autoencoder model, for N_EPOCHS epochs,\n",
    "# save history of loss values for training and validation sets,\n",
    "# history of validation samples evolution, and model weights history,\n",
    "\n",
    "\n",
    "# This code implements the complete training loop for the autoencoder. \n",
    "# Let me break it down:\n",
    "\n",
    "\n",
    "LR = 0.0009\n",
    "# Sets the number of training epochs to 50\n",
    "# Sets the learning rate for the Adam optimizer to 0.0009\n",
    "\n",
    "model_root = pl.Path(MODEL_NAME)\n",
    "model_root.mkdir(exist_ok=True)\n",
    "# Creates a directory path for saving model checkpoints using the MODEL_NAME ('ae_model')\n",
    "# Makes sure the directory exists (creates it if it doesn't)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "# Creates an Adam optimizer to update the model parameters\n",
    "# Adam is an adaptive learning rate optimization algorithm well-suited for deep learning\n",
    "\n",
    "# implement loss explicitly\n",
    "loss = nn.MSELoss()\n",
    "# Defines the loss function as Mean Squared Error (MSE)\n",
    "# This measures the average squared difference between the reconstructed and target images\n",
    "\n",
    "# train the model\n",
    "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
    "sample_history = {}\n",
    "# Creates dictionaries to store training metrics and sample reconstructions\n",
    "# history tracks training and validation losses across epochs\n",
    "# sample_history will store sample reconstruction results at each epoch\n",
    "\n",
    "pbar = tqdm.tqdm(range(N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
    "# Creates a progress bar for tracking the training process\n",
    "# Will show the current epoch and update metrics during training\n",
    "\n",
    "for epoch_idx in pbar:\n",
    "# Starts the main training loop that runs for N_EPOCHS iterations\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # Initializes the epoch loss accumulator\n",
    "    # Sets the model to training mode (enables dropout, batch normalization updates, etc.)\n",
    "    \n",
    "    for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(noisy_data)\n",
    "        loss_value = loss(output, data)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value.detach().cpu().item()\n",
    "    # Iterates through all batches in the training dataset\n",
    "    # For each batch:\n",
    "    # \n",
    "        # Zeros out previous gradients\n",
    "        # Passes the noisy input through the model\n",
    "        # Calculates the MSE loss between the reconstruction and clean data\n",
    "        # Computes gradients via backpropagation\n",
    "        # Updates model parameters using the optimizer\n",
    "        # Accumulates the loss value for epoch-level reporting\n",
    "    \n",
    "    epoch_loss /= len(train_loader)\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['epoch'].append(epoch_idx)\n",
    "    # update progress bar\n",
    "\n",
    "    # Calculates the average loss for the epoch\n",
    "    # Records the loss and epoch number in the history\n",
    "\n",
    "    # evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "            output = model(noisy_data)\n",
    "            loss_value = loss(output, data)\n",
    "            val_loss += loss_value.detach().cpu().item()\n",
    "        val_loss /= len(valid_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "    # Sets the model to evaluation mode (disables dropout, etc.)\n",
    "    # Disables gradient calculation for efficiency\n",
    "    # Computes the validation loss on the entire validation set\n",
    "    # Records the average validation loss in the history\n",
    "    \n",
    "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
    "    # evaluate on samples\n",
    "    # Updates the progress bar with current epoch, training loss, and validation loss\n",
    "    \n",
    "    sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
    "    # This saves the reconstructions and the latent space thanks to\n",
    "    # the eval_on_samples function where in the application of the \n",
    "    # model to the evaluation data the return_z parameter is set \n",
    "    # to true: \n",
    "    # with torch.no_grad():\n",
    "    #     yz = ae_model(xns, return_z=True)\n",
    "    #     yz = [el.detach().cpu().numpy() for el in yz]\n",
    "# \n",
    "    #     y = yz[0]\n",
    "    #     z = yz[1:]\n",
    "    \n",
    "    # The output of eval_on_samples looks like this: \n",
    "\n",
    "    # sample_res = {'z': z, 'y': y, 'epoch': epoch}\n",
    "\n",
    "    \n",
    "    sample_history[epoch_idx] = sample_res\n",
    "    # Evaluates the model on the sample images\n",
    "    # Stores reconstructions for later visualization\n",
    "\n",
    "    # save model weights\n",
    "    torch.save({\n",
    "                'epoch': epoch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "                }, model_root/f'model_{epoch_idx:03d}.pth')\n",
    "\n",
    "    # Saves a checkpoint of the model at each epoch\n",
    "    # The checkpoint includes:\n",
    "    # \n",
    "    # Current epoch number\n",
    "    # Model parameters\n",
    "    # Optimizer state (allows resuming training)\n",
    "    # Loss function\n",
    "    # \n",
    "    # \n",
    "    # Uses a formatted filename with padded epoch number (e.g., 'model_001.pth')\n",
    "# \n",
    "# This is a complete training pipeline that not only trains the model \n",
    "# but also tracks metrics, evaluates on validation data, \n",
    "# and creates visualizations to monitor progress - \n",
    "# all while saving checkpoints for later analysis or resuming training.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dcf908-0c87-42d3-bf1c-dac816d0b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c05457-2ff8-4b3d-a103-62ba65597289",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(sample_history, samples=samples, epoch_stride=5, fig_scale=7, max_images_per_row=4,  title_fontsize=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361cb660-7b4c-4b38-b8fb-3f941b77c838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eebb164-298a-42ae-bb14-3bf3b34d783e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd508738-70a1-4618-953d-a50a1fd7bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Group indices by class\n",
    "class_indices = defaultdict(list)\n",
    "for i, (_, label) in enumerate(fashion_train):\n",
    "    class_indices[label.item()].append(i)\n",
    "\n",
    "# Take half from each class\n",
    "subset_indices = []\n",
    "for class_label, indices in class_indices.items():\n",
    "    tenth_class_size = len(indices) // 30\n",
    "    random.shuffle(indices)\n",
    "    subset_indices.extend(indices[:tenth_class_size])\n",
    "\n",
    "# Create subset\n",
    "subset_fashion_train = Subset(fashion_train, subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d092e647-d188-4d56-b3a5-b042979c5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For subset, you need to access through the underlying dataset\n",
    "print(f\"Subset underlying data shape: {subset_fashion_train.dataset.data.shape}\")\n",
    "print(f\"Subset underlying targets shape: {subset_fashion_train.dataset.targets.shape}\")\n",
    "\n",
    "# To get the actual subset data/targets:\n",
    "subset_data = subset_fashion_train.dataset.data[subset_fashion_train.indices]\n",
    "subset_targets = subset_fashion_train.dataset.targets[subset_fashion_train.indices]\n",
    "print(f\"Actual subset data shape: {subset_data.shape}\")\n",
    "print(f\"Actual subset targets shape: {subset_targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e21f8-a5e9-490e-b92c-083d9bdde80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create DataLoader with your original collate function\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    subset_fashion_train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_ae_dataset,  # Your original function using globals\n",
    "    drop_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b1728-1474-464a-a6a1-285fb44cc186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoEncoder(AutoEncoder):\n",
    "    def __init__(self, input_size, code_size):\n",
    "        self.input_size = list(input_size)\n",
    "        \n",
    "        # For 28x28 input with 2 stride=2 layers: 28÷4 = 7x7 final spatial size\n",
    "        # With 32 channels: final feature map is 32×7×7 = 1568 values\n",
    "        self.hidden_size = 32 * 7 * 7  # = 1568\n",
    "        \n",
    "        self.code_size = code_size\n",
    "        \n",
    "        super(ConvolutionalAutoEncoder, self).__init__(input_size, code_size)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1, stride=1), nn.LeakyReLU(negative_slope=0.3),   # 28×28×8\n",
    "            nn.Conv2d(8, 16, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),  # 14×14×16\n",
    "            nn.Conv2d(16, 32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3), # 7×7×32\n",
    "            \n",
    "            nn.Flatten(),  # 7×7×32 = 1568 values\n",
    "            \n",
    "            nn.Linear(1568, 128), nn.LeakyReLU(negative_slope=0.3),  # 1568 → 128\n",
    "            nn.Linear(128, code_size),  # 128 → code_size\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(code_size, 1568), nn.LeakyReLU(negative_slope=0.3),  # code_size → 1568\n",
    "            \n",
    "            nn.Unflatten(1, (32, 7, 7)),  # 1568 → 32×7×7\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 16, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),  # 7→14\n",
    "            nn.ConvTranspose2d(16, 8, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),   # 14→28\n",
    "            \n",
    "            nn.Conv2d(8, 1, 3, padding=1, stride=1), nn.Tanh(),  # Final output: 28×28×1\n",
    "        )\n",
    "\n",
    "    def decode(self, z):\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d656e62-9c41-45d0-8ff8-3fcb70c6773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = target_size\n",
    "model = ConvolutionalAutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
    "\n",
    "#samples = get_samples(valid_loader)\n",
    "samples = get_samples(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d343aa-b0af-4858-8590-29c27542aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "xns = torch.tensor(samples['images_noisy']).to(device)\n",
    "print(xns.shape)\n",
    "zs = model.encode(xns)\n",
    "ys = model(xns)\n",
    "print(zs.shape)\n",
    "print(ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccac2a-6569-46b7-bd58-c4fee91274a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the autoencoder model, for N_EPOCHS epochs,\n",
    "# save history of loss values for training and validation sets,\n",
    "# history of validation samples evolution, and model weights history,\n",
    "\n",
    "\n",
    "# This code implements the complete training loop for the autoencoder. \n",
    "# Let me break it down:\n",
    "\n",
    "\n",
    "LR = 0.0009\n",
    "# Sets the number of training epochs to 50\n",
    "# Sets the learning rate for the Adam optimizer to 0.0009\n",
    "\n",
    "model_root = pl.Path(MODEL_NAME)\n",
    "model_root.mkdir(exist_ok=True)\n",
    "# Creates a directory path for saving model checkpoints using the MODEL_NAME ('ae_model')\n",
    "# Makes sure the directory exists (creates it if it doesn't)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "# Creates an Adam optimizer to update the model parameters\n",
    "# Adam is an adaptive learning rate optimization algorithm well-suited for deep learning\n",
    "\n",
    "# implement loss explicitly\n",
    "loss = nn.MSELoss()\n",
    "# Defines the loss function as Mean Squared Error (MSE)\n",
    "# This measures the average squared difference between the reconstructed and target images\n",
    "\n",
    "# train the model\n",
    "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
    "sample_history = {}\n",
    "# Creates dictionaries to store training metrics and sample reconstructions\n",
    "# history tracks training and validation losses across epochs\n",
    "# sample_history will store sample reconstruction results at each epoch\n",
    "\n",
    "pbar = tqdm.tqdm(range(N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
    "# Creates a progress bar for tracking the training process\n",
    "# Will show the current epoch and update metrics during training\n",
    "\n",
    "for epoch_idx in pbar:\n",
    "# Starts the main training loop that runs for N_EPOCHS iterations\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # Initializes the epoch loss accumulator\n",
    "    # Sets the model to training mode (enables dropout, batch normalization updates, etc.)\n",
    "    \n",
    "    for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(noisy_data)\n",
    "        loss_value = loss(output, data)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value.detach().cpu().item()\n",
    "    # Iterates through all batches in the training dataset\n",
    "    # For each batch:\n",
    "    # \n",
    "        # Zeros out previous gradients\n",
    "        # Passes the noisy input through the model\n",
    "        # Calculates the MSE loss between the reconstruction and clean data\n",
    "        # Computes gradients via backpropagation\n",
    "        # Updates model parameters using the optimizer\n",
    "        # Accumulates the loss value for epoch-level reporting\n",
    "    \n",
    "    epoch_loss /= len(train_loader)\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['epoch'].append(epoch_idx)\n",
    "    # update progress bar\n",
    "\n",
    "    # Calculates the average loss for the epoch\n",
    "    # Records the loss and epoch number in the history\n",
    "\n",
    "    # evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "            output = model(noisy_data)\n",
    "            loss_value = loss(output, data)\n",
    "            val_loss += loss_value.detach().cpu().item()\n",
    "        val_loss /= len(valid_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "    # Sets the model to evaluation mode (disables dropout, etc.)\n",
    "    # Disables gradient calculation for efficiency\n",
    "    # Computes the validation loss on the entire validation set\n",
    "    # Records the average validation loss in the history\n",
    "    \n",
    "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
    "    # evaluate on samples\n",
    "    # Updates the progress bar with current epoch, training loss, and validation loss\n",
    "    \n",
    "    sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
    "    # This saves the reconstructions and the latent space thanks to\n",
    "    # the eval_on_samples function where in the application of the \n",
    "    # model to the evaluation data the return_z parameter is set \n",
    "    # to true: \n",
    "    # with torch.no_grad():\n",
    "    #     yz = ae_model(xns, return_z=True)\n",
    "    #     yz = [el.detach().cpu().numpy() for el in yz]\n",
    "# \n",
    "    #     y = yz[0]\n",
    "    #     z = yz[1:]\n",
    "    \n",
    "    # The output of eval_on_samples looks like this: \n",
    "\n",
    "    # sample_res = {'z': z, 'y': y, 'epoch': epoch}\n",
    "\n",
    "    \n",
    "    sample_history[epoch_idx] = sample_res\n",
    "    # Evaluates the model on the sample images\n",
    "    # Stores reconstructions for later visualization\n",
    "\n",
    "    # save model weights\n",
    "    torch.save({\n",
    "                'epoch': epoch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "                }, model_root/f'model_{epoch_idx:03d}.pth')\n",
    "\n",
    "    # Saves a checkpoint of the model at each epoch\n",
    "    # The checkpoint includes:\n",
    "    # \n",
    "    # Current epoch number\n",
    "    # Model parameters\n",
    "    # Optimizer state (allows resuming training)\n",
    "    # Loss function\n",
    "    # \n",
    "    # \n",
    "    # Uses a formatted filename with padded epoch number (e.g., 'model_001.pth')\n",
    "# \n",
    "# This is a complete training pipeline that not only trains the model \n",
    "# but also tracks metrics, evaluates on validation data, \n",
    "# and creates visualizations to monitor progress - \n",
    "# all while saving checkpoints for later analysis or resuming training.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45d0e2-bc03-47dc-9ebe-231dd2dc7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05c796-fe96-4ff0-a7f7-46b5f8254483",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(sample_history, samples=samples, epoch_stride=5, fig_scale=7, max_images_per_row=4, title_fontsize=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f91e8-1549-4fde-9456-741ad1face3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c898e9e8-da02-4af6-a9c7-639184ae5141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbdef7b-627f-4b23-bd9c-c61e6cf9880c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e092a8c-c5c6-4566-8e86-648d9d92d9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d705f-2fbe-41cb-822f-f3ec771f2f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d121d-540f-4886-ad67-b22fcc2c1162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63532f6d-e0c8-43e9-906a-6449aaa756da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7fe78b-bff5-4893-b26e-9c877907285a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b83b8-de67-41ee-8035-f18b1c77c670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
