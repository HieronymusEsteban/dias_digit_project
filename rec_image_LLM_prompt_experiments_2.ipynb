{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5de50bf5-56eb-40d7-8016-22959f1f1264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from source import image_id_converter as img_idc\n",
    "from source import sort_img_files as sif\n",
    "from source import llm_input as llm_i\n",
    "from source import llm_output as llm_o\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d38edd-bef0-48c4-84c5-8c89603d07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ebd0d8e-bb05-411b-9684-8e0edd348996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/storage/homefs/sh98e089'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7bea40-b27c-431f-84d2-40c986b376eb",
   "metadata": {},
   "source": [
    "# Using LLM (mini-CPM) for image analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8419f-b468-417d-963b-e80799c4f034",
   "metadata": {},
   "source": [
    "### Define Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d271ab-8183-41e0-b6ff-c175830a1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_prompt():\n",
    "    \"\"\"Create the structured prompt for image analysis.\"\"\"\n",
    "    return \"\"\"\n",
    "    Analyze this image and return ONLY a Python dictionary in exactly this format:\n",
    "    \n",
    "    {\n",
    "        'image_is_photography': X,  # True if the image is a photography, False otherwise\n",
    "        'additional_comments': '' # Any additional observations or empty string if none\n",
    "    }\n",
    "    \n",
    "    Replace X with True (image is a photography) or False (image is not a photography).\n",
    "    Return ONLY the dictionary, no other text.\n",
    "    Your answer MUST have the exact structue of the dictionary described above (all keys MUST be present). \n",
    "    If you cannot answer the question in the way implied by this structure, enter 'None' as value and offer \n",
    "    your answer and explanations under 'additional_comments'.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "578097bb-67ae-4f4b-b4b4-6e50b9c3ed45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9fb3f1-c6af-4fda-8ad7-a271bc19c6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "751ff193-0a60-4050-a286-d8e004156fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More comprehensive check including empty strings and whitespace\n",
    "def has_missing_comprehensive(df):\n",
    "   # Standard missing values\n",
    "   has_standard_missing = df.isnull().any().any()\n",
    "   \n",
    "   # Empty strings and whitespace-only strings\n",
    "   has_empty_strings = False\n",
    "   for col in df.select_dtypes(include=['object']):\n",
    "       if (df[col].astype(str).str.strip() == '').any():\n",
    "           has_empty_strings = True\n",
    "           break\n",
    "   \n",
    "   return has_standard_missing or has_empty_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1efc1130-c8c9-41c9-a4a4-a24d09a13753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conf_matrix_tag(labels_results, label, prediction, cases, filename_tag):\n",
    "    true_positives, false_positives, true_negatives, false_negatives, positives, negatives = cases\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(labels_results[label], labels_results[prediction])\n",
    "    \n",
    "    number_true_positives = true_positives.shape[0]\n",
    "    number_false_positives = false_positives.shape[0]\n",
    "    number_true_negatives = true_negatives.shape[0]\n",
    "    number_false_negatives = false_negatives.shape[0]\n",
    "    \n",
    "    sensitivity = number_true_positives / positives.shape[0]\n",
    "    specificity = number_true_negatives / negatives.shape[0]\n",
    "    \n",
    "    if (number_true_positives > 0) and (number_false_positives > 0):\n",
    "        precision = number_true_positives / (number_true_positives + number_false_positives)\n",
    "        f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
    "    else:\n",
    "        precision = None\n",
    "        f1_score = None\n",
    "        \n",
    "    if positives.shape[0] > 0:\n",
    "        miss_rate = number_false_negatives / positives.shape[0]\n",
    "    else:\n",
    "        miss_rate = None\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15,8))\n",
    "    gs = plt.GridSpec(1, 2, width_ratios=[2, 1])\n",
    "    \n",
    "    plt.subplot(gs[0])\n",
    "    confusion_matrix_data = [[number_true_negatives, number_false_positives], \n",
    "                             [number_false_negatives, number_true_positives]]\n",
    "    heatmap = sns.heatmap(confusion_matrix_data, annot=True, fmt='d', \n",
    "               xticklabels=['Predicted Negative', 'Predicted Positive'], \n",
    "               yticklabels=['Actual Negative', 'Actual Positive'],\n",
    "               cbar_kws={'label': 'Number of Instances'})\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    plt.subplot(gs[1])\n",
    "    plt.axis('off')\n",
    "    if precision is not None:\n",
    "        metrics_text = (f'Performance Metrics:\\n\\n'\n",
    "                       f'True Positives: {number_true_positives}\\n'\n",
    "                       f'False Positives: {number_false_positives}\\n'\n",
    "                       f'True Negatives: {number_true_negatives}\\n'\n",
    "                       f'False Negatives: {number_false_negatives}\\n\\n'\n",
    "                       f'Sensitivity: {sensitivity:.4f}\\n'\n",
    "                       f'Specificity: {specificity:.4f}\\n'\n",
    "                       f'Precision: {precision:.4f}\\n'\n",
    "                       f'Miss Rate: {miss_rate:.4f}\\n'\n",
    "                       f'F1 Score: {f1_score:.4f}')\n",
    "    else:\n",
    "        metrics_text = (f'Performance Metrics:\\n\\n'\n",
    "                       f'True Positives: {number_true_positives}\\n'\n",
    "                       f'False Positives: {number_false_positives}\\n'\n",
    "                       f'True Negatives: {number_true_negatives}\\n'\n",
    "                       f'False Negatives: {number_false_negatives}\\n\\n'\n",
    "                       f'Sensitivity: {sensitivity:.4f}\\n'\n",
    "                       f'Specificity: {specificity:.4f}\\n')\n",
    "        \n",
    "    plt.text(0, 0.5, metrics_text, fontsize=10, \n",
    "            verticalalignment='center')\n",
    "    \n",
    "    plt.suptitle('Photography Detection: Confusion Matrix and Performance Metrics Based on is_photo Label as Ground Truth', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    filename = 'conf_matrix_metrics_' + filename_tag + '.pdf'\n",
    "    output_path = data_path / filename\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65aa5948-cd95-4df4-b522-6c2c2755d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_duration(time_analysis_dict, time_analysis_for_df_dict, analysis_name, duration,\n",
    "                  timestamp_start_is_photo_analysis,\n",
    "                  timestamp_end_is_photo_analysis):\n",
    "    time_analysis_dict[analysis_name] = {}\n",
    "    time_analysis_dict[analysis_name]['duration_str'] = f\"Analysis took: {duration}\"\n",
    "    time_analysis_dict[analysis_name]['duration_seconds'] = total_seconds\n",
    "    time_analysis_dict[analysis_name]['duration_seconds_str'] = f\"Analysis took: {total_seconds:.2f} seconds\"\n",
    "    time_analysis_dict[analysis_name]['duration_minutes'] = total_seconds/60\n",
    "    time_analysis_dict[analysis_name]['duration_minutes_str'] = f\"Analysis took: {total_seconds/60:.2f} minutes\"\n",
    "    time_analysis_dict[analysis_name]['time_stamp_start'] = timestamp_start_is_photo_analysis\n",
    "    time_analysis_dict[analysis_name]['time_stamp_end'] = timestamp_end_is_photo_analysis\n",
    "\n",
    "    time_analysis_for_df_dict['analysis_name'].append(analysis_name)\n",
    "    time_analysis_for_df_dict['time_stamp_start'].append(timestamp_start_is_photo_analysis)\n",
    "    time_analysis_for_df_dict['duration_str'].append(f\"Analysis took: {duration}\")\n",
    "    time_analysis_for_df_dict['duration_seconds'].append(total_seconds)\n",
    "    time_analysis_for_df_dict['duration_seconds_str'].append(f\"Analysis took: {total_seconds:.2f} seconds\")\n",
    "    time_analysis_for_df_dict['duration_minutes'].append(total_seconds/60)\n",
    "    time_analysis_for_df_dict['duration_minutes_str'].append(f\"Analysis took: {total_seconds/60:.2f} minutes\")\n",
    "\n",
    "    return time_analysis_dict, time_analysis_for_df_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbdf7cd-470d-4209-ae65-fa0e9a94b2be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "847d0fb9-1f20-481c-aa15-a1c7880085c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_giub_img_dir_llm(jpg_data_path, create_analysis_prompt, model_function):\n",
    "    # Get time stamp:\n",
    "    timestamp_start_is_photo_analysis = pd.Timestamp.now()\n",
    "    \n",
    "    # Get list of image files to analyse: \n",
    "    image_files = os.listdir(jpg_data_path)\n",
    "    img_ids = [image_file.split('Oberland')[1].split('.')[0] for image_file in image_files]\n",
    "    \n",
    "    # Make empty dictionary to store results:\n",
    "    image_descr = {}\n",
    "    \n",
    "    # Loop through images to get answers: \n",
    "    for image_file in image_files:\n",
    "        image_path = jpg_data_path / image_file\n",
    "        path_str = str(image_path)\n",
    "        #print('\\n')\n",
    "        #print(path_str)\n",
    "        parts = path_str.split('.jpg')\n",
    "        img_id = parts[-2][-3:]\n",
    "    \n",
    "        # Analyse image, get values for each of the categorical variables specified above:\n",
    "        #image_description = analyze_image_structured(image_path)\n",
    "        #image_description = llm_o.analyze_image_structured(image_path, create_analysis_prompt)\n",
    "        image_description = llm_o.analyze_image_structured(image_path, create_analysis_prompt, model_function)\n",
    "        \n",
    "        dict_type_bool = type(image_description) == dict\n",
    "            \n",
    "        #print(image_description)\n",
    "        image_descr[img_id] = image_description\n",
    "    \n",
    "    timestamp_end_is_photo_analysis = pd.Timestamp.now()\n",
    "\n",
    "    return timestamp_start_is_photo_analysis, timestamp_end_is_photo_analysis, image_descr\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f143f-1580-43b2-ba12-475406077535",
   "metadata": {},
   "source": [
    "### Choose LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0ef353-6f6e-4c70-94e0-11a7dcb80fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_function = llm_i.call_minicpm_model\n",
    "#model_function = llm_i.call_ollama_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baecdaa-3eb8-4b84-a687-6d79a2b8dc6b",
   "metadata": {},
   "source": [
    "### Prepare empty dictionary for time analyses and get time stamp for overall workflow duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f45fab53-b91f-4fc4-8034-d348a95a109b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-11-02 14:31:34.798939')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_analyses = {}\n",
    "time_analyses_for_df = {}\n",
    "time_analyses_for_df['analysis_name'] = []\n",
    "time_analyses_for_df['time_stamp_start'] = []\n",
    "time_analyses_for_df['duration_str'] = []\n",
    "time_analyses_for_df['duration_seconds'] = []\n",
    "time_analyses_for_df['duration_seconds_str'] = []\n",
    "time_analyses_for_df['duration_minutes'] = []\n",
    "time_analyses_for_df['duration_minutes_str'] = []\n",
    "\n",
    "timestamp_start_workflow = pd.Timestamp.now()\n",
    "timestamp_start_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1561ab0-c707-484c-95aa-07167bbc7071",
   "metadata": {},
   "source": [
    "### Prepare empty dictionary to store the different response dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c887dc3-3539-401d-97dc-cac715393e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dictionaries = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944fe6c-cbf4-44a5-9a60-2161ea174666",
   "metadata": {},
   "source": [
    "### Prepare empty dictionary for cases with unstructured answers for visual inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c71661-cc6a-4a0f-aec2-372111668756",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_closer_inspection = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376dce6f-6efc-4190-8302-212c4840a5bf",
   "metadata": {},
   "source": [
    "### Prepare empty dictionary for result dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0502d312-4363-420e-8ba2-4e897d9bd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tabular = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03821616-748e-4682-ab50-be308265de7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04ae6972-e011-4dc6-beb7-136a3a7511af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_metrics = pd.DataFrame({'positives': [],\n",
    "              'negatives': [], \n",
    "              'true_positives': [], \n",
    "              'true_negatives': [],\n",
    "              'false_negatives': [], \n",
    "              'false_positives': [], \n",
    "              'sensitivity': [], \n",
    "              'specificity': []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585626b0-743e-49a1-b0a5-ce2dc292831c",
   "metadata": {},
   "source": [
    "### Set paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d135728-ea08-4ebb-87e7-582b5384a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_path = Path('/Users/stephanehess/Documents/CAS_AML/dias_digit_project')\n",
    "#root_path = Path('/Users/stephanehess/Documents/CAS_AML/dias_digit_project/test_yolo_object_train')\n",
    "\n",
    "project_path = Path.cwd()\n",
    "root_path = (project_path / 'test_LLM_prompt_experiments').resolve()\n",
    "#root_path = project_path\n",
    "data_path = root_path / 'data'\n",
    "tif_data_path = root_path / 'data_1'\n",
    "#data_path = root_path / 'visual_genome_data_all'\n",
    "jpg_data_path = root_path / 'data_jpg'\n",
    "#yolo_path = root_path / 'visual_genome_yolo_all'\n",
    "output_dir_not_photo = root_path / 'not_photo'\n",
    "output_dir_with_person = root_path / 'with_person'\n",
    "output_dir_without_person = root_path / 'without_person'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570215b-ddd7-45bb-99f5-adbbd6e80afe",
   "metadata": {},
   "source": [
    "### Create directories for sorting the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3cdd789-34e4-46f1-ac88-427c49866d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(tif_data_path, exist_ok=True)\n",
    "os.makedirs(jpg_data_path, exist_ok=True)\n",
    "os.makedirs(output_dir_not_photo, exist_ok=True)\n",
    "os.makedirs(output_dir_with_person, exist_ok=True)\n",
    "os.makedirs(output_dir_without_person, exist_ok=True)\n",
    "#os.chdir('root_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5f056-ba70-42e2-b65b-8a4f03de7e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c2fe3ae-d0e8-4101-b740-fde42409b7aa",
   "metadata": {},
   "source": [
    "### Copy and convert image files from tif_data_path to jpg_data_path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc32204a-0c76-46d2-ae33-d1b517769bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting BernerOberland002.tif: cannot identify image file '/storage/homefs/sh98e089/test_LLM_prompt_experiments/data_1/BernerOberland002.tif'\n",
      "Error converting BernerOberland023.tif: cannot identify image file '/storage/homefs/sh98e089/test_LLM_prompt_experiments/data_1/BernerOberland023.tif'\n",
      "Converted: BernerOberland081.tif -> BernerOberland081.jpg\n",
      "Converted: BernerOberland033.tif -> BernerOberland033.jpg\n",
      "Converted: BernerOberland043.tif -> BernerOberland043.jpg\n",
      "Error converting BernerOberland008.tif: cannot identify image file '/storage/homefs/sh98e089/test_LLM_prompt_experiments/data_1/BernerOberland008.tif'\n",
      "Error converting BernerOberland107.tif: cannot identify image file '/storage/homefs/sh98e089/test_LLM_prompt_experiments/data_1/BernerOberland107.tif'\n",
      "Converted: BernerOberland082.tif -> BernerOberland082.jpg\n",
      "Error converting BernerOberland035.tif: cannot identify image file '/storage/homefs/sh98e089/test_LLM_prompt_experiments/data_1/BernerOberland035.tif'\n",
      "Error converting BernerOberland003.tif: cannot identify image file '/storage/homefs/sh98e089/test_LLM_prompt_experiments/data_1/BernerOberland003.tif'\n",
      "Converted: BernerOberland015.tif -> BernerOberland015.jpg\n",
      "Converted: BernerOberland022.tif -> BernerOberland022.jpg\n",
      "Successfully converted 6 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['BernerOberland081.jpg',\n",
       " 'BernerOberland033.jpg',\n",
       " 'BernerOberland043.jpg',\n",
       " 'BernerOberland082.jpg',\n",
       " 'BernerOberland015.jpg',\n",
       " 'BernerOberland022.jpg']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "source_folder = tif_data_path\n",
    "destination_folder = jpg_data_path\n",
    "\n",
    "llm_i.convert_tif_to_jpg(source_folder, destination_folder, quality=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bd1c1-a1d1-4be9-9cd9-052b058d3576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f0fea0f-6a59-46eb-b057-12cb5672d4b2",
   "metadata": {},
   "source": [
    "### Load person label data (ground truth) to compare to LLM responses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c7411-e309-4f9f-8051-63c7f4867514",
   "metadata": {},
   "source": [
    "The file with_without_person.csv contains labels added by (human) visual inspection that represent the ground truth. \n",
    " * Column with_person: whether or not any person is in the image.\n",
    " * Column recognisable: whether any person that would be recognisable to a human familiar with said person is in the image.\n",
    " * Column church: whether the image contains a church.\n",
    " * Column is_photo: whether the image is a photography or something else. (this formulation is, I guess, unprecise, as most dias can probably be called a photography of sorts (if a dia shows a painting, I assume a photograph of the painting has been taken), so, to be precise: whether or not the image is showing anything that exists in the real world or is showing a representation of anything that exists in the real world or aspects thereof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e15bba5d-201d-470c-95c1-578906246062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>with_person</th>\n",
       "      <th>person_recognisable</th>\n",
       "      <th>is_photo</th>\n",
       "      <th>church</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  with_person  person_recognisable  is_photo  church\n",
       "0    id001            1                    1         1       0\n",
       "1    id002            0                    0         1       1\n",
       "2    id003            0                    0         1       0\n",
       "3    id004            0                    0         1       0\n",
       "4    id005            0                    0         1       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data = pd.read_csv(data_path/'labels_mod.csv')\n",
    "label_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37a6aa73-8348-4a71-9f83-b606e5107f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>with_person</th>\n",
       "      <th>person_recognisable</th>\n",
       "      <th>is_photo</th>\n",
       "      <th>church</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  with_person  person_recognisable  is_photo  church\n",
       "0      001            1                    1         1       0\n",
       "1      002            0                    0         1       1\n",
       "2      003            0                    0         1       0\n",
       "3      004            0                    0         1       0\n",
       "4      005            0                    0         1       0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconvert image ids to integers (e.g. '234') as strings from the form they were saved in (e.g. 'id234' \n",
    "# to ensure string data type to deal with duck typing): \n",
    "img_ids = list(label_data.image_id)\n",
    "label_data['image_id'] = img_idc.reconvert_image_ids(img_ids)\n",
    "label_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc5225-b73b-4cc3-a1d7-c9ab2cbea829",
   "metadata": {},
   "source": [
    "### The following cell is only required for the test run on the test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e760fae-8f9c-45b6-8eeb-9b50fd715313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store not found\n",
      "['002', '003', '008', '015', '022', '023', '033', '035', '043', '081', '082', '107']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>with_person</th>\n",
       "      <th>person_recognisable</th>\n",
       "      <th>is_photo</th>\n",
       "      <th>church</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>033</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>035</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>043</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>081</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>082</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  with_person  person_recognisable  is_photo  church\n",
       "1        002            0                    0         1       1\n",
       "2        003            0                    0         1       0\n",
       "7        008            0                    0         1       1\n",
       "14       015            1                    1         1       0\n",
       "21       022            0                    0         0       0\n",
       "22       023            1                    1         1       0\n",
       "32       033            0                    0         0       0\n",
       "34       035            0                    0         1       1\n",
       "42       043            0                    0         1       1\n",
       "80       081            0                    0         0       0\n",
       "81       082            0                    0         0       0\n",
       "106      107            0                    0         1       0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only rows referring to images in the smaller data set (test run):\n",
    "\n",
    "# Make sure no .DS_Store file is included in jpg_data_path: \n",
    "import os\n",
    "ds_file_path = jpg_data_path / '.DS_Store'\n",
    "\n",
    "# Remove a specific .DS_Store file\n",
    "if os.path.exists(ds_file_path):\n",
    "    os.remove(ds_file_path)\n",
    "    print(\"Removed .DS_Store\")\n",
    "else:\n",
    "    print(\".DS_Store not found\")\n",
    "\n",
    "# Find all .ipynb_checkpoints directories\n",
    "for checkpoint_dir in jpg_data_path.rglob('.ipynb_checkpoints'):\n",
    "    if checkpoint_dir.is_dir():\n",
    "        print(f\"Removing: {checkpoint_dir}\")\n",
    "        shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Get list of image files present:\n",
    "image_files = os.listdir(jpg_data_path)\n",
    "\n",
    "#image_files.remove(\".ipynb_checkpoints\")\n",
    "\n",
    "\n",
    "\n",
    "# Extract image ids from image file names:\n",
    "img_ids = [image_file.split('Oberland')[1].split('.')[0] for image_file in image_files]\n",
    "img_ids.sort()\n",
    "print(img_ids)\n",
    "\n",
    "# Select relevant rows from label_data data frame by id list: \n",
    "select_bools = [img_id in img_ids for img_id in label_data.image_id]\n",
    "\n",
    "label_data = label_data[select_bools].copy()\n",
    "label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81653ad4-a42c-4677-95bb-f7e15325fa11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "747bfce4-a523-4307-acc9-59960f2bc00e",
   "metadata": {},
   "source": [
    "### Prepare variations of LLM prompt functions to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "847e95f7-c9e7-43d2-8a3a-49ec9c36422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_prompt_basic():\n",
    "    \"\"\"Create the structured prompt for image analysis.\"\"\"\n",
    "    return \"\"\"\n",
    "    Analyze this image and return ONLY a Python dictionary in exactly this format:\n",
    "    \n",
    "    {\n",
    "        'image_is_photography': X,  # True if the image is a photography, False otherwise\n",
    "        'additional_comments': '' # Any additional observations or empty string if none\n",
    "    }\n",
    "    \n",
    "    Replace X with True (image is a photography) or False (image is not a photography).\n",
    "    Return ONLY the dictionary, no other text.\n",
    "    Your answer MUST have the exact structue of the dictionary described above (all keys MUST be present). \n",
    "    If you cannot answer the question in the way implied by this structure, enter 'None' as value and offer \n",
    "    your answer and explanations under 'additional_comments'.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d062470-b460-442a-a400-136a5f86ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_prompt_precise():\n",
    "    \"\"\"Create the structured prompt for image analysis.\"\"\"\n",
    "    return \"\"\"\n",
    "    Analyze this image and return ONLY a Python dictionary in exactly this format:\n",
    "    \n",
    "    {\n",
    "        'image_of_image': X,  # True if the image is a direct representation of something, False if it is an indirect representation (i.e. a representation of a representation of something).\n",
    "        'additional_comments': '' # Any additional observations or empty string if none\n",
    "    }\n",
    "    \n",
    "    Replace X with True (direct representation) or False (indirect representation).\n",
    "    A representation can represent anything that exists or aspects thereof; the represantation can be concrete or abstract. \n",
    "    The image at hand can either be such a direct representation \n",
    "    or else a representation of a representation of anything that exists or of aspects thereof (concrete or abstract). \n",
    "    In other words, you have to determine if the image is a direct (Replace X with True) or an indirect representation (Replace X with False).\n",
    "    Return ONLY the dictionary, no other text.\n",
    "    Your answer MUST have the exact structue of the dictionary described above (all keys MUST be present). \n",
    "    If you cannot answer the question in the way implied by this structure, enter 'None' as value and offer \n",
    "    your answer and explanations under 'additional_comments'.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cecbb64-5046-4049-a284-555549e0fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_prompt_intuitive():\n",
    "    \"\"\"Create the structured prompt for image analysis.\"\"\"\n",
    "    return \"\"\"\n",
    "    Analyze this image and return ONLY a Python dictionary in exactly this format:\n",
    "    \n",
    "    {\n",
    "        'image_is_photography': X,  # True if the image is photography, False otherwise (e.g. if the image is a map or a painting)\n",
    "        'additional_comments': '' # Any additional observations or empty string if none\n",
    "    }\n",
    "    \n",
    "    Replace X with True (image is a photography) or False (otherwise).\n",
    "    Return ONLY the dictionary, no other text.\n",
    "    Your answer MUST have the exact structue of the dictionary described above (all keys MUST be present). \n",
    "    If you cannot answer the question in the way implied by this structure, enter 'None' as value and offer \n",
    "    your answer and explanations under 'additional_comments'.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aad15425-0a43-4c94-9714-6f684288e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_prompt_alternatives():\n",
    "    \"\"\"Create the structured prompt for image analysis.\"\"\"\n",
    "    return \"\"\"\n",
    "    Analyze this image and return ONLY a Python dictionary in exactly this format:\n",
    "    \n",
    "    {\n",
    "        'image_is_photography': X,  # True if the image is photography, False if image is a map, a painting, a drawing, a scheme, a statistics figure, or other.\n",
    "        'additional_comments': '' # Any additional observations or empty string if none.\n",
    "    }\n",
    "    \n",
    "    Replace X with True (image is a photography) or False (image is a map, a painting, a drawing, a scheme, a statistics figure, or other).\n",
    "    Return ONLY the dictionary, no other text.\n",
    "    Your answer MUST have the exact structue of the dictionary described above (all keys MUST be present). \n",
    "    If you cannot answer the question in the way implied by this structure, enter 'None' as value and offer \n",
    "    your answer and explanations under 'additional_comments'.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e11d20-e37f-4888-9903-04025bd0f384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93dad657-2cbe-4da7-a384-3c6716d8a2aa",
   "metadata": {},
   "source": [
    "## Identify non-photo images with basic prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d55c60e-57f4-434f-b46c-a006251e36f5",
   "metadata": {},
   "source": [
    "### Set parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6489666-7024-477e-97b6-f54cc5c2b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters: \n",
    "analysis_name = 'is_photo_basic_struct_minicpm'\n",
    "create_analysis_prompt = create_analysis_prompt_basic\n",
    "keys_list_expected = ['image_is_photography', 'additional_comments']\n",
    "response_variable = 'image_is_photography'\n",
    "\n",
    "label_name = 'is_photo'\n",
    "prediction_name = 'is_photo_pred'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dcfc45-b347-4bd4-8eb4-139fd18da430",
   "metadata": {},
   "source": [
    "### Prepare data objects: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7d2715b-7d27-4215-9d3b-c237eb9a15b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data objects: \n",
    "labels_results_repetitions = []\n",
    "response_dictionaries[analysis_name] = {}\n",
    "\n",
    "ml_metrics_analysis_name = []\n",
    "ml_metrics_time_stamp = []\n",
    "ml_metrics_positives = []\n",
    "ml_metrics_negatives = []\n",
    "ml_metrics_true_positives = []\n",
    "ml_metrics_false_positives = []\n",
    "ml_metrics_true_negatives = []\n",
    "ml_metrics_false_negatives = []\n",
    "ml_metrics_sensitivity = []\n",
    "ml_metrics_specificity = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2212d2-febd-45ec-b3ce-7d82020e19de",
   "metadata": {},
   "source": [
    "### Extract and store results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3e6ef7f-bf24-4b03-86de-2182a4c26c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c15f40ca6b4059abec264ff4506a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "/storage/homefs/sh98e089/venvs/test-qwen-env/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:648: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861fb65cddec4a959283e74b32bc8c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108e0b2d17ea4b1a8bae3a3e341c3aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7ef5b4fefb4cd9bf35fd144439028b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7926cfa3dbf04ecdaf5e28528a292998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2cef853fe7426cad4de43a631a82b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c15ad266b044e2b26e0ef88549020e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79c9dfe546a43e7a82a4e4e9d1291a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c836746a00c4cdd913b6a7648e1b100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eaebc9d24a84e2fb5f3c5403f7877b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d917aa6a447b4e1d8374154b4eae02b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b22998d400b4bfea4c8b3b11dd3543e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213.739372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 7)\n",
      "True Positives: 8\n",
      "False Positives: 0\n",
      "True Negatives: 4\n",
      "False Negatives: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae14bf94a064071b55c15abd839163f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fd4a3be5184c0ba8a6ffb15782bcc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd99422aea3f425db512117e90feb62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bf200cc56747fa81ecb121da7db6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0be1f51eb148fab10dadc8b528fd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b447e493e424b89b1b5663baa62a793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b780eb9ee64b43c8abe6dd720a40451e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa031ab254d40748ee9fd971895345e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7cf46942d14ce28f3ea58d1693ea36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d839bea548e7455fbe4c67551841b12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05632e4feca44814bc493d37aaa031bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a018aa2983264f759df49f4fb2263653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159.241543\n",
      "(12, 7)\n",
      "True Positives: 8\n",
      "False Positives: 0\n",
      "True Negatives: 4\n",
      "False Negatives: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660ea040cf9e40d2995275b6578f6566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afeaa758f624e798b21edd2586133a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21f9fe574f94f5c9661dd3c90bbc89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972f04b94b0e4b00addb0b9fc4da23f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e8119c7c044eb38ecae8c924e80876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0aa039e53e45fb8e45a0516c103943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b038bcedebea4a3fb2810183155efc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d482dcbcbd743558f78c8fa5b673837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e17f1a725754c5495eca118f8800187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3eff9249f7b4deeb4e08d271cb36900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd76a8c04c3743129c071c2fa4d07447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n",
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8ac852256c46cbac9b9bbfc85ab80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name openbmb/MiniCPM_hyphen_V_hyphen_4_5 (originally openbmb/MiniCPM-V-4_5) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "('The read operation timed out', '(Request ID: 5ff5717f-9c55-44b0-93c9-f6edcde3f5e2)')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    125\u001b[39m exc_map: ExceptionMapping = {socket.timeout: ReadTimeout, \u001b[38;5;167;01mOSError\u001b[39;00m: ReadError}\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43msettimeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/software.9/software/Anaconda3/2024.02-1/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m itercount = \u001b[32m0\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m itercount < \u001b[32m5\u001b[39m:\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Analysis with LLM: \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     timestamp_start_is_photo_analysis, timestamp_end_is_photo_analysis, image_descr = \u001b[43manalyse_giub_img_dir_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjpg_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_analysis_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Calculate duration of analysis: \u001b[39;00m\n\u001b[32m      9\u001b[39m     duration = timestamp_end_is_photo_analysis - timestamp_start_is_photo_analysis\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36manalyse_giub_img_dir_llm\u001b[39m\u001b[34m(jpg_data_path, create_analysis_prompt, model_function)\u001b[39m\n\u001b[32m     19\u001b[39m img_id = parts[-\u001b[32m2\u001b[39m][-\u001b[32m3\u001b[39m:]\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Analyse image, get values for each of the categorical variables specified above:\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#image_description = analyze_image_structured(image_path)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#image_description = llm_o.analyze_image_structured(image_path, create_analysis_prompt)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m image_description = \u001b[43mllm_o\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_image_structured\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_analysis_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m dict_type_bool = \u001b[38;5;28mtype\u001b[39m(image_description) == \u001b[38;5;28mdict\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#print(image_description)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/llm_output.py:51\u001b[39m, in \u001b[36manalyze_image_structured\u001b[39m\u001b[34m(image_path, create_analysis_prompt, model_function)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03mMain function that orchestrates the image analysis using specified model.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m \u001b[33;03m    dict: Parsed result dictionary or raw response if parsing fails\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Define prompt for LLM model\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# prompt = create_analysis_prompt()\u001b[39;00m\n\u001b[32m     49\u001b[39m \n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Ask LLM to analyze image using the specified model function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m response_text = \u001b[43mmodel_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_analysis_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Parse response text to find dictionary of expected structure\u001b[39;00m\n\u001b[32m     54\u001b[39m success, result_dict = parse_response_to_dict(response_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/llm_input.py:107\u001b[39m, in \u001b[36mcall_minicpm_model\u001b[39m\u001b[34m(image_path, prompt_function)\u001b[39m\n\u001b[32m     99\u001b[39m model = AutoModel.from_pretrained(\n\u001b[32m    100\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mopenbmb/MiniCPM-V-4_5\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    101\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    102\u001b[39m     attn_implementation=\u001b[33m'\u001b[39m\u001b[33msdpa\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    103\u001b[39m     torch_dtype=torch.bfloat16\n\u001b[32m    104\u001b[39m )\n\u001b[32m    105\u001b[39m model = model.eval().cuda()\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mopenbmb/MiniCPM-V-4_5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    110\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Load and process image\u001b[39;00m\n\u001b[32m    113\u001b[39m image = Image.open(image_path).convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1125\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1123\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1124\u001b[39m     tokenizer_class.register_for_auto_class()\n\u001b[32m-> \u001b[39m\u001b[32m1125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1129\u001b[39m     tokenizer_class = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2000\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   1996\u001b[39m             vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = (\n\u001b[32m   1997\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1998\u001b[39m             )\n\u001b[32m   1999\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2000\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlist_repo_templates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2006\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2007\u001b[39m         template = template.removesuffix(\u001b[33m\"\u001b[39m\u001b[33m.jinja\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2008\u001b[39m         vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jinja\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/transformers/utils/hub.py:164\u001b[39m, in \u001b[36mlist_repo_templates\u001b[39m\u001b[34m(repo_id, local_files_only, revision, cache_dir, token)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremoveprefix\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlist_repo_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.jinja\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/transformers/utils/hub.py:164\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremoveprefix\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlist_repo_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.jinja\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/huggingface_hub/hf_api.py:3083\u001b[39m, in \u001b[36mHfApi.list_repo_tree\u001b[39m\u001b[34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[39m\n\u001b[32m   3081\u001b[39m encoded_path_in_repo = \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + quote(path_in_repo, safe=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m path_in_repo \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3082\u001b[39m tree_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.endpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mencoded_path_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3083\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaginate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtree_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecursive\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexpand\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mRepoFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mRepoFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/huggingface_hub/utils/_pagination.py:36\u001b[39m, in \u001b[36mpaginate\u001b[39m\u001b[34m(path, params, headers)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fetch a list of models/datasets/spaces and paginate through results.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03mThis is using the same \"Link\" header format as GitHub.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m \u001b[33;03m- https://docs.github.com/en/rest/guides/traversing-with-pagination#link-header\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m session = get_session()\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m r = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m hf_raise_for_status(r)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m r.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpx/_client.py:1053\u001b[39m, in \u001b[36mClient.get\u001b[39m\u001b[34m(self, url, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1038\u001b[39m     url: URL | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1046\u001b[39m     extensions: RequestExtensions | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1047\u001b[39m ) -> Response:\n\u001b[32m   1048\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33;03m    Send a `GET` request.\u001b[39;00m\n\u001b[32m   1050\u001b[39m \n\u001b[32m   1051\u001b[39m \u001b[33;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[32m   1052\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:89\u001b[39m, in \u001b[36mHfHubTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     87\u001b[39m request_id = _add_request_id(request)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.RequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m request_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     92\u001b[39m         \u001b[38;5;66;03m# Taken from https://stackoverflow.com/a/58270258\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/software.9/software/Anaconda3/2024.02-1/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    156\u001b[39m     value = typ()\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/test-qwen-env/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: ('The read operation timed out', '(Request ID: 5ff5717f-9c55-44b0-93c9-f6edcde3f5e2)')"
     ]
    }
   ],
   "source": [
    "itercount = 0\n",
    "\n",
    "while itercount < 5:\n",
    "\n",
    "    # Analysis with LLM: \n",
    "    timestamp_start_is_photo_analysis, timestamp_end_is_photo_analysis, image_descr = analyse_giub_img_dir_llm(jpg_data_path, create_analysis_prompt, model_function)\n",
    "\n",
    "    # Calculate duration of analysis: \n",
    "    duration = timestamp_end_is_photo_analysis - timestamp_start_is_photo_analysis\n",
    "    total_seconds = duration.total_seconds()\n",
    "    print(total_seconds)\n",
    "\n",
    "    # Store information about duration: \n",
    "    time_analyses, time_analyses_for_df = store_duration(time_analyses, time_analyses_for_df, analysis_name, \n",
    "                   duration,timestamp_start_is_photo_analysis,\n",
    "                  timestamp_end_is_photo_analysis)\n",
    "    \n",
    "    # Store dictionary with LLM responses:\n",
    "    response_dictionaries[analysis_name][timestamp_start_is_photo_analysis] = image_descr\n",
    "    \n",
    "    # Extract LLM responses from dictionary:\n",
    "    img_ids, is_photo, img_ids_closer_inspection = \\\n",
    "    llm_o.extract_vals_from_response_dict(img_ids, image_descr, keys_list_expected, response_variable)\n",
    "    \n",
    "    # Check if the response variable lists has the same length as id list:\n",
    "    # print('Length of img_ids:')\n",
    "    # print(len(img_ids))\n",
    "    # print('Length of is_photo:')\n",
    "    # print(len(is_photo))\n",
    "    \n",
    "    # Put response variables into data frame: \n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids)\n",
    "    predictions = pd.DataFrame({'image_id': img_ids, \n",
    "                               prediction_name: is_photo,\n",
    "                               'time_stamp': timestamp_ids})\n",
    "    \n",
    "    # Check for missing values:\n",
    "    # print(predictions.isnull().any().any())\n",
    "    # print(predictions.isna().any().any())\n",
    "    # print(has_missing_comprehensive(predictions))\n",
    "    \n",
    "    # Merge label data with the predictions:\n",
    "    label_data_c = label_data.copy()\n",
    "    labels_results = label_data_c.merge(predictions, how='inner', on='image_id')\n",
    "    print(labels_results.shape)\n",
    "    labels_results_repetitions.append(labels_results)\n",
    "    \n",
    "    # Save labels and predictions in dictionary: \n",
    "    #results_tabular[analysis_name] = {}\n",
    "    #results_tabular[analysis_name][timestamp_start_is_photo_analysis] = labels_results\n",
    "    \n",
    "    # Save image list for closer inspection:\n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids_closer_inspection)\n",
    "    imgs_closer_inspection = pd.DataFrame({'image_id': img_ids_closer_inspection,\n",
    "    'time_stamp': timestamp_ids})\n",
    "    images_closer_inspection[analysis_name] = imgs_closer_inspection\n",
    "    \n",
    "    \n",
    "    # Calculate sensitivity and specificity for photography predictions and get lists images with positive photography predictions:\n",
    "    subsets_and_metrics = llm_o.get_classification_subsets_metrics(labels_results, label_name, prediction_name)\n",
    "    positives, negatives, true_positives, true_negatives, \\\n",
    "    false_negatives, false_positives, sensitivity, specificity = subsets_and_metrics\n",
    "    # ml_metrics['analysis_name'] = analysis_name\n",
    "    # ml_metrics['time_stamp'] = timestamp_start_is_photo_analysis\n",
    "    # ml_metrics['positives'] = positives.shape[0]\n",
    "    # ml_metrics['negatives'] = negatives.shape[0]\n",
    "    # ml_metrics['true_positives'] = true_positives.shape[0]\n",
    "    # ml_metrics['false_positives'] = false_positives.shape[0]\n",
    "    # ml_metrics['true_negatives'] = true_negatives.shape[0]\n",
    "    # ml_metrics['false_negatives'] = false_negatives.shape[0]\n",
    "    # ml_metrics['sensitivity'] = sensitivity\n",
    "    # ml_metrics['specificity'] = specificity\n",
    "\n",
    "\n",
    "    ml_metrics_analysis_name.append(analysis_name)\n",
    "    ml_metrics_time_stamp.append(timestamp_start_is_photo_analysis)\n",
    "    ml_metrics_positives.append(positives.shape[0])\n",
    "    ml_metrics_negatives.append(negatives.shape[0])\n",
    "    ml_metrics_true_positives.append(true_positives.shape[0])\n",
    "    ml_metrics_false_positives.append(false_positives.shape[0])\n",
    "    ml_metrics_true_negatives.append(true_negatives.shape[0])\n",
    "    ml_metrics_false_negatives.append(false_negatives.shape[0])\n",
    "    ml_metrics_sensitivity.append(sensitivity)\n",
    "    ml_metrics_specificity.append(specificity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'True Positives: {true_positives.shape[0]}')\n",
    "    print(f'False Positives: {false_positives.shape[0]}')\n",
    "    print(f'True Negatives: {true_negatives.shape[0]}')\n",
    "    print(f'False Negatives: {false_negatives.shape[0]}')\n",
    "    \n",
    "    itercount += 1\n",
    "\n",
    "results_tabular[analysis_name] = pd.concat(labels_results_repetitions, ignore_index=True)\n",
    "\n",
    "\n",
    "ml_metrics_one_analysis = pd.DataFrame({})\n",
    "\n",
    "ml_metrics_one_analysis['analysis_name'] = ml_metrics_analysis_name\n",
    "ml_metrics_one_analysis['time_stamp'] = ml_metrics_time_stamp\n",
    "ml_metrics_one_analysis['positives'] = ml_metrics_positives\n",
    "ml_metrics_one_analysis['negatives'] = ml_metrics_negatives\n",
    "ml_metrics_one_analysis['true_positives'] = ml_metrics_true_positives\n",
    "ml_metrics_one_analysis['false_positives'] = ml_metrics_false_positives\n",
    "ml_metrics_one_analysis['true_negatives'] = ml_metrics_true_negatives\n",
    "ml_metrics_one_analysis['false_negatives'] = ml_metrics_false_negatives\n",
    "ml_metrics_one_analysis['sensitivity'] = ml_metrics_sensitivity\n",
    "ml_metrics_one_analysis['specificity'] = ml_metrics_specificity\n",
    "\n",
    "ml_metrics = pd.concat([ml_metrics, ml_metrics_one_analysis], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbbacdf-dbf2-4d27-b2cf-88cf40d1e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files[0].split('Oberland')[1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b3967-a7f8-4fd8-a175-79a667f2e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7bc871-7057-41a3-8593-c6d77565c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196e331-0c52-4eee-954e-b15890f4f9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a122e454-ac7d-4c22-806b-cb69a9481f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da5064-4ff9-466c-858e-c64f7cde11f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84575590-bcdb-4356-a5b4-bcc497f4135c",
   "metadata": {},
   "source": [
    "## Identify non-photo images with intuitive prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf890d-0b48-4d7d-8f09-dc9b504a0716",
   "metadata": {},
   "source": [
    "### Set parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c63d8-1965-4493-9af9-d3fc519ba778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters: \n",
    "analysis_name = 'is_photo_intuitive_struct_minicpm'\n",
    "create_analysis_prompt = create_analysis_prompt_intuitive\n",
    "keys_list_expected = ['image_is_photography', 'additional_comments']\n",
    "response_variable = 'image_is_photography'\n",
    "\n",
    "label_name = 'is_photo'\n",
    "prediction_name = 'is_photo_pred'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd511d-7d60-4c66-ae26-c8afddd96df9",
   "metadata": {},
   "source": [
    "### Prepare data objects: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72ca94-cc5b-46c3-8ed8-02f19d8a8a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data objects: \n",
    "labels_results_repetitions = []\n",
    "response_dictionaries[analysis_name] = {}\n",
    "\n",
    "ml_metrics_analysis_name = []\n",
    "ml_metrics_time_stamp = []\n",
    "ml_metrics_positives = []\n",
    "ml_metrics_negatives = []\n",
    "ml_metrics_true_positives = []\n",
    "ml_metrics_false_positives = []\n",
    "ml_metrics_true_negatives = []\n",
    "ml_metrics_false_negatives = []\n",
    "ml_metrics_sensitivity = []\n",
    "ml_metrics_specificity = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a83fddc-d6ab-4ab5-ae3f-c57b4324eebc",
   "metadata": {},
   "source": [
    "### Extract and store results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539dd53-cbe6-424c-8264-c52cadb99b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "itercount = 0\n",
    "\n",
    "while itercount < 5:\n",
    "\n",
    "    # Analysis with LLM: \n",
    "    timestamp_start_is_photo_analysis, timestamp_end_is_photo_analysis, image_descr = analyse_giub_img_dir_llm(jpg_data_path, create_analysis_prompt, model_function)\n",
    "\n",
    "    # Calculate duration of analysis: \n",
    "    duration = timestamp_end_is_photo_analysis - timestamp_start_is_photo_analysis\n",
    "    total_seconds = duration.total_seconds()\n",
    "    print(total_seconds)\n",
    "\n",
    "    # Store information about duration: \n",
    "    time_analyses, time_analyses_for_df = store_duration(time_analyses, time_analyses_for_df, analysis_name, \n",
    "                   duration,timestamp_start_is_photo_analysis,\n",
    "                  timestamp_end_is_photo_analysis)\n",
    "    \n",
    "    # Store dictionary with LLM responses:\n",
    "    response_dictionaries[analysis_name][timestamp_start_is_photo_analysis] = image_descr\n",
    "    \n",
    "    # Extract LLM responses from dictionary:\n",
    "    img_ids, is_photo, img_ids_closer_inspection = \\\n",
    "    llm_o.extract_vals_from_response_dict(img_ids, image_descr, keys_list_expected, response_variable)\n",
    "    \n",
    "    # Check if the response variable lists has the same length as id list:\n",
    "    # print('Length of img_ids:')\n",
    "    # print(len(img_ids))\n",
    "    # print('Length of is_photo:')\n",
    "    # print(len(is_photo))\n",
    "    \n",
    "    # Put response variables into data frame: \n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids)\n",
    "    predictions = pd.DataFrame({'image_id': img_ids, \n",
    "                               prediction_name: is_photo,\n",
    "                               'time_stamp': timestamp_ids})\n",
    "    \n",
    "    # Check for missing values:\n",
    "    # print(predictions.isnull().any().any())\n",
    "    # print(predictions.isna().any().any())\n",
    "    # print(has_missing_comprehensive(predictions))\n",
    "    \n",
    "    # Merge label data with the predictions:\n",
    "    label_data_c = label_data.copy()\n",
    "    labels_results = label_data_c.merge(predictions, how='inner', on='image_id')\n",
    "    print(labels_results.shape)\n",
    "    labels_results_repetitions.append(labels_results)\n",
    "    \n",
    "    # Save labels and predictions in dictionary: \n",
    "    #results_tabular[analysis_name] = {}\n",
    "    #results_tabular[analysis_name][timestamp_start_is_photo_analysis] = labels_results\n",
    "    \n",
    "    # Save image list for closer inspection:\n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids_closer_inspection)\n",
    "    imgs_closer_inspection = pd.DataFrame({'image_id': img_ids_closer_inspection,\n",
    "    'time_stamp': timestamp_ids})\n",
    "    images_closer_inspection[analysis_name] = imgs_closer_inspection\n",
    "    \n",
    "    \n",
    "    # Calculate sensitivity and specificity for photography predictions and get lists images with positive photography predictions:\n",
    "    subsets_and_metrics = llm_o.get_classification_subsets_metrics(labels_results, label_name, prediction_name)\n",
    "    positives, negatives, true_positives, true_negatives, \\\n",
    "    false_negatives, false_positives, sensitivity, specificity = subsets_and_metrics\n",
    "    # ml_metrics['analysis_name'] = analysis_name\n",
    "    # ml_metrics['time_stamp'] = timestamp_start_is_photo_analysis\n",
    "    # ml_metrics['positives'] = positives.shape[0]\n",
    "    # ml_metrics['negatives'] = negatives.shape[0]\n",
    "    # ml_metrics['true_positives'] = true_positives.shape[0]\n",
    "    # ml_metrics['false_positives'] = false_positives.shape[0]\n",
    "    # ml_metrics['true_negatives'] = true_negatives.shape[0]\n",
    "    # ml_metrics['false_negatives'] = false_negatives.shape[0]\n",
    "    # ml_metrics['sensitivity'] = sensitivity\n",
    "    # ml_metrics['specificity'] = specificity\n",
    "\n",
    "\n",
    "    ml_metrics_analysis_name.append(analysis_name)\n",
    "    ml_metrics_time_stamp.append(timestamp_start_is_photo_analysis)\n",
    "    ml_metrics_positives.append(positives.shape[0])\n",
    "    ml_metrics_negatives.append(negatives.shape[0])\n",
    "    ml_metrics_true_positives.append(true_positives.shape[0])\n",
    "    ml_metrics_false_positives.append(false_positives.shape[0])\n",
    "    ml_metrics_true_negatives.append(true_negatives.shape[0])\n",
    "    ml_metrics_false_negatives.append(false_negatives.shape[0])\n",
    "    ml_metrics_sensitivity.append(sensitivity)\n",
    "    ml_metrics_specificity.append(specificity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'True Positives: {true_positives.shape[0]}')\n",
    "    print(f'False Positives: {false_positives.shape[0]}')\n",
    "    print(f'True Negatives: {true_negatives.shape[0]}')\n",
    "    print(f'False Negatives: {false_negatives.shape[0]}')\n",
    "    \n",
    "    itercount += 1\n",
    "\n",
    "results_tabular[analysis_name] = pd.concat(labels_results_repetitions, ignore_index=True)\n",
    "\n",
    "\n",
    "ml_metrics_one_analysis = pd.DataFrame({'positives': [],\n",
    "              'negatives': [], \n",
    "              'true_positives': [], \n",
    "              'true_negatives': [],\n",
    "              'false_negatives': [], \n",
    "              'false_positives': [], \n",
    "              'sensitivity': [], \n",
    "              'specificity': []})\n",
    "\n",
    "ml_metrics_one_analysis['analysis_name'] = ml_metrics_analysis_name\n",
    "ml_metrics_one_analysis['time_stamp'] = ml_metrics_time_stamp\n",
    "ml_metrics_one_analysis['positives'] = ml_metrics_positives\n",
    "ml_metrics_one_analysis['negatives'] = ml_metrics_negatives\n",
    "ml_metrics_one_analysis['true_positives'] = ml_metrics_true_positives\n",
    "ml_metrics_one_analysis['false_positives'] = ml_metrics_false_positives\n",
    "ml_metrics_one_analysis['true_negatives'] = ml_metrics_true_negatives\n",
    "ml_metrics_one_analysis['false_negatives'] = ml_metrics_false_negatives\n",
    "ml_metrics_one_analysis['sensitivity'] = ml_metrics_sensitivity\n",
    "ml_metrics_one_analysis['specificity'] = ml_metrics_specificity\n",
    "\n",
    "ml_metrics = pd.concat([ml_metrics, ml_metrics_one_analysis], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93e2cb-2929-4eb3-8bd6-06a76ce90446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dfdf89-ebb2-4357-a1df-57c59d24aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tabular[analysis_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514e5ac-bfb1-4c52-87eb-01d44974d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e02512-2951-45ef-bad6-1888e41524d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62da3e62-8e1a-4e1f-8893-13c396b523a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36315001-2822-461b-ba3f-07f7e5c2d40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6118f3a-4194-472a-b0fa-e46d82313cb4",
   "metadata": {},
   "source": [
    "## Identify non-photo images with alternatives prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8245faa-d047-4141-afc2-f418fc6bff16",
   "metadata": {},
   "source": [
    "### Set parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd5852-ac15-4c8d-848d-513055779eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters: \n",
    "analysis_name = 'is_photo_alternatives_struct_minicpm'\n",
    "create_analysis_prompt = create_analysis_prompt_alternatives\n",
    "keys_list_expected = ['image_is_photography', 'additional_comments']\n",
    "response_variable = 'image_is_photography'\n",
    "\n",
    "label_name = 'is_photo'\n",
    "prediction_name = 'is_photo_pred'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d87089-29aa-4584-953f-7d91baa16aff",
   "metadata": {},
   "source": [
    "### Prepare data objects: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b05bf2-ed8c-4f4b-9fbc-b8e87ff3c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data objects: \n",
    "labels_results_repetitions = []\n",
    "response_dictionaries[analysis_name] = {}\n",
    "\n",
    "ml_metrics_analysis_name = []\n",
    "ml_metrics_time_stamp = []\n",
    "ml_metrics_positives = []\n",
    "ml_metrics_negatives = []\n",
    "ml_metrics_true_positives = []\n",
    "ml_metrics_false_positives = []\n",
    "ml_metrics_true_negatives = []\n",
    "ml_metrics_false_negatives = []\n",
    "ml_metrics_sensitivity = []\n",
    "ml_metrics_specificity = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ae3ce-cf94-419c-97ef-a24f38b65260",
   "metadata": {},
   "source": [
    "### Extract and store results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d5cd7d-0a84-4d85-b063-4c264a08e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "itercount = 0\n",
    "\n",
    "while itercount < 5:\n",
    "\n",
    "    # Analysis with LLM: \n",
    "    timestamp_start_is_photo_analysis, timestamp_end_is_photo_analysis, image_descr = analyse_giub_img_dir_llm(jpg_data_path, create_analysis_prompt, model_function)\n",
    "\n",
    "    # Calculate duration of analysis: \n",
    "    duration = timestamp_end_is_photo_analysis - timestamp_start_is_photo_analysis\n",
    "    total_seconds = duration.total_seconds()\n",
    "    print(total_seconds)\n",
    "\n",
    "    # Store information about duration: \n",
    "    time_analyses, time_analyses_for_df = store_duration(time_analyses, time_analyses_for_df, analysis_name, \n",
    "                   duration,timestamp_start_is_photo_analysis,\n",
    "                  timestamp_end_is_photo_analysis)\n",
    "    \n",
    "    # Store dictionary with LLM responses:\n",
    "    response_dictionaries[analysis_name][timestamp_start_is_photo_analysis] = image_descr\n",
    "    \n",
    "    # Extract LLM responses from dictionary:\n",
    "    img_ids, is_photo, img_ids_closer_inspection = \\\n",
    "    llm_o.extract_vals_from_response_dict(img_ids, image_descr, keys_list_expected, response_variable)\n",
    "    \n",
    "    # Check if the response variable lists has the same length as id list:\n",
    "    # print('Length of img_ids:')\n",
    "    # print(len(img_ids))\n",
    "    # print('Length of is_photo:')\n",
    "    # print(len(is_photo))\n",
    "    \n",
    "    # Put response variables into data frame: \n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids)\n",
    "    predictions = pd.DataFrame({'image_id': img_ids, \n",
    "                               prediction_name: is_photo,\n",
    "                               'time_stamp': timestamp_ids})\n",
    "    \n",
    "    # Check for missing values:\n",
    "    # print(predictions.isnull().any().any())\n",
    "    # print(predictions.isna().any().any())\n",
    "    # print(has_missing_comprehensive(predictions))\n",
    "    \n",
    "    # Merge label data with the predictions:\n",
    "    label_data_c = label_data.copy()\n",
    "    labels_results = label_data_c.merge(predictions, how='inner', on='image_id')\n",
    "    print(labels_results.shape)\n",
    "    labels_results_repetitions.append(labels_results)\n",
    "    \n",
    "    # Save labels and predictions in dictionary: \n",
    "    #results_tabular[analysis_name] = {}\n",
    "    #results_tabular[analysis_name][timestamp_start_is_photo_analysis] = labels_results\n",
    "    \n",
    "    # Save image list for closer inspection:\n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids_closer_inspection)\n",
    "    imgs_closer_inspection = pd.DataFrame({'image_id': img_ids_closer_inspection,\n",
    "    'time_stamp': timestamp_ids})\n",
    "    images_closer_inspection[analysis_name] = imgs_closer_inspection\n",
    "    \n",
    "    \n",
    "    # Calculate sensitivity and specificity for photography predictions and get lists images with positive photography predictions:\n",
    "    subsets_and_metrics = llm_o.get_classification_subsets_metrics(labels_results, label_name, prediction_name)\n",
    "    positives, negatives, true_positives, true_negatives, \\\n",
    "    false_negatives, false_positives, sensitivity, specificity = subsets_and_metrics\n",
    "    # ml_metrics['analysis_name'] = analysis_name\n",
    "    # ml_metrics['time_stamp'] = timestamp_start_is_photo_analysis\n",
    "    # ml_metrics['positives'] = positives.shape[0]\n",
    "    # ml_metrics['negatives'] = negatives.shape[0]\n",
    "    # ml_metrics['true_positives'] = true_positives.shape[0]\n",
    "    # ml_metrics['false_positives'] = false_positives.shape[0]\n",
    "    # ml_metrics['true_negatives'] = true_negatives.shape[0]\n",
    "    # ml_metrics['false_negatives'] = false_negatives.shape[0]\n",
    "    # ml_metrics['sensitivity'] = sensitivity\n",
    "    # ml_metrics['specificity'] = specificity\n",
    "\n",
    "\n",
    "    ml_metrics_analysis_name.append(analysis_name)\n",
    "    ml_metrics_time_stamp.append(timestamp_start_is_photo_analysis)\n",
    "    ml_metrics_positives.append(positives.shape[0])\n",
    "    ml_metrics_negatives.append(negatives.shape[0])\n",
    "    ml_metrics_true_positives.append(true_positives.shape[0])\n",
    "    ml_metrics_false_positives.append(false_positives.shape[0])\n",
    "    ml_metrics_true_negatives.append(true_negatives.shape[0])\n",
    "    ml_metrics_false_negatives.append(false_negatives.shape[0])\n",
    "    ml_metrics_sensitivity.append(sensitivity)\n",
    "    ml_metrics_specificity.append(specificity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'True Positives: {true_positives.shape[0]}')\n",
    "    print(f'False Positives: {false_positives.shape[0]}')\n",
    "    print(f'True Negatives: {true_negatives.shape[0]}')\n",
    "    print(f'False Negatives: {false_negatives.shape[0]}')\n",
    "    \n",
    "    itercount += 1\n",
    "\n",
    "results_tabular[analysis_name] = pd.concat(labels_results_repetitions, ignore_index=True)\n",
    "\n",
    "\n",
    "ml_metrics_one_analysis = pd.DataFrame({'positives': [],\n",
    "              'negatives': [], \n",
    "              'true_positives': [], \n",
    "              'true_negatives': [],\n",
    "              'false_negatives': [], \n",
    "              'false_positives': [], \n",
    "              'sensitivity': [], \n",
    "              'specificity': []})\n",
    "\n",
    "ml_metrics_one_analysis['analysis_name'] = ml_metrics_analysis_name\n",
    "ml_metrics_one_analysis['time_stamp'] = ml_metrics_time_stamp\n",
    "ml_metrics_one_analysis['positives'] = ml_metrics_positives\n",
    "ml_metrics_one_analysis['negatives'] = ml_metrics_negatives\n",
    "ml_metrics_one_analysis['true_positives'] = ml_metrics_true_positives\n",
    "ml_metrics_one_analysis['false_positives'] = ml_metrics_false_positives\n",
    "ml_metrics_one_analysis['true_negatives'] = ml_metrics_true_negatives\n",
    "ml_metrics_one_analysis['false_negatives'] = ml_metrics_false_negatives\n",
    "ml_metrics_one_analysis['sensitivity'] = ml_metrics_sensitivity\n",
    "ml_metrics_one_analysis['specificity'] = ml_metrics_specificity\n",
    "\n",
    "ml_metrics = pd.concat([ml_metrics, ml_metrics_one_analysis], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9e5a5-aaac-448b-93bf-0d206c3cb70b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3303f44d-a354-45eb-9f8d-d5d4bf792b0d",
   "metadata": {},
   "source": [
    "## Identify non-photo images with precise prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80e6b6-238c-4b20-abeb-8b493d77aa07",
   "metadata": {},
   "source": [
    "### Set parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163ecb1-1e6b-4666-8449-a7c305dc7c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters: \n",
    "analysis_name = 'is_photo_precise_struct_minicpm'\n",
    "create_analysis_prompt = create_analysis_prompt_precise\n",
    "keys_list_expected = ['image_of_image', 'additional_comments']\n",
    "response_variable = 'image_of_image'\n",
    "\n",
    "label_name = 'is_photo'\n",
    "prediction_name = 'is_photo_pred'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8d326-ab14-41b7-a2b6-7923a656080d",
   "metadata": {},
   "source": [
    "### Prepare data objects: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deefe89a-6204-423e-ae55-39bd1e6f0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data objects: \n",
    "labels_results_repetitions = []\n",
    "response_dictionaries[analysis_name] = {}\n",
    "\n",
    "ml_metrics_analysis_name = []\n",
    "ml_metrics_time_stamp = []\n",
    "ml_metrics_positives = []\n",
    "ml_metrics_negatives = []\n",
    "ml_metrics_true_positives = []\n",
    "ml_metrics_false_positives = []\n",
    "ml_metrics_true_negatives = []\n",
    "ml_metrics_false_negatives = []\n",
    "ml_metrics_sensitivity = []\n",
    "ml_metrics_specificity = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef94359-cad5-44fe-91ca-892c7d6a1cdc",
   "metadata": {},
   "source": [
    "### Extract and store results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded7d60-fdf8-4e12-8f23-1f27aad7c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "itercount = 0\n",
    "\n",
    "while itercount < 5:\n",
    "\n",
    "    # Analysis with LLM: \n",
    "    timestamp_start_is_photo_analysis, timestamp_end_is_photo_analysis, image_descr = analyse_giub_img_dir_llm(jpg_data_path, create_analysis_prompt, model_function)\n",
    "\n",
    "    # Calculate duration of analysis: \n",
    "    duration = timestamp_end_is_photo_analysis - timestamp_start_is_photo_analysis\n",
    "    total_seconds = duration.total_seconds()\n",
    "    print(total_seconds)\n",
    "\n",
    "    # Store information about duration: \n",
    "    time_analyses, time_analyses_for_df = store_duration(time_analyses, time_analyses_for_df, analysis_name, \n",
    "                   duration,timestamp_start_is_photo_analysis,\n",
    "                  timestamp_end_is_photo_analysis)\n",
    "    \n",
    "    # Store dictionary with LLM responses:\n",
    "    response_dictionaries[analysis_name][timestamp_start_is_photo_analysis] = image_descr\n",
    "    \n",
    "    # Extract LLM responses from dictionary:\n",
    "    img_ids, is_photo, img_ids_closer_inspection = \\\n",
    "    llm_o.extract_vals_from_response_dict(img_ids, image_descr, keys_list_expected, response_variable)\n",
    "    \n",
    "    # Check if the response variable lists has the same length as id list:\n",
    "    # print('Length of img_ids:')\n",
    "    # print(len(img_ids))\n",
    "    # print('Length of is_photo:')\n",
    "    # print(len(is_photo))\n",
    "    \n",
    "    # Put response variables into data frame: \n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids)\n",
    "    predictions = pd.DataFrame({'image_id': img_ids, \n",
    "                               prediction_name: is_photo,\n",
    "                               'time_stamp': timestamp_ids})\n",
    "    \n",
    "    # Check for missing values:\n",
    "    # print(predictions.isnull().any().any())\n",
    "    # print(predictions.isna().any().any())\n",
    "    # print(has_missing_comprehensive(predictions))\n",
    "    \n",
    "    # Merge label data with the predictions:\n",
    "    label_data_c = label_data.copy()\n",
    "    labels_results = label_data_c.merge(predictions, how='inner', on='image_id')\n",
    "    print(labels_results.shape)\n",
    "    labels_results_repetitions.append(labels_results)\n",
    "    \n",
    "    # Save labels and predictions in dictionary: \n",
    "    #results_tabular[analysis_name] = {}\n",
    "    #results_tabular[analysis_name][timestamp_start_is_photo_analysis] = labels_results\n",
    "    \n",
    "    # Save image list for closer inspection:\n",
    "    timestamp_ids = [timestamp_start_is_photo_analysis] * len(img_ids_closer_inspection)\n",
    "    imgs_closer_inspection = pd.DataFrame({'image_id': img_ids_closer_inspection,\n",
    "    'time_stamp': timestamp_ids})\n",
    "    images_closer_inspection[analysis_name] = imgs_closer_inspection\n",
    "    \n",
    "    \n",
    "    # Calculate sensitivity and specificity for photography predictions and get lists images with positive photography predictions:\n",
    "    subsets_and_metrics = llm_o.get_classification_subsets_metrics(labels_results, label_name, prediction_name)\n",
    "    positives, negatives, true_positives, true_negatives, \\\n",
    "    false_negatives, false_positives, sensitivity, specificity = subsets_and_metrics\n",
    "    # ml_metrics['analysis_name'] = analysis_name\n",
    "    # ml_metrics['time_stamp'] = timestamp_start_is_photo_analysis\n",
    "    # ml_metrics['positives'] = positives.shape[0]\n",
    "    # ml_metrics['negatives'] = negatives.shape[0]\n",
    "    # ml_metrics['true_positives'] = true_positives.shape[0]\n",
    "    # ml_metrics['false_positives'] = false_positives.shape[0]\n",
    "    # ml_metrics['true_negatives'] = true_negatives.shape[0]\n",
    "    # ml_metrics['false_negatives'] = false_negatives.shape[0]\n",
    "    # ml_metrics['sensitivity'] = sensitivity\n",
    "    # ml_metrics['specificity'] = specificity\n",
    "\n",
    "\n",
    "    ml_metrics_analysis_name.append(analysis_name)\n",
    "    ml_metrics_time_stamp.append(timestamp_start_is_photo_analysis)\n",
    "    ml_metrics_positives.append(positives.shape[0])\n",
    "    ml_metrics_negatives.append(negatives.shape[0])\n",
    "    ml_metrics_true_positives.append(true_positives.shape[0])\n",
    "    ml_metrics_false_positives.append(false_positives.shape[0])\n",
    "    ml_metrics_true_negatives.append(true_negatives.shape[0])\n",
    "    ml_metrics_false_negatives.append(false_negatives.shape[0])\n",
    "    ml_metrics_sensitivity.append(sensitivity)\n",
    "    ml_metrics_specificity.append(specificity)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'True Positives: {true_positives.shape[0]}')\n",
    "    print(f'False Positives: {false_positives.shape[0]}')\n",
    "    print(f'True Negatives: {true_negatives.shape[0]}')\n",
    "    print(f'False Negatives: {false_negatives.shape[0]}')\n",
    "    \n",
    "    itercount += 1\n",
    "\n",
    "results_tabular[analysis_name] = pd.concat(labels_results_repetitions, ignore_index=True)\n",
    "\n",
    "\n",
    "ml_metrics_one_analysis = pd.DataFrame({'positives': [],\n",
    "              'negatives': [], \n",
    "              'true_positives': [], \n",
    "              'true_negatives': [],\n",
    "              'false_negatives': [], \n",
    "              'false_positives': [], \n",
    "              'sensitivity': [], \n",
    "              'specificity': []})\n",
    "\n",
    "ml_metrics_one_analysis['analysis_name'] = ml_metrics_analysis_name\n",
    "ml_metrics_one_analysis['time_stamp'] = ml_metrics_time_stamp\n",
    "ml_metrics_one_analysis['positives'] = ml_metrics_positives\n",
    "ml_metrics_one_analysis['negatives'] = ml_metrics_negatives\n",
    "ml_metrics_one_analysis['true_positives'] = ml_metrics_true_positives\n",
    "ml_metrics_one_analysis['false_positives'] = ml_metrics_false_positives\n",
    "ml_metrics_one_analysis['true_negatives'] = ml_metrics_true_negatives\n",
    "ml_metrics_one_analysis['false_negatives'] = ml_metrics_false_negatives\n",
    "ml_metrics_one_analysis['sensitivity'] = ml_metrics_sensitivity\n",
    "ml_metrics_one_analysis['specificity'] = ml_metrics_specificity\n",
    "\n",
    "ml_metrics = pd.concat([ml_metrics, ml_metrics_one_analysis], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e15ee-c214-4674-a2a2-30a7560dbcdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50def63-bb3a-4edd-b17e-17014e40a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_id = timestamp_start_is_photo_analysis.strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b84e40-4271-4c8c-9fb0-dcb48846032c",
   "metadata": {},
   "source": [
    "## Save ml-metrics data frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d21010-357d-4ff5-9277-116c9c3da7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file name: \n",
    "#date = str(timestamp_end_is_photo_analysis).split('.')[0][0:10]\n",
    "filename = 'ml_metrics_prompt_exp_struct_minicpm_' + timestamp_id + '.csv'\n",
    "ml_metrics_output_path = os.path.join(data_path, filename)\n",
    "\n",
    "# Save csv-file: \n",
    "ml_metrics.to_csv(ml_metrics_output_path, index=False)\n",
    "\n",
    "# Reload saved csv table to check if saving worked:\n",
    "ml_metrics_reloaded = pd.read_csv(ml_metrics_output_path)\n",
    "ml_metrics_reloaded.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d3dae-dfeb-4ab3-b327-b73ad9695577",
   "metadata": {},
   "source": [
    "## Save response dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b3f1d-fd66-45f3-9105-7717b6a6aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define file name: \n",
    "#date = str(timestamp_end_is_photo_analysis).split('.')[0][0:10]\n",
    "filename = 'responses_prompt_exp_struct_minicpm_' + timestamp_id + '.pkl'\n",
    "\n",
    "# Save dictionary with LLM responses:\n",
    "img_analysis_output_path = os.path.join(data_path, filename)\n",
    "with open(img_analysis_output_path, 'wb') as f:\n",
    "   pickle.dump(response_dictionaries, f)\n",
    "\n",
    "# Reload saved dictionary to check if saving worked:\n",
    "with open(img_analysis_output_path, 'rb') as f:\n",
    "   reloaded_image_descr = pickle.load(f)\n",
    "\n",
    "# Check if original and reloaded dictionary are the same:\n",
    "print(len(image_descr))\n",
    "print(type(image_descr))\n",
    "print(type(reloaded_image_descr))\n",
    "print(len(reloaded_image_descr))\n",
    "\n",
    "print(image_descr.keys() == reloaded_image_descr.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1319be-905b-471f-b9fd-706b0408a780",
   "metadata": {},
   "source": [
    "## Save labels and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be394aa6-5b9e-4050-b684-d5cff2ca6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define file name: \n",
    "#current_timestamp = pd.Timestamp.now()\n",
    "#current_date_time = current_timestamp.strftime('%Y-%m-%d %H:%M')\n",
    "results_file_name = 'results_prompt_exp_struct_minicpm_' + timestamp_id + '.pkl'\n",
    "\n",
    "# Save dictionary with LLM responses:\n",
    "results_tabular_output_path = os.path.join(data_path, results_file_name)\n",
    "with open(results_tabular_output_path, 'wb') as f:\n",
    "   pickle.dump(results_tabular, f)\n",
    "\n",
    "# Reload saved dictionary to check if saving worked:\n",
    "with open(results_tabular_output_path, 'rb') as f:\n",
    "   reloaded_results_tabular = pickle.load(f)\n",
    "\n",
    "# Check if original and reloaded dictionary are the same:\n",
    "print(len(results_tabular))\n",
    "print(type(results_tabular))\n",
    "print(type(reloaded_results_tabular))\n",
    "print(len(reloaded_results_tabular))\n",
    "\n",
    "print(results_tabular.keys() == reloaded_results_tabular.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d2ac2-2eba-4d5a-a128-27c055043b46",
   "metadata": {},
   "source": [
    "## Calculate duration of analysis overall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6671fb-d2e6-4b18-bf58-7ebb322d0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_end_workflow = pd.Timestamp.now()\n",
    "timestamp_end_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8c4bc-ec18-4a6d-b59a-2656f485274c",
   "metadata": {},
   "source": [
    "## Save time analyses: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39f470-ad16-496b-904e-21fbae7ba317",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define file name: \n",
    "# current_date_time = current_timestamp.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "time_analyses_df_file_name = 'times_df_prompt_exp_struct_minicpm_' + timestamp_id + '.pkl'\n",
    "\n",
    "# Save dictionary:\n",
    "time_analyses_df_output_path = os.path.join(data_path, time_analyses_df_file_name)\n",
    "with open(time_analyses_df_output_path, 'wb') as f:\n",
    "   pickle.dump(time_analyses_for_df, f)\n",
    "\n",
    "# Reload saved dictionary to check if saving worked:\n",
    "with open(time_analyses_df_output_path, 'rb') as f:\n",
    "   reloaded_time_analyses_for_df = pickle.load(f)\n",
    "\n",
    "# Check if original and reloaded dictionary are the same:\n",
    "print(len(time_analyses_for_df))\n",
    "print(type(time_analyses_for_df))\n",
    "print(type(reloaded_time_analyses_for_df))\n",
    "print(len(reloaded_time_analyses_for_df))\n",
    "\n",
    "print(time_analyses_for_df.keys() == reloaded_time_analyses_for_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a5430-02fe-42d1-ad28-241c5edf48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9704fd8-042c-437f-b552-5bd05b686f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ea35c-0969-4df8-932c-9dbe13ec236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9304483-e32e-472e-accb-87f03c2faca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1879e3b-d293-4dc2-9178-0367e8051f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbaaf60-5011-48aa-9ed9-5570f7c772c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0032e47-c2fc-4fb0-957e-949421dd8e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41035e54-1c64-4d18-b0f0-f9bd6f73086a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test-qwen-env)",
   "language": "python",
   "name": "test_qwen-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
