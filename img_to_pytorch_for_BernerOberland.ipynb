{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90991bc-d3a5-48a0-a815-15f359081594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from source import image_id_converter as img_idc\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f2ee2-9479-4eb2-8529-370a87703ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be23f125-798f-49bb-b647-b1a254c216d1",
   "metadata": {},
   "source": [
    "### Older version: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad118521-b553-45f7-bf08-62487997f7ae",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def load_image_paths(image_directory, extensions=('.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "    \"\"\"\n",
    "    Load all image file paths from a directory.\n",
    "    \n",
    "    Args:\n",
    "        image_directory (str): Path to directory containing images\n",
    "        extensions (tuple): Allowed file extensions\n",
    "        \n",
    "    Returns:\n",
    "        list: Sorted list of image file paths\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    image_dir = Path(image_directory)\n",
    "    \n",
    "    if not image_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {image_directory} does not exist\")\n",
    "    \n",
    "    for ext in extensions:\n",
    "        # Find files with current extension (case insensitive)\n",
    "        image_paths.extend(image_dir.glob(f\"*{ext}\"))\n",
    "        image_paths.extend(image_dir.glob(f\"*{ext.upper()}\"))\n",
    "    \n",
    "    # Sort paths to ensure consistent ordering\n",
    "    image_paths = sorted([str(path) for path in image_paths])\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images in {image_directory}\")\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "def convert_to_grayscale(image):\n",
    "    \"\"\"\n",
    "    Convert PIL image to grayscale.\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Grayscale image\n",
    "    \"\"\"\n",
    "    return image.convert('L')\n",
    "\n",
    "\n",
    "def apply_aging_effect(image):\n",
    "    \"\"\"\n",
    "    Apply aging effects to PIL image (adapted from your existing function).\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Aged image\n",
    "    \"\"\"\n",
    "    # Ensure image is in RGB mode for aging effects\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Heavy JPEG compression using temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:\n",
    "        temp_path = temp_file.name\n",
    "        image.save(temp_path, 'JPEG', quality=15)\n",
    "        image = Image.open(temp_path)\n",
    "        os.unlink(temp_path)  # Clean up temp file\n",
    "    \n",
    "    # Significant brightness reduction\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    image = enhancer.enhance(0.8)\n",
    "    \n",
    "    # Low contrast\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    image = enhancer.enhance(0.9)\n",
    "    \n",
    "    # Add significant noise\n",
    "    img_array = np.array(image)\n",
    "    noise = np.random.normal(0, 0.1, img_array.shape).astype(np.uint8)\n",
    "    img_array = np.clip(img_array.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "    image = Image.fromarray(img_array)\n",
    "    \n",
    "    # Strong blur\n",
    "    image = image.filter(ImageFilter.GaussianBlur(radius=1.5))\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(28, 28), convert_grayscale=True, apply_aging=False):\n",
    "    \"\"\"\n",
    "    Load and preprocess a single image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to image file\n",
    "        target_size (tuple): Target size for resizing (width, height)\n",
    "        convert_grayscale (bool): Whether to convert to grayscale\n",
    "        apply_aging (bool): Whether to apply aging effects\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Preprocessed image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Apply aging effects first (works best on RGB images)\n",
    "        if apply_aging:\n",
    "            image = apply_aging_effect(image)\n",
    "        \n",
    "        # Convert to grayscale if requested\n",
    "        if convert_grayscale:\n",
    "            image = convert_to_grayscale(image)\n",
    "        \n",
    "        # Resize to target size\n",
    "        image = image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_transforms(mean=0.5, std=1.0):\n",
    "    \"\"\"\n",
    "    Create image transforms matching your MNIST setup.\n",
    "    \n",
    "    Args:\n",
    "        mean (float): Normalization mean\n",
    "        std (float): Normalization standard deviation\n",
    "        \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: Transform pipeline\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean,), (std,))\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def create_label_transform():\n",
    "    \"\"\"\n",
    "    Create label transform matching your MNIST setup.\n",
    "    \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: Label transform pipeline\n",
    "    \"\"\"\n",
    "    label_transform = transforms.Compose([\n",
    "        lambda x: torch.LongTensor([x])\n",
    "    ])\n",
    "    return label_transform\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class that mimics torchvision.datasets.MNIST structure.\n",
    "    \n",
    "    This dataset loads images lazily (on-demand) and applies preprocessing\n",
    "    and transforms similar to how MNIST dataset works.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, target_size=(28, 28), \n",
    "                 convert_grayscale=True, apply_aging=False,\n",
    "                 transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            image_paths (list): List of image file paths\n",
    "            labels (list): List of integer labels corresponding to images\n",
    "            target_size (tuple): Target size for resizing images\n",
    "            convert_grayscale (bool): Whether to convert images to grayscale\n",
    "            apply_aging (bool): Whether to apply aging effects\n",
    "            transform (callable): Transform to apply to images\n",
    "            target_transform (callable): Transform to apply to labels\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.target_size = target_size\n",
    "        self.convert_grayscale = convert_grayscale\n",
    "        self.apply_aging = apply_aging\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Validate that we have equal number of images and labels\n",
    "        if len(image_paths) != len(labels):\n",
    "            raise ValueError(f\"Number of images ({len(image_paths)}) must match number of labels ({len(labels)})\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, label) where image is a tensor and label is a tensor\n",
    "        \"\"\"\n",
    "        # Load and preprocess image (lazy loading)\n",
    "        image = load_and_preprocess_image(\n",
    "            self.image_paths[idx],\n",
    "            target_size=self.target_size,\n",
    "            convert_grayscale=self.convert_grayscale,\n",
    "            apply_aging=self.apply_aging\n",
    "        )\n",
    "        \n",
    "        # Handle case where image loading failed\n",
    "        if image is None:\n",
    "            # Return a black image as fallback\n",
    "            if self.convert_grayscale:\n",
    "                image = Image.new('L', self.target_size, 0)\n",
    "            else:\n",
    "                image = Image.new('RGB', self.target_size, (0, 0, 0))\n",
    "        \n",
    "        # Get label\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "def create_image_dataset(image_paths, labels, target_size=(28, 28),\n",
    "                        convert_grayscale=True, apply_aging=False,\n",
    "                        mean=0.5, std=1.0):\n",
    "    \"\"\"\n",
    "    Create a complete image dataset ready for use with DataLoader.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of image file paths\n",
    "        labels (list): List of integer labels for the images\n",
    "        target_size (tuple): Target size for resizing images\n",
    "        convert_grayscale (bool): Whether to convert images to grayscale\n",
    "        apply_aging (bool): Whether to apply aging effects\n",
    "        mean (float): Normalization mean\n",
    "        std (float): Normalization standard deviation\n",
    "        \n",
    "    Returns:\n",
    "        CustomImageDataset: Dataset ready for use with DataLoader\n",
    "    \"\"\"\n",
    "    # Step 1: Create transforms\n",
    "    transform = create_transforms(mean=mean, std=std)\n",
    "    target_transform = create_label_transform()\n",
    "    \n",
    "    # Step 2: Create dataset\n",
    "    dataset = CustomImageDataset(\n",
    "        image_paths=image_paths,\n",
    "        labels=labels,\n",
    "        target_size=target_size,\n",
    "        convert_grayscale=convert_grayscale,\n",
    "        apply_aging=apply_aging,\n",
    "        transform=transform,\n",
    "        target_transform=target_transform\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def analyze_image_sizes(image_directory, extensions=('.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "    \"\"\"\n",
    "    Analyze image sizes in a directory to help determine appropriate target_size.\n",
    "    \n",
    "    Args:\n",
    "        image_directory (str): Path to directory containing images\n",
    "        extensions (tuple): Allowed file extensions\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing size analysis results\n",
    "    \"\"\"\n",
    "    # Get all image paths\n",
    "    image_paths = load_image_paths(image_directory, extensions)\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(\"No images found in directory\")\n",
    "        return None\n",
    "    \n",
    "    sizes = []\n",
    "    widths = []\n",
    "    heights = []\n",
    "    failed_images = []\n",
    "    \n",
    "    print(f\"Analyzing {len(image_paths)} images...\")\n",
    "    \n",
    "    # Analyze each image\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "                sizes.append((width, height))\n",
    "                widths.append(width)\n",
    "                heights.append(height)\n",
    "        except Exception as e:\n",
    "            failed_images.append((image_path, str(e)))\n",
    "            print(f\"Failed to read {image_path}: {e}\")\n",
    "        \n",
    "        # Progress indicator for large datasets\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(image_paths)} images...\")\n",
    "    \n",
    "    if not sizes:\n",
    "        print(\"No valid images found\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate statistics\n",
    "    unique_sizes = list(set(sizes))\n",
    "    all_same_size = len(unique_sizes) == 1\n",
    "    \n",
    "    min_width = min(widths)\n",
    "    max_width = max(widths)\n",
    "    avg_width = sum(widths) / len(widths)\n",
    "    \n",
    "    min_height = min(heights)\n",
    "    max_height = max(heights)\n",
    "    avg_height = sum(heights) / len(heights)\n",
    "    \n",
    "    min_size = (min_width, min_height)\n",
    "    max_size = (max_width, max_height)\n",
    "    avg_size = (avg_width, avg_height)\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'total_images': len(image_paths),\n",
    "        'valid_images': len(sizes),\n",
    "        'failed_images': len(failed_images),\n",
    "        'all_same_size': all_same_size,\n",
    "        'unique_sizes_count': len(unique_sizes),\n",
    "        'min_size': min_size,\n",
    "        'max_size': max_size,\n",
    "        'avg_size': avg_size,\n",
    "        'min_width': min_width,\n",
    "        'max_width': max_width,\n",
    "        'avg_width': avg_width,\n",
    "        'min_height': min_height,\n",
    "        'max_height': max_height,\n",
    "        'avg_height': avg_height,\n",
    "        'failed_images': failed_images\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"IMAGE SIZE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total images found: {results['total_images']}\")\n",
    "    print(f\"Valid images: {results['valid_images']}\")\n",
    "    print(f\"Failed to read: {results['failed_images']}\")\n",
    "    print(f\"\\nAll images same size: {'Yes' if all_same_size else 'No'}\")\n",
    "    print(f\"Number of unique sizes: {results['unique_sizes_count']}\")\n",
    "    \n",
    "    print(f\"\\nSize ranges:\")\n",
    "    print(f\"  Minimum size: {min_size[0]} x {min_size[1]}\")\n",
    "    print(f\"  Maximum size: {max_size[0]} x {max_size[1]}\")\n",
    "    print(f\"  Average size: {avg_size[0]:.1f} x {avg_size[1]:.1f}\")\n",
    "    \n",
    "    print(f\"\\nWidth range: {min_width} - {max_width} (avg: {avg_width:.1f})\")\n",
    "    print(f\"Height range: {min_height} - {max_height} (avg: {avg_height:.1f})\")\n",
    "    \n",
    "    if results['failed_images']:\n",
    "        print(f\"\\nFailed images:\")\n",
    "        for path, error in results['failed_images'][:5]:  # Show first 5 failures\n",
    "            print(f\"  {path}: {error}\")\n",
    "        if len(results['failed_images']) > 5:\n",
    "            print(f\"  ... and {len(results['failed_images']) - 5} more\")\n",
    "    \n",
    "    # Suggest target size\n",
    "    if all_same_size:\n",
    "        print(f\"\\nRecommendation: Use target_size={min_size} (all images are the same size)\")\n",
    "    else:\n",
    "        # Suggest a reasonable target size based on minimum dimensions\n",
    "        suggested_size = min(min_width, min_height)\n",
    "        # Round to common sizes\n",
    "        common_sizes = [28, 32, 64, 128, 224, 256, 512]\n",
    "        suggested_size = min(common_sizes, key=lambda x: abs(x - suggested_size))\n",
    "        print(f\"\\nRecommendation: Consider target_size=({suggested_size}, {suggested_size})\")\n",
    "        print(f\"  (Based on minimum dimension and common image sizes)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def collate_ae_dataset(samples, noise_rate=0.0, device='cpu'):\n",
    "    \"\"\"\n",
    "    The function collates samples into a batch, and creates noisy samples if DENOISING is True\n",
    "    for the denoising autoencoder.\n",
    "    \n",
    "    Args:\n",
    "        samples: List of (image, label) tuples from the dataset\n",
    "        noise_rate (float): Rate of noise to add (0.0 = no noise)\n",
    "        device (str or torch.device): Device to move tensors to\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (noisy_images, clean_images, labels) all on specified device\n",
    "    \"\"\"\n",
    "    xs = [s[0] for s in samples]\n",
    "    ys = [s[1] for s in samples]\n",
    "    # Extracts the first element (input data) from each sample into list xs.\n",
    "    # Extracts the second element (labels or targets) from each sample into list ys.\n",
    "    # This assumes each sample is a tuple or list with at least two elements.\n",
    "    \n",
    "    xs = torch.stack(xs)\n",
    "    ys = torch.concat(ys)\n",
    "    # torch.stack(xs) combines the list of input tensors into a single \n",
    "    # tensor along a new dimension (creating a batch dimension).\n",
    "    # torch.concat(ys) concatenates the label tensors along \n",
    "    # the existing first dimension. This suggests the labels might have \n",
    "    # variable lengths or already include a batch-like dimension.\n",
    "    \n",
    "    add_noise = noise_rate > 0.\n",
    "    # Checks if noise should be added based on noise_rate parameter.\n",
    "    # If noise_rate is greater than 0, noise will be added to the inputs.\n",
    "    \n",
    "    if add_noise:\n",
    "        sh = xs.shape\n",
    "        noise_mask = torch.bernoulli(torch.full(sh, noise_rate))  # 0 (keep) or 1 (replace with noise)\n",
    "        # Gets the shape of the input tensor batch.\n",
    "        # Creates a binary mask using Bernoulli sampling, where each element has noise_rate probability of being 1 \n",
    "        # (indicating where noise will be applied) and 1-noise_rate probability of being 0.\n",
    "              \n",
    "        sp_noise = torch.bernoulli(torch.full(sh, 0.5))-0.5  # -0.5 or 0.5\n",
    "        # Generates the actual noise values as either -0.5 or 0.5.\n",
    "        # First creates a tensor of the same shape filled with 0.5, then applies Bernoulli \n",
    "        # sampling to get 0s or 1s.\n",
    "        # Subtracts 0.5 to convert to -0.5 or 0.5 (this creates salt and pepper noise).\n",
    "          \n",
    "        xns = xs * (1-noise_mask) + sp_noise * noise_mask\n",
    "        # Creates the noisy input xns by:\n",
    "            # Keeping original values where the mask is 0: xs * (1-noise_mask)\n",
    "            # Adding noise values where the mask is 1: sp_noise * noise_mask\n",
    "            # The result is a tensor where some values are preserved \n",
    "            # from the original input and others are replaced with noise.\n",
    "        \n",
    "        # sp = sp_noise\n",
    "    else:\n",
    "        xns = xs\n",
    "    # If no noise is to be added, the noisy input is the same as the original input.\n",
    "    \n",
    "    return xns.to(device), xs.to(device), ys.to(device)\n",
    "    # Returns three tensors, all moved to the specified device (likely GPU):\n",
    "    # xns: The inputs with noise added (or original inputs if no noise)\n",
    "    # xs: The original clean inputs\n",
    "    # ys: The labels or targets\n",
    "    #\n",
    "    #\n",
    "    # This return structure is typical for denoising autoencoders, where you need \n",
    "    # both the noisy input (fed to the encoder) and the clean target \n",
    "    # (used to compute the reconstruction loss).\n",
    "    #\n",
    "    # This function is specifically designed for training denoising autoencoders, \n",
    "    # where the model learns to remove noise from corrupted inputs by trying \n",
    "    # to reconstruct the original clean data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432257d8-eb51-445b-9b4c-d640cf973777",
   "metadata": {},
   "source": [
    "### New version:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3fa956-d811-4260-acca-d6afc29dd60d",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def load_image_paths(image_directory, extensions=('.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "    \"\"\"\n",
    "    Load all image file paths from a directory.\n",
    "    \n",
    "    Args:\n",
    "        image_directory (str): Path to directory containing images\n",
    "        extensions (tuple): Allowed file extensions\n",
    "        \n",
    "    Returns:\n",
    "        list: Sorted list of image file paths\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    image_dir = Path(image_directory)\n",
    "    \n",
    "    if not image_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {image_directory} does not exist\")\n",
    "    \n",
    "    for ext in extensions:\n",
    "        # Find files with current extension (case insensitive)\n",
    "        image_paths.extend(image_dir.glob(f\"*{ext}\"))\n",
    "        image_paths.extend(image_dir.glob(f\"*{ext.upper()}\"))\n",
    "    \n",
    "    # Sort paths to ensure consistent ordering\n",
    "    image_paths = sorted([str(path) for path in image_paths])\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images in {image_directory}\")\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "def convert_to_grayscale(image):\n",
    "    \"\"\"\n",
    "    Convert PIL image to grayscale.\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Grayscale image\n",
    "    \"\"\"\n",
    "    return image.convert('L')\n",
    "\n",
    "\n",
    "def apply_aging_effect(image):\n",
    "    \"\"\"\n",
    "    Apply aging effects to PIL image (adapted from your existing function).\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Aged image\n",
    "    \"\"\"\n",
    "    # Ensure image is in RGB mode for aging effects\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Heavy JPEG compression using temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:\n",
    "        temp_path = temp_file.name\n",
    "        image.save(temp_path, 'JPEG', quality=15)\n",
    "        image = Image.open(temp_path)\n",
    "        os.unlink(temp_path)  # Clean up temp file\n",
    "    \n",
    "    # Significant brightness reduction\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    image = enhancer.enhance(0.8)\n",
    "    \n",
    "    # Low contrast\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    image = enhancer.enhance(0.9)\n",
    "    \n",
    "    # Add significant noise\n",
    "    img_array = np.array(image)\n",
    "    noise = np.random.normal(0, 0.1, img_array.shape).astype(np.uint8)\n",
    "    img_array = np.clip(img_array.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "    image = Image.fromarray(img_array)\n",
    "    \n",
    "    # Strong blur\n",
    "    image = image.filter(ImageFilter.GaussianBlur(radius=1.5))\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(28, 28), convert_grayscale=True, apply_aging=False):\n",
    "    \"\"\"\n",
    "    Load and preprocess a single image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to image file\n",
    "        target_size (tuple): Target size for resizing (width, height)\n",
    "        convert_grayscale (bool): Whether to convert to grayscale\n",
    "        apply_aging (bool): Whether to apply aging effects\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Preprocessed image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Apply aging effects first (works best on RGB images)\n",
    "        if apply_aging:\n",
    "            image = apply_aging_effect(image)\n",
    "        \n",
    "        # Convert to grayscale if requested\n",
    "        if convert_grayscale:\n",
    "            image = convert_to_grayscale(image)\n",
    "        \n",
    "        # Resize to target size\n",
    "        image = image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_transforms(mean=0.5, std=1.0):\n",
    "    \"\"\"\n",
    "    Create image transforms matching your MNIST setup.\n",
    "    \n",
    "    Args:\n",
    "        mean (float): Normalization mean\n",
    "        std (float): Normalization standard deviation\n",
    "        \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: Transform pipeline\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean,), (std,))\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def create_label_transform():\n",
    "    \"\"\"\n",
    "    Create label transform matching your MNIST setup.\n",
    "    \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: Label transform pipeline\n",
    "    \"\"\"\n",
    "    label_transform = transforms.Compose([\n",
    "        lambda x: torch.LongTensor([x])\n",
    "    ])\n",
    "    return label_transform\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class that exactly mimics torchvision.datasets.MNIST structure.\n",
    "    \n",
    "    This dataset replicates ALL MNIST properties including .data and .targets tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, target_size=(28, 28), \n",
    "                 convert_grayscale=True, apply_aging=False,\n",
    "                 transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with MNIST-compatible structure.\n",
    "        \n",
    "        Args:\n",
    "            image_paths (list): List of image file paths\n",
    "            labels (list): List of integer labels corresponding to images\n",
    "            target_size (tuple): Target size for resizing images\n",
    "            convert_grayscale (bool): Whether to convert images to grayscale\n",
    "            apply_aging (bool): Whether to apply aging effects\n",
    "            transform (callable): Transform to apply to images\n",
    "            target_transform (callable): Transform to apply to labels\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.target_size = target_size\n",
    "        self.convert_grayscale = convert_grayscale\n",
    "        self.apply_aging = apply_aging\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Validate that we have equal number of images and labels\n",
    "        if len(image_paths) != len(labels):\n",
    "            raise ValueError(f\"Number of images ({len(image_paths)}) must match number of labels ({len(labels)})\")\n",
    "        \n",
    "        # Create MNIST-compatible attributes\n",
    "        self.targets = torch.tensor(labels, dtype=torch.long)  # Shape: [N]\n",
    "        self._data = None  # Will be loaded when first accessed\n",
    "        \n",
    "        print(f\"Initializing dataset with {len(image_paths)} images...\")\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"\n",
    "        MNIST-compatible data property that returns all images as a tensor.\n",
    "        Shape: [N, H, W] for grayscale or [N, H, W, C] for color\n",
    "        Dtype: torch.uint8 (same as MNIST)\n",
    "        \n",
    "        Images are loaded and cached on first access.\n",
    "        \"\"\"\n",
    "        if self._data is None:\n",
    "            print(\"Loading all images into .data tensor (this may take a moment)...\")\n",
    "            self._load_all_images()\n",
    "        return self._data\n",
    "    \n",
    "    def _load_all_images(self):\n",
    "        \"\"\"Load all images into the .data tensor.\"\"\"\n",
    "        all_images = []\n",
    "        \n",
    "        for i, image_path in enumerate(self.image_paths):\n",
    "            # Load and preprocess image\n",
    "            image = load_and_preprocess_image(\n",
    "                image_path,\n",
    "                target_size=self.target_size,\n",
    "                convert_grayscale=self.convert_grayscale,\n",
    "                apply_aging=self.apply_aging\n",
    "            )\n",
    "            \n",
    "            # Handle failed loading\n",
    "            if image is None:\n",
    "                if self.convert_grayscale:\n",
    "                    image = Image.new('L', self.target_size, 0)\n",
    "                else:\n",
    "                    image = Image.new('RGB', self.target_size, (0, 0, 0))\n",
    "            \n",
    "            # Convert to numpy array with uint8 dtype (like MNIST)\n",
    "            img_array = np.array(image, dtype=np.uint8)\n",
    "            all_images.append(img_array)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Loaded {i + 1}/{len(self.image_paths)} images...\")\n",
    "        \n",
    "        # Stack all images and convert to tensor\n",
    "        all_images = np.stack(all_images, axis=0)\n",
    "        self._data = torch.from_numpy(all_images)\n",
    "        \n",
    "        print(f\"Loaded .data tensor with shape: {self._data.shape}, dtype: {self._data.dtype}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, label) where image is a tensor and label is a tensor\n",
    "            \n",
    "        Note: This applies transforms to the raw data, just like MNIST\n",
    "        \"\"\"\n",
    "        # Get raw image from .data tensor (this will load all images if needed)\n",
    "        raw_image = self.data[idx]  # Shape: [H, W] or [H, W, C]\n",
    "        \n",
    "        # Convert tensor back to PIL Image for transforms\n",
    "        if raw_image.dim() == 2:  # Grayscale\n",
    "            image = Image.fromarray(raw_image.numpy(), mode='L')\n",
    "        else:  # Color\n",
    "            image = Image.fromarray(raw_image.numpy(), mode='RGB')\n",
    "        \n",
    "        # Get label from targets tensor\n",
    "        label = self.targets[idx].item()  # Convert to Python int\n",
    "        \n",
    "        # Apply transforms (same as MNIST)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "def create_image_dataset(image_paths, labels, target_size=(28, 28),\n",
    "                        convert_grayscale=True, apply_aging=False,\n",
    "                        mean=0.5, std=1.0):\n",
    "    \"\"\"\n",
    "    Create a complete image dataset ready for use with DataLoader.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of image file paths\n",
    "        labels (list): List of integer labels for the images\n",
    "        target_size (tuple): Target size for resizing images\n",
    "        convert_grayscale (bool): Whether to convert images to grayscale\n",
    "        apply_aging (bool): Whether to apply aging effects\n",
    "        mean (float): Normalization mean\n",
    "        std (float): Normalization standard deviation\n",
    "        \n",
    "    Returns:\n",
    "        CustomImageDataset: Dataset ready for use with DataLoader\n",
    "    \"\"\"\n",
    "    # Step 1: Create transforms\n",
    "    transform = create_transforms(mean=mean, std=std)\n",
    "    target_transform = create_label_transform()\n",
    "    \n",
    "    # Step 2: Create dataset\n",
    "    dataset = CustomImageDataset(\n",
    "        image_paths=image_paths,\n",
    "        labels=labels,\n",
    "        target_size=target_size,\n",
    "        convert_grayscale=convert_grayscale,\n",
    "        apply_aging=apply_aging,\n",
    "        transform=transform,\n",
    "        target_transform=target_transform\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def analyze_image_sizes(image_directory, extensions=('.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "    \"\"\"\n",
    "    Analyze image sizes in a directory to help determine appropriate target_size.\n",
    "    \n",
    "    Args:\n",
    "        image_directory (str): Path to directory containing images\n",
    "        extensions (tuple): Allowed file extensions\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing size analysis results\n",
    "    \"\"\"\n",
    "    # Get all image paths\n",
    "    image_paths = load_image_paths(image_directory, extensions)\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(\"No images found in directory\")\n",
    "        return None\n",
    "    \n",
    "    sizes = []\n",
    "    widths = []\n",
    "    heights = []\n",
    "    failed_images = []\n",
    "    \n",
    "    print(f\"Analyzing {len(image_paths)} images...\")\n",
    "    \n",
    "    # Analyze each image\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "                sizes.append((width, height))\n",
    "                widths.append(width)\n",
    "                heights.append(height)\n",
    "        except Exception as e:\n",
    "            failed_images.append((image_path, str(e)))\n",
    "            print(f\"Failed to read {image_path}: {e}\")\n",
    "        \n",
    "        # Progress indicator for large datasets\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(image_paths)} images...\")\n",
    "    \n",
    "    if not sizes:\n",
    "        print(\"No valid images found\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate statistics\n",
    "    unique_sizes = list(set(sizes))\n",
    "    all_same_size = len(unique_sizes) == 1\n",
    "    \n",
    "    min_width = min(widths)\n",
    "    max_width = max(widths)\n",
    "    avg_width = sum(widths) / len(widths)\n",
    "    \n",
    "    min_height = min(heights)\n",
    "    max_height = max(heights)\n",
    "    avg_height = sum(heights) / len(heights)\n",
    "    \n",
    "    min_size = (min_width, min_height)\n",
    "    max_size = (max_width, max_height)\n",
    "    avg_size = (avg_width, avg_height)\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'total_images': len(image_paths),\n",
    "        'valid_images': len(sizes),\n",
    "        'failed_images': len(failed_images),\n",
    "        'all_same_size': all_same_size,\n",
    "        'unique_sizes_count': len(unique_sizes),\n",
    "        'min_size': min_size,\n",
    "        'max_size': max_size,\n",
    "        'avg_size': avg_size,\n",
    "        'min_width': min_width,\n",
    "        'max_width': max_width,\n",
    "        'avg_width': avg_width,\n",
    "        'min_height': min_height,\n",
    "        'max_height': max_height,\n",
    "        'avg_height': avg_height,\n",
    "        'failed_images': failed_images\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"IMAGE SIZE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total images found: {results['total_images']}\")\n",
    "    print(f\"Valid images: {results['valid_images']}\")\n",
    "    print(f\"Failed to read: {results['failed_images']}\")\n",
    "    print(f\"\\nAll images same size: {'Yes' if all_same_size else 'No'}\")\n",
    "    print(f\"Number of unique sizes: {results['unique_sizes_count']}\")\n",
    "    \n",
    "    print(f\"\\nSize ranges:\")\n",
    "    print(f\"  Minimum size: {min_size[0]} x {min_size[1]}\")\n",
    "    print(f\"  Maximum size: {max_size[0]} x {max_size[1]}\")\n",
    "    print(f\"  Average size: {avg_size[0]:.1f} x {avg_size[1]:.1f}\")\n",
    "    \n",
    "    print(f\"\\nWidth range: {min_width} - {max_width} (avg: {avg_width:.1f})\")\n",
    "    print(f\"Height range: {min_height} - {max_height} (avg: {avg_height:.1f})\")\n",
    "    \n",
    "    if results['failed_images']:\n",
    "        print(f\"\\nFailed images:\")\n",
    "        for path, error in results['failed_images'][:5]:  # Show first 5 failures\n",
    "            print(f\"  {path}: {error}\")\n",
    "        if len(results['failed_images']) > 5:\n",
    "            print(f\"  ... and {len(results['failed_images']) - 5} more\")\n",
    "    \n",
    "    # Suggest target size\n",
    "    if all_same_size:\n",
    "        print(f\"\\nRecommendation: Use target_size={min_size} (all images are the same size)\")\n",
    "    else:\n",
    "        # Suggest a reasonable target size based on minimum dimensions\n",
    "        suggested_size = min(min_width, min_height)\n",
    "        # Round to common sizes\n",
    "        common_sizes = [28, 32, 64, 128, 224, 256, 512]\n",
    "        suggested_size = min(common_sizes, key=lambda x: abs(x - suggested_size))\n",
    "        print(f\"\\nRecommendation: Consider target_size=({suggested_size}, {suggested_size})\")\n",
    "        print(f\"  (Based on minimum dimension and common image sizes)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def collate_ae_dataset(samples, noise_rate=0.0, device='cpu'):\n",
    "    \"\"\"\n",
    "    The function collates samples into a batch, and creates noisy samples if DENOISING is True\n",
    "    for the denoising autoencoder.\n",
    "    \n",
    "    Args:\n",
    "        samples: List of (image, label) tuples from the dataset\n",
    "        noise_rate (float): Rate of noise to add (0.0 = no noise)\n",
    "        device (str or torch.device): Device to move tensors to\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (noisy_images, clean_images, labels) all on specified device\n",
    "    \"\"\"\n",
    "    xs = [s[0] for s in samples]\n",
    "    ys = [s[1] for s in samples]\n",
    "    # Extracts the first element (input data) from each sample into list xs.\n",
    "    # Extracts the second element (labels or targets) from each sample into list ys.\n",
    "    # This assumes each sample is a tuple or list with at least two elements.\n",
    "    \n",
    "    xs = torch.stack(xs)\n",
    "    ys = torch.concat(ys)\n",
    "    # torch.stack(xs) combines the list of input tensors into a single \n",
    "    # tensor along a new dimension (creating a batch dimension).\n",
    "    # torch.concat(ys) concatenates the label tensors along \n",
    "    # the existing first dimension. This suggests the labels might have \n",
    "    # variable lengths or already include a batch-like dimension.\n",
    "    \n",
    "    add_noise = noise_rate > 0.\n",
    "    # Checks if noise should be added based on noise_rate parameter.\n",
    "    # If noise_rate is greater than 0, noise will be added to the inputs.\n",
    "    \n",
    "    if add_noise:\n",
    "        sh = xs.shape\n",
    "        noise_mask = torch.bernoulli(torch.full(sh, noise_rate))  # 0 (keep) or 1 (replace with noise)\n",
    "        # Gets the shape of the input tensor batch.\n",
    "        # Creates a binary mask using Bernoulli sampling, where each element has noise_rate probability of being 1 \n",
    "        # (indicating where noise will be applied) and 1-noise_rate probability of being 0.\n",
    "              \n",
    "        sp_noise = torch.bernoulli(torch.full(sh, 0.5))-0.5  # -0.5 or 0.5\n",
    "        # Generates the actual noise values as either -0.5 or 0.5.\n",
    "        # First creates a tensor of the same shape filled with 0.5, then applies Bernoulli \n",
    "        # sampling to get 0s or 1s.\n",
    "        # Subtracts 0.5 to convert to -0.5 or 0.5 (this creates salt and pepper noise).\n",
    "          \n",
    "        xns = xs * (1-noise_mask) + sp_noise * noise_mask\n",
    "        # Creates the noisy input xns by:\n",
    "            # Keeping original values where the mask is 0: xs * (1-noise_mask)\n",
    "            # Adding noise values where the mask is 1: sp_noise * noise_mask\n",
    "            # The result is a tensor where some values are preserved \n",
    "            # from the original input and others are replaced with noise.\n",
    "        \n",
    "        # sp = sp_noise\n",
    "    else:\n",
    "        xns = xs\n",
    "    # If no noise is to be added, the noisy input is the same as the original input.\n",
    "    \n",
    "    return xns.to(device), xs.to(device), ys.to(device)\n",
    "    # Returns three tensors, all moved to the specified device (likely GPU):\n",
    "    # xns: The inputs with noise added (or original inputs if no noise)\n",
    "    # xs: The original clean inputs\n",
    "    # ys: The labels or targets\n",
    "    #\n",
    "    #\n",
    "    # This return structure is typical for denoising autoencoders, where you need \n",
    "    # both the noisy input (fed to the encoder) and the clean target \n",
    "    # (used to compute the reconstruction loss).\n",
    "    #\n",
    "    # This function is specifically designed for training denoising autoencoders, \n",
    "    # where the model learns to remove noise from corrupted inputs by trying \n",
    "    # to reconstruct the original clean data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db1d20-b0c5-48de-a4b1-db65c23c94b1",
   "metadata": {},
   "source": [
    "### Version 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc60fea-0551-492d-922f-59ffed17f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def load_image_paths(image_directory, extensions=('.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "    \"\"\"\n",
    "    Load all image file paths from a directory.\n",
    "    \n",
    "    Args:\n",
    "        image_directory (str): Path to directory containing images\n",
    "        extensions (tuple): Allowed file extensions\n",
    "        \n",
    "    Returns:\n",
    "        list: Sorted list of image file paths\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    image_dir = Path(image_directory)\n",
    "    \n",
    "    if not image_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {image_directory} does not exist\")\n",
    "    \n",
    "    for ext in extensions:\n",
    "        # Find files with current extension (case insensitive)\n",
    "        image_paths.extend(image_dir.glob(f\"*{ext}\"))\n",
    "        image_paths.extend(image_dir.glob(f\"*{ext.upper()}\"))\n",
    "    \n",
    "    # Sort paths to ensure consistent ordering\n",
    "    image_paths = sorted([str(path) for path in image_paths])\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images in {image_directory}\")\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "def convert_to_grayscale(image):\n",
    "    \"\"\"\n",
    "    Convert PIL image to grayscale.\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Grayscale image\n",
    "    \"\"\"\n",
    "    return image.convert('L')\n",
    "\n",
    "\n",
    "def apply_aging_effect(image):\n",
    "    \"\"\"\n",
    "    Apply aging effects to PIL image (adapted from your existing function).\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Aged image\n",
    "    \"\"\"\n",
    "    # Ensure image is in RGB mode for aging effects\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Heavy JPEG compression using temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:\n",
    "        temp_path = temp_file.name\n",
    "        image.save(temp_path, 'JPEG', quality=15)\n",
    "        image = Image.open(temp_path)\n",
    "        os.unlink(temp_path)  # Clean up temp file\n",
    "    \n",
    "    # Significant brightness reduction\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    image = enhancer.enhance(0.8)\n",
    "    \n",
    "    # Low contrast\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    image = enhancer.enhance(0.9)\n",
    "    \n",
    "    # Add significant noise\n",
    "    img_array = np.array(image)\n",
    "    noise = np.random.normal(0, 0.1, img_array.shape).astype(np.uint8)\n",
    "    img_array = np.clip(img_array.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "    image = Image.fromarray(img_array)\n",
    "    \n",
    "    # Strong blur\n",
    "    image = image.filter(ImageFilter.GaussianBlur(radius=1.5))\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(28, 28), convert_grayscale=True, apply_aging=False):\n",
    "    \"\"\"\n",
    "    Load and preprocess a single image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to image file\n",
    "        target_size (tuple): Target size for resizing (width, height)\n",
    "        convert_grayscale (bool): Whether to convert to grayscale\n",
    "        apply_aging (bool): Whether to apply aging effects\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Preprocessed image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Apply aging effects first (works best on RGB images)\n",
    "        if apply_aging:\n",
    "            image = apply_aging_effect(image)\n",
    "        \n",
    "        # Convert to grayscale if requested\n",
    "        if convert_grayscale:\n",
    "            image = convert_to_grayscale(image)\n",
    "        \n",
    "        # Resize to target size\n",
    "        image = image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_transforms(mean=0.5, std=1.0):\n",
    "    \"\"\"\n",
    "    Create image transforms matching your MNIST setup.\n",
    "    \n",
    "    Args:\n",
    "        mean (float): Normalization mean\n",
    "        std (float): Normalization standard deviation\n",
    "        \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: Transform pipeline\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean,), (std,))\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def create_label_transform():\n",
    "    \"\"\"\n",
    "    Create label transform matching your MNIST setup.\n",
    "    \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: Label transform pipeline\n",
    "    \"\"\"\n",
    "    label_transform = transforms.Compose([\n",
    "        lambda x: torch.LongTensor([x])\n",
    "    ])\n",
    "    return label_transform\n",
    "\n",
    "\n",
    "def analyze_image_sizes(image_directory, extensions=('.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "    \"\"\"\n",
    "    Analyze image sizes in a directory to help determine appropriate target_size.\n",
    "    \n",
    "    Args:\n",
    "        image_directory (str): Path to directory containing images\n",
    "        extensions (tuple): Allowed file extensions\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing size analysis results\n",
    "    \"\"\"\n",
    "    # Get all image paths\n",
    "    image_paths = load_image_paths(image_directory, extensions)\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(\"No images found in directory\")\n",
    "        return None\n",
    "    \n",
    "    sizes = []\n",
    "    widths = []\n",
    "    heights = []\n",
    "    failed_images = []\n",
    "    \n",
    "    print(f\"Analyzing {len(image_paths)} images...\")\n",
    "    \n",
    "    # Analyze each image\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "                sizes.append((width, height))\n",
    "                widths.append(width)\n",
    "                heights.append(height)\n",
    "        except Exception as e:\n",
    "            failed_images.append((image_path, str(e)))\n",
    "            print(f\"Failed to read {image_path}: {e}\")\n",
    "        \n",
    "        # Progress indicator for large datasets\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(image_paths)} images...\")\n",
    "    \n",
    "    if not sizes:\n",
    "        print(\"No valid images found\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate statistics\n",
    "    unique_sizes = list(set(sizes))\n",
    "    all_same_size = len(unique_sizes) == 1\n",
    "    \n",
    "    min_width = min(widths)\n",
    "    max_width = max(widths)\n",
    "    avg_width = sum(widths) / len(widths)\n",
    "    \n",
    "    min_height = min(heights)\n",
    "    max_height = max(heights)\n",
    "    avg_height = sum(heights) / len(heights)\n",
    "    \n",
    "    min_size = (min_width, min_height)\n",
    "    max_size = (max_width, max_height)\n",
    "    avg_size = (avg_width, avg_height)\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'total_images': len(image_paths),\n",
    "        'valid_images': len(sizes),\n",
    "        'failed_images': len(failed_images),\n",
    "        'all_same_size': all_same_size,\n",
    "        'unique_sizes_count': len(unique_sizes),\n",
    "        'min_size': min_size,\n",
    "        'max_size': max_size,\n",
    "        'avg_size': avg_size,\n",
    "        'min_width': min_width,\n",
    "        'max_width': max_width,\n",
    "        'avg_width': avg_width,\n",
    "        'min_height': min_height,\n",
    "        'max_height': max_height,\n",
    "        'avg_height': avg_height,\n",
    "        'failed_images': failed_images\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"IMAGE SIZE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total images found: {results['total_images']}\")\n",
    "    print(f\"Valid images: {results['valid_images']}\")\n",
    "    print(f\"Failed to read: {results['failed_images']}\")\n",
    "    print(f\"\\nAll images same size: {'Yes' if all_same_size else 'No'}\")\n",
    "    print(f\"Number of unique sizes: {results['unique_sizes_count']}\")\n",
    "    \n",
    "    print(f\"\\nSize ranges:\")\n",
    "    print(f\"  Minimum size: {min_size[0]} x {min_size[1]}\")\n",
    "    print(f\"  Maximum size: {max_size[0]} x {max_size[1]}\")\n",
    "    print(f\"  Average size: {avg_size[0]:.1f} x {avg_size[1]:.1f}\")\n",
    "    \n",
    "    print(f\"\\nWidth range: {min_width} - {max_width} (avg: {avg_width:.1f})\")\n",
    "    print(f\"Height range: {min_height} - {max_height} (avg: {avg_height:.1f})\")\n",
    "    \n",
    "    if results['failed_images']:\n",
    "        print(f\"\\nFailed images:\")\n",
    "        for path, error in results['failed_images'][:5]:  # Show first 5 failures\n",
    "            print(f\"  {path}: {error}\")\n",
    "        if len(results['failed_images']) > 5:\n",
    "            print(f\"  ... and {len(results['failed_images']) - 5} more\")\n",
    "    \n",
    "    # Suggest target size\n",
    "    if all_same_size:\n",
    "        print(f\"\\nRecommendation: Use target_size={min_size} (all images are the same size)\")\n",
    "    else:\n",
    "        # Suggest a reasonable target size based on minimum dimensions\n",
    "        suggested_size = min(min_width, min_height)\n",
    "        # Round to common sizes\n",
    "        common_sizes = [28, 32, 64, 128, 224, 256, 512]\n",
    "        suggested_size = min(common_sizes, key=lambda x: abs(x - suggested_size))\n",
    "        print(f\"\\nRecommendation: Consider target_size=({suggested_size}, {suggested_size})\")\n",
    "        print(f\"  (Based on minimum dimension and common image sizes)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class that exactly mimics torchvision.datasets.MNIST structure.\n",
    "    \n",
    "    This dataset replicates ALL MNIST properties including .data and .targets tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, target_size=(28, 28), \n",
    "                 convert_grayscale=True, apply_aging=False,\n",
    "                 transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with MNIST-compatible structure.\n",
    "        \n",
    "        Args:\n",
    "            image_paths (list): List of image file paths\n",
    "            labels (list): List of integer labels corresponding to images\n",
    "            target_size (tuple): Target size for resizing images\n",
    "            convert_grayscale (bool): Whether to convert images to grayscale\n",
    "            apply_aging (bool): Whether to apply aging effects\n",
    "            transform (callable): Transform to apply to images\n",
    "            target_transform (callable): Transform to apply to labels\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.target_size = target_size\n",
    "        self.convert_grayscale = convert_grayscale\n",
    "        self.apply_aging = apply_aging\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Validate that we have equal number of images and labels\n",
    "        if len(image_paths) != len(labels):\n",
    "            raise ValueError(f\"Number of images ({len(image_paths)}) must match number of labels ({len(labels)})\")\n",
    "        \n",
    "        # Create MNIST-compatible attributes\n",
    "        self.targets = torch.tensor(labels, dtype=torch.long)  # Shape: [N]\n",
    "        self._data = None  # Will be loaded when first accessed\n",
    "        \n",
    "        print(f\"Initializing dataset with {len(image_paths)} images...\")\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"\n",
    "        MNIST-compatible data property that returns all images as a tensor.\n",
    "        Shape: [N, H, W] for grayscale or [N, H, W, C] for color\n",
    "        Dtype: torch.uint8 (same as MNIST)\n",
    "        \n",
    "        Images are loaded and cached on first access.\n",
    "        \"\"\"\n",
    "        if self._data is None:\n",
    "            print(\"Loading all images into .data tensor (this may take a moment)...\")\n",
    "            self._load_all_images()\n",
    "        return self._data\n",
    "    \n",
    "    def _load_all_images(self):\n",
    "        \"\"\"Load all images into the .data tensor.\"\"\"\n",
    "        all_images = []\n",
    "        \n",
    "        for i, image_path in enumerate(self.image_paths):\n",
    "            # Load and preprocess image\n",
    "            image = load_and_preprocess_image(\n",
    "                image_path,\n",
    "                target_size=self.target_size,\n",
    "                convert_grayscale=self.convert_grayscale,\n",
    "                apply_aging=self.apply_aging\n",
    "            )\n",
    "            \n",
    "            # Handle failed loading\n",
    "            if image is None:\n",
    "                if self.convert_grayscale:\n",
    "                    image = Image.new('L', self.target_size, 0)\n",
    "                else:\n",
    "                    image = Image.new('RGB', self.target_size, (0, 0, 0))\n",
    "            \n",
    "            # Convert to numpy array with uint8 dtype (like MNIST)\n",
    "            img_array = np.array(image, dtype=np.uint8)\n",
    "            all_images.append(img_array)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Loaded {i + 1}/{len(self.image_paths)} images...\")\n",
    "        \n",
    "        # Stack all images and convert to tensor\n",
    "        all_images = np.stack(all_images, axis=0)\n",
    "        self._data = torch.from_numpy(all_images)\n",
    "        \n",
    "        print(f\"Loaded .data tensor with shape: {self._data.shape}, dtype: {self._data.dtype}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, label) where image is a tensor and label is a tensor\n",
    "            \n",
    "        Note: This applies transforms to the raw data, just like MNIST\n",
    "        \"\"\"\n",
    "        # Get raw image from .data tensor (this will load all images if needed)\n",
    "        raw_image = self.data[idx]  # Shape: [H, W] or [H, W, C]\n",
    "        \n",
    "        # Convert tensor back to PIL Image for transforms\n",
    "        if raw_image.dim() == 2:  # Grayscale\n",
    "            image = Image.fromarray(raw_image.numpy(), mode='L')\n",
    "        else:  # Color\n",
    "            image = Image.fromarray(raw_image.numpy(), mode='RGB')\n",
    "        \n",
    "        # Get label from targets tensor\n",
    "        label = self.targets[idx].item()  # Convert to Python int\n",
    "        \n",
    "        # Apply transforms (same as MNIST)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "def collate_ae_dataset(samples, noise_rate=0.0, device='cpu'):\n",
    "    \"\"\"\n",
    "    The function collates samples into a batch, and creates noisy samples if DENOISING is True\n",
    "    for the denoising autoencoder.\n",
    "    \n",
    "    Args:\n",
    "        samples: List of (image, label) tuples from the dataset\n",
    "        noise_rate (float): Rate of noise to add (0.0 = no noise)\n",
    "        device (str or torch.device): Device to move tensors to\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (noisy_images, clean_images, labels) all on specified device\n",
    "    \"\"\"\n",
    "    xs = [s[0] for s in samples]\n",
    "    ys = [s[1] for s in samples]\n",
    "    # Extracts the first element (input data) from each sample into list xs.\n",
    "    # Extracts the second element (labels or targets) from each sample into list ys.\n",
    "    # This assumes each sample is a tuple or list with at least two elements.\n",
    "    \n",
    "    xs = torch.stack(xs)\n",
    "    ys = torch.concat(ys)\n",
    "    # torch.stack(xs) combines the list of input tensors into a single \n",
    "    # tensor along a new dimension (creating a batch dimension).\n",
    "    # torch.concat(ys) concatenates the label tensors along \n",
    "    # the existing first dimension. This suggests the labels might have \n",
    "    # variable lengths or already include a batch-like dimension.\n",
    "    \n",
    "    add_noise = noise_rate > 0.\n",
    "    # Checks if noise should be added based on noise_rate parameter.\n",
    "    # If noise_rate is greater than 0, noise will be added to the inputs.\n",
    "    \n",
    "    if add_noise:\n",
    "        sh = xs.shape\n",
    "        noise_mask = torch.bernoulli(torch.full(sh, noise_rate))  # 0 (keep) or 1 (replace with noise)\n",
    "        # Gets the shape of the input tensor batch.\n",
    "        # Creates a binary mask using Bernoulli sampling, where each element has noise_rate probability of being 1 \n",
    "        # (indicating where noise will be applied) and 1-noise_rate probability of being 0.\n",
    "              \n",
    "        sp_noise = torch.bernoulli(torch.full(sh, 0.5))-0.5  # -0.5 or 0.5\n",
    "        # Generates the actual noise values as either -0.5 or 0.5.\n",
    "        # First creates a tensor of the same shape filled with 0.5, then applies Bernoulli \n",
    "        # sampling to get 0s or 1s.\n",
    "        # Subtracts 0.5 to convert to -0.5 or 0.5 (this creates salt and pepper noise).\n",
    "          \n",
    "        xns = xs * (1-noise_mask) + sp_noise * noise_mask\n",
    "        # Creates the noisy input xns by:\n",
    "            # Keeping original values where the mask is 0: xs * (1-noise_mask)\n",
    "            # Adding noise values where the mask is 1: sp_noise * noise_mask\n",
    "            # The result is a tensor where some values are preserved \n",
    "            # from the original input and others are replaced with noise.\n",
    "        \n",
    "        # sp = sp_noise\n",
    "    else:\n",
    "        xns = xs\n",
    "    # If no noise is to be added, the noisy input is the same as the original input.\n",
    "    \n",
    "    return xns.to(device), xs.to(device), ys.to(device)\n",
    "    # Returns three tensors, all moved to the specified device (likely GPU):\n",
    "    # xns: The inputs with noise added (or original inputs if no noise)\n",
    "    # xs: The original clean inputs\n",
    "    # ys: The labels or targets\n",
    "    #\n",
    "    #\n",
    "    # This return structure is typical for denoising autoencoders, where you need \n",
    "    # both the noisy input (fed to the encoder) and the clean target \n",
    "    # (used to compute the reconstruction loss).\n",
    "    #\n",
    "    # This function is specifically designed for training denoising autoencoders, \n",
    "    # where the model learns to remove noise from corrupted inputs by trying \n",
    "    # to reconstruct the original clean data.\n",
    "\n",
    "\n",
    "def create_image_dataset(image_paths, labels, target_size=(28, 28),\n",
    "                        convert_grayscale=True, apply_aging=False,\n",
    "                        mean=0.5, std=1.0):\n",
    "    \"\"\"\n",
    "    Create a complete image dataset ready for use with DataLoader.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of image file paths\n",
    "        labels (list): List of integer labels for the images\n",
    "        target_size (tuple): Target size for resizing images\n",
    "        convert_grayscale (bool): Whether to convert images to grayscale\n",
    "        apply_aging (bool): Whether to apply aging effects\n",
    "        mean (float): Normalization mean\n",
    "        std (float): Normalization standard deviation\n",
    "        \n",
    "    Returns:\n",
    "        CustomImageDataset: Dataset ready for use with DataLoader\n",
    "    \"\"\"\n",
    "    # Step 1: Create transforms\n",
    "    transform = create_transforms(mean=mean, std=std)\n",
    "    target_transform = create_label_transform()\n",
    "    \n",
    "    # Step 2: Create dataset\n",
    "    dataset = CustomImageDataset(\n",
    "        image_paths=image_paths,\n",
    "        labels=labels,\n",
    "        target_size=target_size,\n",
    "        convert_grayscale=convert_grayscale,\n",
    "        apply_aging=apply_aging,\n",
    "        transform=transform,\n",
    "        target_transform=target_transform\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae667bca-a240-48be-94f8-9885c944f3e6",
   "metadata": {},
   "source": [
    "def collate_ae_dataset(samples):\n",
    "    \"\"\"\n",
    "    The function collates sampels into a batch, and creates noisy samples if DENOISING is True\n",
    "    for the denoising autoencoder.\n",
    "    \"\"\"\n",
    "    xs = [s[0] for s in samples]\n",
    "    ys = [s[1] for s in samples]\n",
    "    #Extracts the first element (input data) from each sample into list xs.\n",
    "    #Extracts the second element (labels or targets) from each sample into list ys.\n",
    "    #This assumes each sample is a tuple or list with at least two elements.\n",
    "    \n",
    "    xs = torch.stack(xs)\n",
    "    ys = torch.concat(ys)\n",
    "    #torch.stack(xs) combines the list of input tensors into a single \n",
    "    #tensor along a new dimension (creating a batch dimension).\n",
    "    #torch.concat(ys) concatenates the label tensors along \n",
    "    #the existing first dimension. This suggests the labels might have \n",
    "    #variable lengths or already include a batch-like dimension.\n",
    "\n",
    "    add_noise = NOISE_RATE > 0.\n",
    "    #Checks if noise should be added based on a global variable NOISE_RATE.\n",
    "    #If NOISE_RATE is greater than 0, noise will be added to the inputs.\n",
    "    \n",
    "    if add_noise:\n",
    "      sh = xs.shape\n",
    "      noise_mask = torch.bernoulli(torch.full(sh, NOISE_RATE))  # 0 (keep) or 1 (replace with noise)\n",
    "      #Gets the shape of the input tensor batch.\n",
    "      #Creates a binary mask using Bernoulli sampling, where each element has NOISE_RATE probability of being 1 \n",
    "      #(indicating where noise will be applied) and 1-NOISE_RATE probability of being 0.\n",
    "            \n",
    "      sp_noise = torch.bernoulli(torch.full(sh, 0.5))-0.5  # -1 or 1\n",
    "      #Generates the actual noise values as either -0.5 or 0.5.\n",
    "      #First creates a tensor of the same shape filled with 0.5, then applies Bernoulli \n",
    "      #sampling to get 0s or 1s.\n",
    "      #Subtracts 0.5 to convert to -0.5 or 0.5 (this creates salt and pepper noise).\n",
    "        \n",
    "      xns = xs * (1-noise_mask) + sp_noise * noise_mask\n",
    "      #Creates the noisy input xns by:\n",
    "          #Keeping original values where the mask is 0: xs * (1-noise_mask)\n",
    "          #Adding noise values where the mask is 1: sp_noise * noise_mask\n",
    "          #The result is a tensor where some values are preserved \n",
    "          #from the original input and others are replaced with noise.\n",
    "      \n",
    "      # sp = sp_noise\n",
    "    else:\n",
    "       xns = xs\n",
    "    #If no noise is to be added, the noisy input is the same as the original input.\n",
    "\n",
    "    return xns.to(device), xs.to(device), ys.to(device)\n",
    "    #Returns three tensors, all moved to the specified device (likely GPU):\n",
    "\n",
    "    #xns: The inputs with noise added (or original inputs if no noise)\n",
    "    #xs: The original clean inputs\n",
    "    #ys: The labels or targets\n",
    "    #\n",
    "    #\n",
    "    #This return structure is typical for denoising autoencoders, where you need \n",
    "    #both the noisy input (fed to the encoder) and the clean target \n",
    "    #(used to compute the reconstruction loss).\n",
    "    #\n",
    "    #This function is specifically designed for training denoising autoencoders, \n",
    "    #where the model learns to remove noise from corrupted inputs by trying \n",
    "    #to reconstruct the original clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2a01e-a8a1-4511-b110-554f37549158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_ae_dataset(samples):\n",
    "    \"\"\"\n",
    "    The function collates sampels into a batch, and creates noisy samples if DENOISING is True\n",
    "    for the denoising autoencoder.\n",
    "    \"\"\"\n",
    "    xs = [s[0] for s in samples]\n",
    "    ys = [s[1] for s in samples]\n",
    "    #Extracts the first element (input data) from each sample into list xs.\n",
    "    #Extracts the second element (labels or targets) from each sample into list ys.\n",
    "    #This assumes each sample is a tuple or list with at least two elements.\n",
    "    \n",
    "    xs = torch.stack(xs)\n",
    "    ys = torch.concat(ys)\n",
    "    #torch.stack(xs) combines the list of input tensors into a single \n",
    "    #tensor along a new dimension (creating a batch dimension).\n",
    "    #torch.concat(ys) concatenates the label tensors along \n",
    "    #the existing first dimension. This suggests the labels might have \n",
    "    #variable lengths or already include a batch-like dimension.\n",
    "\n",
    "    add_noise = NOISE_RATE > 0.\n",
    "    #Checks if noise should be added based on a global variable NOISE_RATE.\n",
    "    #If NOISE_RATE is greater than 0, noise will be added to the inputs.\n",
    "    \n",
    "    if add_noise:\n",
    "      sh = xs.shape\n",
    "      noise_mask = torch.bernoulli(torch.full(sh, NOISE_RATE))  # 0 (keep) or 1 (replace with noise)\n",
    "      #Gets the shape of the input tensor batch.\n",
    "      #Creates a binary mask using Bernoulli sampling, where each element has NOISE_RATE probability of being 1 \n",
    "      #(indicating where noise will be applied) and 1-NOISE_RATE probability of being 0.\n",
    "            \n",
    "      sp_noise = torch.bernoulli(torch.full(sh, 0.5))-0.5  # -1 or 1\n",
    "      #Generates the actual noise values as either -0.5 or 0.5.\n",
    "      #First creates a tensor of the same shape filled with 0.5, then applies Bernoulli \n",
    "      #sampling to get 0s or 1s.\n",
    "      #Subtracts 0.5 to convert to -0.5 or 0.5 (this creates salt and pepper noise).\n",
    "        \n",
    "      xns = xs * (1-noise_mask) + sp_noise * noise_mask\n",
    "      #Creates the noisy input xns by:\n",
    "          #Keeping original values where the mask is 0: xs * (1-noise_mask)\n",
    "          #Adding noise values where the mask is 1: sp_noise * noise_mask\n",
    "          #The result is a tensor where some values are preserved \n",
    "          #from the original input and others are replaced with noise.\n",
    "      \n",
    "      # sp = sp_noise\n",
    "    else:\n",
    "       xns = xs\n",
    "    #If no noise is to be added, the noisy input is the same as the original input.\n",
    "\n",
    "    return xns.to(device), xs.to(device), ys.to(device)\n",
    "    #Returns three tensors, all moved to the specified device (likely GPU):\n",
    "\n",
    "    #xns: The inputs with noise added (or original inputs if no noise)\n",
    "    #xs: The original clean inputs\n",
    "    #ys: The labels or targets\n",
    "    #\n",
    "    #\n",
    "    #This return structure is typical for denoising autoencoders, where you need \n",
    "    #both the noisy input (fed to the encoder) and the clean target \n",
    "    #(used to compute the reconstruction loss).\n",
    "    #\n",
    "    #This function is specifically designed for training denoising autoencoders, \n",
    "    #where the model learns to remove noise from corrupted inputs by trying \n",
    "    #to reconstruct the original clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ad763-5af9-40ef-8633-213b4738fb86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af426c-1e58-44b9-9189-1542edda4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(valid_loader):\n",
    "  # 1. get numpy array of all validation images:\n",
    "  val_images_noisy = []\n",
    "  val_images = []\n",
    "  val_labels = []\n",
    "\n",
    "  for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "      val_images_noisy.append(noisy_data.detach().cpu().numpy())\n",
    "      val_images.append(data.detach().cpu().numpy())\n",
    "      val_labels.append(target.detach().cpu().numpy())\n",
    "\n",
    "  val_images_noisy = np.concatenate(val_images_noisy, axis=0)\n",
    "  val_images = np.concatenate(val_images, axis=0)\n",
    "  val_labels = np.concatenate(val_labels, axis=0)\n",
    "\n",
    "  # 2. get numpy array of balanced validation samples for visualization:\n",
    "  sample_images_noisy = []\n",
    "  sample_images = []\n",
    "  sample_labels = []\n",
    "  single_el_idx = []  # indexes of single element per class for visualization\n",
    "\n",
    "  n_class = np.max(val_labels) + 1\n",
    "  # Determines the number of classes (for MNIST, this would be 10, representing digits 0-9).\n",
    "  for class_idx in range(n_class):\n",
    "    map_c = val_labels == class_idx\n",
    "\n",
    "    ims_c_noisy = val_images_noisy[map_c]\n",
    "    ims_c = val_images[map_c]\n",
    "    print('class label:')\n",
    "    print(class_idx)\n",
    "    print('shape selected class')\n",
    "    print(ims_c.shape)\n",
    "    # For each class:\n",
    "       # Creates a boolean mask map_c identifying all samples of the current class.\n",
    "       # Extracts noisy and clean images for just this class.\n",
    "      \n",
    "\n",
    "    samples_idx = np.random.choice(len(ims_c), N_SAMPLE, replace=False)\n",
    "\n",
    "    ims_c_noisy_samples = ims_c_noisy[samples_idx]\n",
    "    ims_c_samples = ims_c[samples_idx]\n",
    "    # Randomly selects N_SAMPLE images from the current class.\n",
    "    # replace=False ensures no duplicates are selected.\n",
    "    # Extracts both noisy and clean versions of these sampled images.\n",
    "      \n",
    "\n",
    "    sample_images_noisy.append(ims_c_noisy_samples)\n",
    "    sample_images.append(ims_c_samples)\n",
    "\n",
    "    sample_labels.append([class_idx]*N_SAMPLE)\n",
    "\n",
    "    # Adds the sampled noisy images, clean images, and labels to their respective lists.\n",
    "    # Creates an array of N_SAMPLE repeated labels for this class.\n",
    "\n",
    "    start_idx = N_SAMPLE*class_idx\n",
    "    single_el_idx.extend([start_idx + i for i in range(min(N_VIS_SAMPLE, N_SAMPLE))])\n",
    "    # Calculates the indices for the first N_VIS_SAMPLE elements of this class in the final concatenated array.\n",
    "    # These indices will be used to extract a smaller subset for visualization.\n",
    "\n",
    "    \n",
    "  sample_images_noisy = np.concatenate(sample_images_noisy, axis=0)\n",
    "  sample_images = np.concatenate(sample_images, axis=0)\n",
    "  sample_labels = np.concatenate(sample_labels, axis=0)\n",
    "  single_el_idx = np.array(single_el_idx)\n",
    "  #Combines all class samples into single arrays.\n",
    "  #Converts the index list to a NumPy array.\n",
    "\n",
    "  samples = {\n",
    "      'images_noisy': sample_images_noisy,\n",
    "      'images': sample_images,\n",
    "      'labels': sample_labels,\n",
    "      'single_el_idx': single_el_idx\n",
    "\n",
    "  }\n",
    "  return samples\n",
    "# Creates and returns a dictionary with all collected samples.\n",
    "\n",
    "\n",
    "# This function ensures we have:\n",
    "# \n",
    "# A balanced number of samples for each class (equal representation)\n",
    "# Both noisy and clean versions of each image\n",
    "# A mapping between the noisy and clean versions\n",
    "# A subset of indices for visualization purposes\n",
    "# This is particularly useful for creating visualizations that show how the model behaves across different classes, or for comparing reconstruction quality across digits.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d87e7-1730-4229-bd34-35f5c8b1f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np_showable(pt_img):\n",
    "  np_im = pt_img.detach().cpu().numpy()\n",
    "  if len(np_im.shape) == 4:\n",
    "    np_im = np_im[0]\n",
    "\n",
    "  if np_im.shape[0] > 3:\n",
    "    np_im = np_im[-3:]\n",
    "\n",
    "  return (eo.rearrange(np_im, 'c h w -> h w c')/2+.5).clip(0., 1.)\n",
    "\n",
    "#This function converts a PyTorch tensor image to a NumPy array suitable for visualization.\n",
    "#pt_img.detach().cpu().numpy() - Detaches the tensor from the computation graph, moves it to CPU if it's on GPU, and converts it to a NumPy array.\n",
    "#if len(np_im.shape) == 4: - Checks if the image has a batch dimension (shape: [batch, channels, height, width]).\n",
    "#np_im = np_im[0] - If there's a batch dimension, takes only the first image in the batch.\n",
    "#if np_im.shape[0] > 3: - Checks if there are more than 3 channels.\n",
    "#np_im = np_im[-3:] - If there are more than 3 channels, keeps only the last 3 channels (useful for handling multi-channel data).\n",
    "#eo.rearrange(np_im, 'c h w -> h w c') - Uses the einops library to rearrange the tensor from PyTorch's [channels, height, width] format to matplotlib's [height, width, channels] format.\n",
    "#/2+.5 - Applies normalization assuming the image data is in the range [-1, 1], converting it to [0, 1].\n",
    "#.clip(0., 1.) - Ensures all values are within the [0, 1] range, clamping any values outside this range.\n",
    "\n",
    "def plot_im(im, is_torch=True):\n",
    "  plt.imshow(to_np_showable(im) if is_torch else im, cmap='gray')\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "#This function plots a single image.\n",
    "#is_torch=True - Default parameter indicating whether the input is a PyTorch tensor.\n",
    "#to_np_showable(im) if is_torch else im - Converts the image to a NumPy array if it's a PyTorch tensor, otherwise uses it directly.\n",
    "#plt.imshow(..., cmap='gray') - Displays the image using matplotlib with a grayscale colormap.\n",
    "#plt.show() - Renders the plot.\n",
    "#plt.close() - Closes the figure to free up memory.\n",
    "\n",
    "def plot_im_samples(ds, n=5, is_torch=False):\n",
    "  fig, axs = plt.subplots(1, n, figsize=(16, n))\n",
    "  for i, image in enumerate(ds[:n]):\n",
    "      axs[i].imshow(to_np_showable(image) if is_torch else image, cmap='gray')\n",
    "      axs[i].set_axis_off()\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "#This function plots multiple images from a dataset in a row.\n",
    "#ds - The dataset or collection of images to sample from.\n",
    "#n=5 - Default number of images to display.\n",
    "#is_torch=False - Default parameter indicating whether the inputs are PyTorch tensors.\n",
    "#plt.subplots(1, n, figsize=(16, n)) - Creates a figure with a single row of n subplots, with a width of 16 inches and height of n inches.\n",
    "#The loop iterates through the first n images in the dataset:\n",
    "#\n",
    "#axs[i].imshow(...) - Displays each image in its corresponding subplot.\n",
    "#axs[i].set_axis_off() - Removes axis labels and ticks for cleaner visualization.\n",
    "#\n",
    "#\n",
    "#plt.show() - Renders the entire plot with all images.\n",
    "#plt.close() - Closes the figure to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f50b9-bb6f-41f9-b91f-51433e65dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and std of an array with numpy:\n",
    "def get_mean_std(x):\n",
    "    x_mean = np.mean(x)\n",
    "    x_std = np.std(x)\n",
    "    return x_mean, x_std\n",
    "\n",
    "# get min and max of an array with numpy:\n",
    "def get_min_max(x):\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    return x_min, x_max\n",
    "\n",
    "def is_iterable(obj):\n",
    "    try:\n",
    "        iter(obj)\n",
    "    except Exception:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "#This function checks if an object is iterable (can be looped over).\n",
    "#It uses a try-except block to attempt to call iter(obj), which will succeed only if obj is iterable.\n",
    "#If calling iter(obj) raises any exception, the function returns False.\n",
    "#If no exception occurs, the function returns True.\n",
    "\n",
    "def type_len(obj):\n",
    "    t = type(obj)\n",
    "    if is_iterable(obj):\n",
    "        sfx = f', shape: {obj.shape}' if t == np.ndarray else ''\n",
    "        print(f'type: {t}, len: {len(obj)}{sfx}')\n",
    "    else:\n",
    "        print(f'type: {t}, len: {len(obj)}')\n",
    "\n",
    "#This is a utility function for debugging that prints information about an object.\n",
    "#t = type(obj) - Gets the type of the provided object.\n",
    "#It checks if the object is iterable using the is_iterable function defined earlier.\n",
    "#If the object is iterable:\n",
    "#\n",
    "#It checks if the object is a NumPy array (t == np.ndarray).\n",
    "#If it's a NumPy array, it adds shape information to the output string.\n",
    "#It prints the type and length of the object, along with shape information if applicable.\n",
    "#\n",
    "#\n",
    "#If the object is not iterable, it still attempts to print the type and length (though this might raise an error if len() isn't applicable to the object).\n",
    "#\n",
    "#Note: There seems to be an issue with the type_len function - it tries to call len() on non-iterable objects in the else clause, \n",
    "#which would typically cause an error. This might be a bug in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33cc4e0-c874-4eaa-950a-8cd3a9ae9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging 2d matrix of images in 1 image\n",
    "def mosaic(mtr_of_ims):\n",
    "  ny = len(mtr_of_ims)\n",
    "  assert(ny != 0)\n",
    "  #Gets the number of rows in the matrix and asserts that it's not empty.\n",
    "\n",
    "  nx = len(mtr_of_ims[0])\n",
    "  assert(nx != 0)\n",
    "  #Gets the number of columns in the first row and asserts that it's not empty.\n",
    "\n",
    "  im_sh = mtr_of_ims[0][0].shape\n",
    "\n",
    "  assert (2 <= len(im_sh) <= 3)\n",
    "  #Gets the shape of the first image in the matrix.\n",
    "  #Verifies that the image is either 2D (grayscale) or 3D (with channels).\n",
    "    \n",
    "  multichannel = len(im_sh) == 3\n",
    "\n",
    "  if multichannel:\n",
    "    h, w, c = im_sh\n",
    "  else:\n",
    "    h, w = im_sh\n",
    "  #Determines if the images have multiple channels.\n",
    "  #If multichannel, unpacks height, width, and channels. Otherwise, just height and width.\n",
    "\n",
    "  h_c = h * ny + 1 * (ny-1)\n",
    "  w_c = w * nx + 1 * (nx-1)\n",
    "  #Calculates the total height and width of the canvas.\n",
    "  #Adds 1 pixel spacing between images (both horizontally and vertically).\n",
    "\n",
    "  canv_sh = (h_c, w_c, c) if multichannel else (h_c, w_c)\n",
    "  canvas = np.ones(shape=canv_sh, dtype=np.float32)*0.5\n",
    "  #Defines the shape of the canvas based on whether images are multichannel.\n",
    "  #Creates a canvas filled with gray (0.5) values, assuming image values are in [0,1] range.\n",
    "\n",
    "  for iy, row in enumerate(mtr_of_ims):\n",
    "    y_ofs = iy * (h + 1)\n",
    "    #Loops through each row of images.\n",
    "    #Calculates the vertical offset for the current row.\n",
    "    for ix, im in enumerate(row):\n",
    "      x_ofs = ix * (w + 1)\n",
    "      #Loops through each image in the current row.\n",
    "      #Calculates the horizontal offset for the current image.\n",
    "      canvas[y_ofs:y_ofs + h, x_ofs:x_ofs + w] = im\n",
    "      #Copies the current image to the appropriate position in the canvas.\n",
    "      #This uses NumPy's array slicing to place the image at the correct location.\n",
    "  return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1282bdf0-08a6-49fb-8423-78829574e1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "419008bd-3a6d-41f5-a00d-83b97b51e16c",
   "metadata": {},
   "source": [
    "## Define file paths: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddf17f-30b2-4e36-bbbf-75e1ecfe36b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project_path = Path.cwd()\n",
    "root_path = (project_path / '..').resolve()\n",
    "\n",
    "# Define paths\n",
    "image_dir = root_path/'data'  # Replace with your directory containing images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6d721-e97c-4264-9aa8-e5af521ccbad",
   "metadata": {},
   "source": [
    "## Before running the workflow below manually copy the image files into root_path/'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e610d48-d1ff-435d-9caf-b8b9436d7132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6c578de-0f90-468a-ac33-440cae517f7a",
   "metadata": {},
   "source": [
    "## Get information about image size: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4822edc-4de4-4eba-85e6-d1319c7cea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analyze_image_sizes(image_dir, extensions=('.jpg', '.jpeg', '.tif', '.tiff'))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd606ba-3261-4b0c-8a87-bde29feb2dbe",
   "metadata": {},
   "source": [
    "## Load meta data about images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec491021-6709-44f0-9dff-e1972cde2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_without_person = pd.read_csv(image_dir/'with_without_person_mod.csv')\n",
    "with_without_person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d87325-6ee9-4be0-9bb9-91f54c2c9de1",
   "metadata": {},
   "source": [
    "## Map image paths to labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89e4f2-ed5d-41b9-9634-1607b274ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = list(with_without_person.image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea326527-813b-4413-8e9c-efd347ad414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_without_person['image_id'] = img_idc.reconvert_image_ids(img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad5a25-1ac4-4d1b-b2c2-488946781cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_without_person.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa36a9-544a-4dec-b63e-e781a07ebd92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4b98d-8b57-4a72-a602-11c02b073ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = load_image_paths(image_dir)\n",
    "image_paths[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f676af-3aa6-46e7-a465-6caf999cb215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c5bd8-b697-4ecd-b360-f180ea019a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = []\n",
    "for image_path in image_paths:\n",
    "    path_str = str(image_path)\n",
    "    parts = path_str.split('.tif')\n",
    "    img_id = parts[-2][-3:]\n",
    "    img_ids.append(img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93d3476-846a-415a-93f8-39df9773e245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2851227d-2824-4051-b84e-eb11420da699",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_for_mapping = pd.DataFrame({'image_id': img_ids, 'image_paths': image_paths})\n",
    "image_paths_for_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc542948-5638-4427-96d2-2ffef86fb9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f385d3-f57c-4616-b9f0-5349c459ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_labels_mapping = with_without_person.merge(image_paths_for_mapping, how='inner', on='image_id')\n",
    "ids_labels_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c02718-9d82-41cd-9266-ef3b97241ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_person_only = ids_labels_mapping.loc[ids_labels_mapping.with_person == 1].copy()\n",
    "with_person_only.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf6220-2ff7-4377-9293-91f134a9b38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a1dc7c-b8c4-43e2-ab0b-0a7f2da9530b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3d26bdc0-f5be-4b67-96e2-a50528efcdea",
   "metadata": {},
   "source": [
    "labels = list(ids_labels_mapping.recognisable)\n",
    "print(type(labels))\n",
    "print(labels[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a79254-7c8e-4614-ab50-413148ee8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(ids_labels_mapping.with_person)\n",
    "print(type(labels))\n",
    "print(labels[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0a893-8dd0-432c-b9b4-cfb6f2aad51d",
   "metadata": {},
   "source": [
    "## Test convert_to_grayscale function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2c540-f2ea-45e0-9d02-35e41e9c43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and process image\n",
    "image_path = image_paths[100]\n",
    "original = Image.open(image_path)\n",
    "processed = convert_to_grayscale(original)  # Or any other function\n",
    "\n",
    "# Plot side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(original)\n",
    "ax1.set_title('Original')\n",
    "ax1.axis('off')\n",
    "\n",
    "#ax2.imshow(processed, cmap='gray' if processed.mode == 'L' else None)\n",
    "ax2.imshow(processed)\n",
    "ax2.set_title('After convert_to_grayscale')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c4e03-df8e-4e2f-b06f-1674601faf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = convert_to_grayscale(original)\n",
    "original_array = np.array(original)\n",
    "processed_array = np.array(processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3618f-434b-46f9-8952-9aa9ef35e554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2802de8-1679-449d-8a67-a59cf3b93ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values\n",
    "print(f\"Shape: {original_array.shape}\")\n",
    "print(f\"Data type: {original_array.dtype}\")\n",
    "print(f\"Min value: {original_array.min()}\")\n",
    "print(f\"Max value: {original_array.max()}\")\n",
    "print(f\"Mean value: {original_array.mean():.2f}\")\n",
    "print(f\"Unique values (first 10): {np.unique(original_array)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626886f0-903a-423f-b16b-e64918e58f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9becf63-0683-429d-8c37-6816131ff724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values\n",
    "print(f\"Shape: {processed_array.shape}\")\n",
    "print(f\"Data type: {processed_array.dtype}\")\n",
    "print(f\"Min value: {processed_array.min()}\")\n",
    "print(f\"Max value: {processed_array.max()}\")\n",
    "print(f\"Mean value: {processed_array.mean():.2f}\")\n",
    "print(f\"Unique values (first 10): {np.unique(processed_array)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca73c3b-2040-4e01-98b6-0cacf106e639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978a73b2-1d5c-4f25-8014-ee604a9ea44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baac750a-7bc8-422c-9261-d39fe91faa4a",
   "metadata": {},
   "source": [
    "## Test apply_aging_effect function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30b4fb-f753-47c9-a176-f49066caa3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and process image\n",
    "image_path = image_paths[0]\n",
    "original = Image.open(image_path)\n",
    "processed = apply_aging_effect(original)  # Or any other function\n",
    "\n",
    "# Plot side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(original)\n",
    "ax1.set_title('Original')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(processed, cmap='gray' if processed.mode == 'L' else None)\n",
    "ax2.set_title('After aging effect')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81e249-fdfd-45f3-8599-2dc6f6022155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42450e53-b7f0-4578-b6ce-9588d807b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_array = np.array(original)\n",
    "processed_array = np.array(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf3b80-9e11-4886-99b4-139ac2993aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c76707-e424-488e-aa84-57705fe5ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(original_array.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Histogram of Pixel Values')\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0efe05-28a5-4d91-93ea-676c0768ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(processed_array.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Histogram of Pixel Values')\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369023c-c390-4c9f-9eee-d2edd8cb719e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27216b8a-353a-4916-aecd-a97c5249685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_1 = load_and_preprocess_image(image_paths[0], target_size=(512, 512), convert_grayscale=True, apply_aging=False)\n",
    "type(test_image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80ad15-00b3-443a-abaf-a3328c93a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_2 = load_and_preprocess_image(image_paths[0], target_size=(512, 512), convert_grayscale=True, apply_aging=True)\n",
    "type(test_image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfc1a3e-8455-4182-baff-791a5151740a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0695c1dd-00a1-485a-8fa0-8bc1a307bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image_1, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50103d-a453-4a0e-ad9e-1d50bc51812d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d233c-9ee7-4ffe-90d5-895f18e68dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image_2, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0864d39-fbe4-444f-ad2c-ac30b74b4bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552dafa-1938-4b78-ad83-f0a7d76e1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(test_image_1, cmap='gray')\n",
    "ax1.set_title('Original')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(test_image_2, cmap='gray')\n",
    "ax2.set_title('After aging')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f812a-d7cd-4f77-8df8-7fa38bd2a227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052c624f-ea29-4733-a24d-6bdfa714e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_3 = load_and_preprocess_image(image_paths[0], target_size=(50, 50), convert_grayscale=True, apply_aging=False)\n",
    "type(test_image_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5bcbd-4f65-475a-a920-95de419e38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image_3, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9045a653-8f51-4c3c-8ece-67afc473d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader with the collate function\n",
    "from functools import partial\n",
    "\n",
    "# Set your noise rate and device\n",
    "NOISE_RATE = 0.1  # or whatever you use\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a52c3-7f99-46d1-8be9-1d4fb1ee34ef",
   "metadata": {},
   "source": [
    "## Try out work flow on a small subset of the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067f8a0-56ce-4431-ba0d-e52a61b6aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_few = image_paths[0:3]\n",
    "labels_few = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef875e7c-1df9-4462-abfa-0e6d6db99d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set globals (your existing code)\n",
    "NOISE_RATE = 0.1\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e030b0b-c9e6-4cec-8719-eb4a9d3936ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Load image paths\n",
    "# already done\n",
    "\n",
    "# Step 2: Create your labels list (matching image_paths order)\n",
    "# already done\n",
    "\n",
    "# Step 3: Create dataset (equivalent to train_dataset)\n",
    "dataset = create_image_dataset(\n",
    "    image_paths=image_paths_few,\n",
    "    labels=labels_few,\n",
    "    target_size=(572, 572),\n",
    "    convert_grayscale=True,\n",
    "    apply_aging=False,\n",
    "    mean=0.5,\n",
    "    std=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f95762-93b4-4ccf-8803-9fb4a026c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905214d9-52c6-4a34-903d-d88eae6449c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 4: Create DataLoader with your original collate function\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_ae_dataset,  # Your original function using globals\n",
    "    drop_last=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b6be4-c0eb-4342-a0c9-cfc0c299035b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb355e23-ff4e-46ec-89a9-26d75c989f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"DataLoader length: {len(train_loader)}\")\n",
    "print(f\"Dataset length: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a63f41-2e8a-45d4-ad8f-a4e4ec6b92a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0f005-b515-427a-87dc-09497b8e9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Use exactly like your MNIST workflow\n",
    "for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "    # Your existing autoencoder code works unchanged!\n",
    "    print(f\"Batch {batch_idx}: {noisy_data.shape}, {data.shape}, {target.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552cb69-c45e-4924-ad15-a329709e987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLE = 1\n",
    "N_VIS_SAMPLE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1227fc-aa8c-49f6-8805-66778da6b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_samples(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d0419-8425-473b-9142-732525fb94b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_el_idx = samples['single_el_idx']\n",
    "plot_im_samples(samples['images_noisy'][single_el_idx, 0], n=4, is_torch=False)\n",
    "plot_im_samples(samples['images'][single_el_idx, 0], n=4, is_torch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb3129-c543-4237-bbc6-f09fe25a9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code loads and examines a single sample from the validation dataset:\n",
    "for sample in dataset:\n",
    "    img, label = sample\n",
    "    print(type_len(img))\n",
    "    print(type_len(label))\n",
    "    print(img.shape, label.shape)\n",
    "    plt.hist(img.flatten(), bins=100)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9b1ec-f858-451f-8a2b-29ebc444a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line of code creates a histogram of the raw MNIST validation dataset pixel values:\n",
    "\n",
    "plt.hist(dataset.data.numpy().flatten(), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ff6e3-ca4d-48a7-bf1a-56f138e94d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c418f1-8335-4c6c-88dd-7da1b15eaf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_loader:\n",
    "    #- Begins iteration through the validation data loader, which provides batches of data.\n",
    "\n",
    "    noisy_img, img, label = sample\n",
    "    #Unpacks the first batch from the data loader into three components:\n",
    "        #noisy_img: The input images with noise added (for denoising autoencoder training)\n",
    "        #img: The original clean images (targets for reconstruction)\n",
    "        #label: The class labels for the images\n",
    "    print(type_len(noisy_img))\n",
    "    print(type_len(img))\n",
    "    print(type_len(label))\n",
    "    \n",
    "    print(noisy_img.shape, img.shape, label.shape)\n",
    "    #Directly prints the shapes of all three tensors in the batch.\n",
    "    \n",
    "    #plt.hist(img.flatten(), bins=100)\n",
    "    #A commented-out line that would create a histogram of all pixel values in the clean image batch if uncommented.\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef1405-db6d-409f-85a2-8b1c5d3cecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops as eo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871bdd2-6634-4f32-9008-64eab94c83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import einops as eo\n",
    "import pathlib as pl\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import collections  as mc\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from time import time as timer\n",
    "#import umap\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Audio\n",
    "import IPython\n",
    "\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563a563-ec49-4d07-b52f-1ad16f4b4041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb6059-2a94-42ee-b789-536128293c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ca9ae-e3c3-47b7-8eb4-bc458ade5cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcad9d4-c14c-47e4-bbfa-c6c493339510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, code_size):\n",
    "        self.input_size = list(input_size)  # shape of data sample\n",
    "        self.flat_data_size = np.prod(self.input_size)\n",
    "        self.hidden_size = 128\n",
    "\n",
    "        self.code_size = code_size  # code size\n",
    "\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        #Creates an autoencoder neural network that inherits from PyTorch's nn.Module.\n",
    "        #Takes two parameters:\n",
    "        #\n",
    "        #input_size: The shape of input data (e.g., [1, 28, 28] for MNIST)\n",
    "        #code_size: The dimension of the encoded representation (bottleneck)\n",
    "        #\n",
    "        #\n",
    "        #Calculates the flattened input size by multiplying all dimensions.\n",
    "        #Sets an intermediate hidden layer size of 128 neurons.\n",
    "        #Calls the parent class initializer.\n",
    "\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(self.flat_data_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size, self.code_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        #Defines the encoder network as a sequence of operations:\n",
    "            #\n",
    "            #Flattens the input (e.g., converts a 2D image to 1D)\n",
    "            #Linear layer mapping from input size to hidden size\n",
    "            #ReLU activation\n",
    "            #Linear layer mapping from hidden size to code size\n",
    "            #Sigmoid activation (constrains the encoded values to [0, 1])\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.code_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size, self.flat_data_size),\n",
    "            nn.Tanh(),  # Think: why tanh?\n",
    "\n",
    "            nn.Unflatten(1, self.input_size),\n",
    "        )\n",
    "        #Defines the decoder network:\n",
    "            #Linear layer from code size to hidden size\n",
    "            #ReLU activation\n",
    "            #Linear layer from hidden size back to the flattened input size\n",
    "            #Tanh activation (outputs values in [-1, 1], matching the normalized input range)\n",
    "            #Unflattens the output back to the original input shape\n",
    "\n",
    "#Regarding \"why tanh?\": Tanh is used because the input images were normalized to approximately [-0.5, 0.5] \n",
    "    #range (using m=0.5, s=1.0). Tanh outputs values in the range [-1, 1], \n",
    "    #which after scaling by 1.1 in the decode method closely matches the input data range.\n",
    "\n",
    "    def forward(self, x, return_z=False):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return (decoded, encoded) if return_z else decoded\n",
    "    # The forward pass:\n",
    "        #Encodes the input\n",
    "        #Decodes the encoded representation\n",
    "        #If return_z=True, returns both the reconstruction and the encoded values\n",
    "        #Otherwise, just returns the reconstruction\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)*1.1\n",
    "# Helper methods to encode and decode separately\n",
    "# Note the multiplication by 1.1 in the decode method, \n",
    "    # which slightly amplifies the output range to better match the input data distribution\n",
    "\n",
    "        \n",
    "\n",
    "    def get_n_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    # Utility method to count the total number of trainable parameters in the model\n",
    "\n",
    "\n",
    "def eval_on_samples(ae_model, epoch, samples):\n",
    "    # this is called on end of each training epoch\n",
    "    xns = samples['images_noisy']\n",
    "    xns = torch.tensor(xns, dtype=torch.float32).to(device)\n",
    "    #labels = samples['labels']\n",
    "\n",
    "# Function to evaluate the autoencoder on sample data after each epoch\n",
    "# Takes the model, current epoch number, and samples dictionary\n",
    "# Extracts noisy images from the samples and converts them to a PyTorch tensor on the target device\n",
    "# The labels are extracted but commented out (not used)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yz = ae_model(xns, return_z=True)\n",
    "        yz = [el.detach().cpu().numpy() for el in yz]\n",
    "\n",
    "        y = yz[0]\n",
    "        z = yz[1:]\n",
    "    # Uses torch.no_grad() to disable gradient calculation (for efficiency during evaluation)\n",
    "    # Gets both reconstructions and encodings (i.e. latent space!) by calling the model with return_z=True\n",
    "    # Converts all outputs to NumPy arrays\n",
    "    # Separates the reconstructions y and encodings z\n",
    "\n",
    "    res = {'z': z, 'y': y, 'epoch': epoch}\n",
    "    return res\n",
    "\n",
    "# Creates and returns a dictionary containing:\n",
    "\n",
    "# z: The encoded representations\n",
    "# y: The reconstructed images\n",
    "# epoch: The current epoch number\n",
    "# \n",
    "\n",
    "# This evaluation function captures the model's performance at each epoch, allowing for tracking reconstruction quality and analyzing the learned representations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfc7bae-2221-4265-92dc-d4ee1b405ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883a7f8-ddd8-4ce5-8165-b4fc8c37a36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4ba1f-effe-4f0f-8480-4fbce6a4b952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cdce34-b59f-470a-b9b9-6a1ff3994a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673bb84f-38a9-4178-940d-012988338561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecb916-0107-4a49-aed5-e5aaaff2530e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff6be69-0a98-4b3c-bcde-7b0de9d4d9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73381bfe-c689-42d6-a747-3741f72b7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(history, logscale=True):\n",
    "    \"\"\"\n",
    "    plot training loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = history['loss']\n",
    "    v_loss = history['val_loss']\n",
    "    epochs = history['epoch']\n",
    "\n",
    "    # This function visualizes training history (loss over epochs).\n",
    "    # Extracts training loss, validation loss, and epoch numbers from the history dictionary.\n",
    "\n",
    "    \n",
    "    plot = plt.semilogy if logscale else plt.plot\n",
    "    # Cleverly chooses between logarithmic scale (plt.semilogy) or linear scale (plt.plot) based on the logscale parameter.\n",
    "    # Default is logarithmic scale, which is often better for visualizing loss curves as they typically decrease exponentially.\n",
    "    \n",
    "    plot(epochs, loss, label='training');\n",
    "    plot(epochs, v_loss, label='validation');\n",
    "    # Plots both training and validation loss curves using the selected plotting function.\n",
    "    # Labels each curve for the legend.\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # Adds a legend, axis labels, displays the plot, and then closes the figure.\n",
    "\n",
    "\n",
    "\n",
    "def plot_samples(sample_history, samples, epoch_stride=5, fig_scale=1):\n",
    "    \"\"\"\n",
    "    Plots input, noisy samples (for DAE) and reconstruction.\n",
    "    Each `epoch_stride`-th epoch\n",
    "    \"\"\"\n",
    "    # This function visualizes sample reconstructions over training epochs.\n",
    "    # Shows how the model's reconstruction capability improves over time.\n",
    "\n",
    "    single_el_idx = samples['single_el_idx']\n",
    "    images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
    "    images = samples['images'][single_el_idx, 0]\n",
    "    # Extracts indices for selected samples to visualize.\n",
    "    # Gets the noisy input images and the original clean images for these samples.\n",
    "    # The , 0 indexing suggests selecting the first channel of each image.\n",
    "\n",
    "    last_epoch = np.max(list(sample_history.keys()))\n",
    "    # Determines the last epoch number in the history data.\n",
    "\n",
    "    for epoch_idx, hist_el in sample_history.items():\n",
    "      if epoch_idx % epoch_stride != 0 and epoch_idx != last_epoch:\n",
    "        continue\n",
    "    # Iterates through each epoch's results in the history.\n",
    "    # Uses epoch_stride to select only every nth epoch (to avoid too many visualizations).\n",
    "    # Always includes the last epoch regardless of the stride.\n",
    "\n",
    "      samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
    "    # Creates an array of three sets of images to visualize side by side:\n",
    "       # The noisy input images\n",
    "       # The model's reconstructions for the current epoch\n",
    "       # The original clean images (ground truth)\n",
    "\n",
    "      ny = len(samples_arr)\n",
    "      nx = len(samples_arr[0])\n",
    "\n",
    "      plt.figure(figsize=(fig_scale*nx, fig_scale*ny))\n",
    "      # Calculates the dimensions of the visualization grid.\n",
    "      # Creates a figure with size proportional to the number of samples.\n",
    "\n",
    "        \n",
    "      m = mosaic(samples_arr)\n",
    "      # Uses the previously defined mosaic function to create a grid of all images.\n",
    "\n",
    "      plt.title(f'after epoch {int(epoch_idx)}')\n",
    "      plt.imshow(m, cmap='gray', vmin=-.5, vmax=.5)\n",
    "      # Adds a title showing which epoch this visualization represents.\n",
    "      # Displays the mosaic with a grayscale colormap and fixed value range.\n",
    "      # The vmin=-.5, vmax=.5 matches the normalized data range we've seen before.\n",
    "\n",
    "        \n",
    "      plt.tight_layout(pad=0.1, h_pad=0, w_pad=0)\n",
    "      plt.show()\n",
    "      plt.close()\n",
    "      # Ensures proper spacing in the figure.\n",
    "      # Displays the figure and then closes it to free memory.\n",
    "\n",
    "# This function creates a powerful visualization showing the progression of the model's reconstruction ability across epochs. Each visualization has three rows:\n",
    "# \n",
    "# The noisy inputs\n",
    "# The model's reconstructions\n",
    "# The original clean images (targets)\n",
    "# \n",
    "# This makes it easy to see how the model gradually learns to denoise and reconstruct the images over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634b993-b198-4364-8cd9-6af3ec2fc434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a984fbf-a3c1-439f-a680-3a060ec98c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are utility functions for working with trained models at different stages of training. Let me break them down:\n",
    "\n",
    "def run_on_trained(model, root_dir, run_fn, ep=None, model_filename=None):\n",
    "    \"\"\"\n",
    "    Helper function to excecute any function on model in state after `ep` training epoch\n",
    "    \"\"\"\n",
    "    # This function loads a model checkpoint and runs a specified function on it.\n",
    "    # Parameters:\n",
    "    # \n",
    "    # model: The neural network model instance\n",
    "    # root_dir: Directory containing saved model checkpoints\n",
    "    # run_fn: The function to run on the loaded model\n",
    "    # ep: Specific epoch to load (optional)\n",
    "    # model_filename: Specific checkpoint file to load (optional)\n",
    "\n",
    "    if model_filename is None:\n",
    "        if ep is not None:\n",
    "            model_filename = root_dir/f'model_{ep:03d}.pth'\n",
    "        else:\n",
    "            model_filename = sorted(list(root_dir.glob('*.pth')))[-1]  # last model state\n",
    "    # Determines which model checkpoint file to load:\n",
    "    # \n",
    "    # If a specific filename is provided, use that (in this case this code block would be skipped)\n",
    "    # If an epoch number is provided, construct the filename using a pattern\n",
    "    # If neither is provided, use the last checkpoint file (by alphabetical sorting)\n",
    "    # The code uses pathlib's Path objects for file handling (using / for path joining)\n",
    "\n",
    "    \n",
    "    model_dict = torch.load(model_filename,weights_only=False)\n",
    "\n",
    "    model.load_state_dict(model_dict['model_state_dict'])\n",
    "\n",
    "    # Loads the saved model state from the specified file\n",
    "    # The weights_only=False parameter indicates to load the full state dictionary (not just weights)\n",
    "    # Restores the model parameters from the saved state dictionary\n",
    "    \n",
    "\n",
    "    run_fn(model)\n",
    "    # Calls the provided function on the loaded model\n",
    "\n",
    "def run_on_all_training_history(model, root_dir, run_fn, n_ep=None):\n",
    "    \"\"\"\n",
    "    Helper function to excecute any function on model state after each of the training epochs\n",
    "    \"\"\"\n",
    "    # This function runs a specified function on multiple model checkpoints from different training epochs.\n",
    "    # Parameters:\n",
    "    # \n",
    "    # model: The neural network model instance\n",
    "    # root_dir: Directory containing saved model checkpoints\n",
    "    # run_fn: The function to run on each loaded model state\n",
    "    # n_ep: Specific number of epochs to process (optional)\n",
    "    \n",
    "    if n_ep is not None:\n",
    "        for ep in range(n_ep):\n",
    "            print(f'running on epoch {ep+1}/{n_ep}...')\n",
    "            run_on_trained(model, root_dir, run_fn, ep=ep)\n",
    "    # If a specific number of epochs is provided:\n",
    "    # \n",
    "    # Iterates through each epoch from 0 to n_ep-1\n",
    "    # Prints progress information\n",
    "    # Calls run_on_trained for each epoch\n",
    "    \n",
    "    else:\n",
    "        for model_filename in sorted(root_dir.glob('*.pth')):\n",
    "            print(f'running on checkpoint {model_filename}...')\n",
    "            run_on_trained(model, root_dir, run_fn, model_filename=model_filename)\n",
    "\n",
    "    # If no specific number of epochs is provided:\n",
    "    # \n",
    "    # Finds all .pth files in the root directory\n",
    "    # Sorts them (presumably by name, which would be by epoch if using the naming pattern)\n",
    "    # Processes each checkpoint file in order\n",
    "    \n",
    "    print(f'done')\n",
    "\n",
    "    # Prints a completion message when all checkpoints have been processed\n",
    "    # \n",
    "    # These utility functions make it easy to:\n",
    "    # \n",
    "    # Analyze a model at a specific point in its training history\n",
    "    # Run the same analysis across multiple stages of training\n",
    "    # Visualize or evaluate how the model's behavior changes over the course of training\n",
    "    # \n",
    "    # They're particularly useful for post-training analysis, debugging, and creating visualizations of model evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d734e38-9c3a-4445-ac5c-58e422ab8880",
   "metadata": {},
   "source": [
    "## Try out some models to check if the code works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d9609-9bf3-460d-b36e-19390208daf5",
   "metadata": {},
   "source": [
    "### Try untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b5b64-3ba1-4cfb-a616-a909901f673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block initializes and tests the autoencoder model with a sample batch. \n",
    "# Let me explain it line by line:\n",
    "train_batch = next(iter(train_loader))\n",
    "# Gets the first batch from the training data loader without running a full epoch\n",
    "# iter(train_loader) creates an iterator from the data loader\n",
    "# next() retrieves the first element from that iterator (the first batch)\n",
    "xns, xs, ys = train_batch\n",
    "# Unpacks the batch into three components:\n",
    " # xns: The noisy input images\n",
    " # xs: The clean original images\n",
    " # ys: The class labels\n",
    "\n",
    "print('sample shapes:', xns.shape, xs.shape, ys.shape)\n",
    "# Prints the shapes of all three tensors to verify their dimensions\n",
    "# Likely shows something like [batch_size, 1, 28, 28] for the images\n",
    "\n",
    "in_size = xns.shape[1:]\n",
    "print(in_size)\n",
    "# Extracts the input size excluding the batch dimension\n",
    "# For MNIST, this would be [1, 28, 28] (channels, height, width)\n",
    "\n",
    "ae = AutoEncoder(input_size=in_size, code_size=10).to(device)\n",
    "# Creates an instance of the AutoEncoder model:\n",
    "# \n",
    "# input_size is set to the dimensions of the input data\n",
    "# code_size=10 defines the bottleneck dimension (the size of the encoded representation)\n",
    "# .to(device) moves the model to the appropriate device (CPU or GPU)\n",
    "\n",
    "y = ae(xns)\n",
    "# Performs a forward pass through the model with the noisy images\n",
    "# The model attempts to reconstruct the clean images from the noisy ones\n",
    "# Since return_z=False by default, this only returns the reconstructions\n",
    "\n",
    "print('output shape:', y.shape)\n",
    "# Prints the shape of the model's output\n",
    "# Should match the input shape, as the autoencoder reconstructs the original dimensions\n",
    "\n",
    "plot_im_samples(xns, is_torch=True)\n",
    "# Visualizes a few of the noisy input images using the previously defined function\n",
    "\n",
    "plot_im_samples(y, is_torch=True)\n",
    "# Visualizes the corresponding reconstructed images\n",
    "# This allows comparing the model's initial reconstructions before training -> The reconstructions are \n",
    "# just noise because the model has not been trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a410fc0e-4062-4aa2-b81a-ee07089b61dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17708be-87a3-47bd-b977-3df5c9e972c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code compares the pixel value distributions of an input image and its reconstruction. \n",
    "# Here's what each line does:\n",
    "\n",
    "x = xns[0]# - y[1]\n",
    "# Selects the first image from the batch of noisy inputs.\n",
    "# Note that there's a commented-out subtraction (# - y[1])\n",
    "\n",
    "d = y[0]# - y[1]\n",
    "# Selects the first image from the batch of reconstructed outputs.\n",
    "# Again, there's a commented-out subtraction\n",
    "\n",
    "im0 = x[0].detach().cpu().numpy()\n",
    "# Takes the first channel of the selected input image\n",
    "# Detaches it from the computation graph (no gradients needed)\n",
    "# Moves it to CPU if it was on GPU\n",
    "# Converts it to a NumPy array\n",
    "\n",
    "im1 = d[0].detach().cpu().numpy()\n",
    "# Does the same conversion process for the reconstructed image\n",
    "\n",
    "# plt.imshow(im, cmap='gray', vmin=-1, vmax=1)\n",
    "# This is a commented-out visualization that would display the image\n",
    "\n",
    "bins = np.linspace(-1, 1, 100)\n",
    "# Creates 100 evenly spaced histogram bins from -1 to 1\n",
    "# This range is chosen to match the expected range of pixel values\n",
    "\n",
    "plt.hist(im0.flatten(), bins, alpha=0.3);\n",
    "# Creates a histogram of all pixel values in the input image\n",
    "# flatten() converts the 2D image to a 1D array\n",
    "# alpha=0.3 makes the histogram semi-transparent\n",
    "\n",
    "plt.hist(im1.flatten(), bins, alpha=0.3);\n",
    "# Creates a histogram of all pixel values in the reconstructed image\n",
    "# Using the same bins and transparency\n",
    "# Overlaid on the same plot as the input image histogram\n",
    "\n",
    "\n",
    "# This visualization allows comparing the distribution of pixel values between \n",
    "# the noisy input and the reconstruction. It helps assess how well the autoencoder \n",
    "# is preserving the overall pixel value distribution and whether \n",
    "# it's correctly mapping values from the input distribution to the expected output distribution.\n",
    "# The semi-transparent overlapping histograms make it easy to see differences \n",
    "# in how pixel values are distributed between the original and reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc47a02-4ac5-4f64-842f-f73f572fc07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09b66f48-4528-45b9-8486-c7d2f9a7f620",
   "metadata": {},
   "source": [
    "### Try overfitting dense model (no validation set) to check if the model learns anything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c9dd84-884e-4d05-b1d9-bd0f700baa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code sets up the final model configuration and prepares the sample data for training:\n",
    "\n",
    "CODE_SIZE = 20\n",
    "# Sets the dimensionality of the encoded representation (bottleneck) to 5\n",
    "# This is smaller than the previous test where code_size was 10, \n",
    "# creating a more compressed representation\n",
    "\n",
    "\n",
    "NOISE_RATE = 0\n",
    "# Sets the noise rate for the denoising autoencoder to 0\n",
    "# This means no artificial noise will be added, making it \n",
    "# function as a standard autoencoder rather than a denoising one\n",
    "\n",
    "MODEL_NAME = 'ae_model'\n",
    "# Assigns a name to the model, likely used for saving checkpoints and organizing results\n",
    "\n",
    "model = AutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
    "# Creates a new instance of the AutoEncoder with:\n",
    "# \n",
    "# The previously determined input size (from the shape of the data)\n",
    "# The newly defined CODE_SIZE of 5\n",
    "# Placed on the appropriate device (CPU or GPU)\n",
    "\n",
    "samples = get_samples(train_loader)\n",
    "# Calls the previously defined get_samples function to create a balanced set of \n",
    "# validation samples\n",
    "# These samples will be used to monitor reconstruction quality during training\n",
    "# The function selects representative samples from each class for visualization\n",
    "# \n",
    "# This code block is preparing the final model configuration before training. \n",
    "# It's worth noting that with NOISE_RATE set to 0, this will train a standard \n",
    "# autoencoder rather than a denoising autoencoder, despite the earlier code \n",
    "# being set up to handle noise addition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8234bdc-ba43-4110-8f60-bef9fa9f4b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the autoencoder model, for N_EPOCHS epochs,\n",
    "# save history of loss values for training and validation sets,\n",
    "# history of validation samples evolution, and model weights history,\n",
    "\n",
    "\n",
    "# This code implements the complete training loop for the autoencoder. \n",
    "# Let me break it down:\n",
    "\n",
    "N_EPOCHS = 50\n",
    "LR = 0.0009\n",
    "# Sets the number of training epochs to 50\n",
    "# Sets the learning rate for the Adam optimizer to 0.0009\n",
    "\n",
    "model_root = pl.Path(MODEL_NAME)\n",
    "model_root.mkdir(exist_ok=True)\n",
    "# Creates a directory path for saving model checkpoints using the MODEL_NAME ('ae_model')\n",
    "# Makes sure the directory exists (creates it if it doesn't)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "# Creates an Adam optimizer to update the model parameters\n",
    "# Adam is an adaptive learning rate optimization algorithm well-suited for deep learning\n",
    "\n",
    "# implement loss explicitly\n",
    "loss = nn.MSELoss()\n",
    "# Defines the loss function as Mean Squared Error (MSE)\n",
    "# This measures the average squared difference between the reconstructed and target images\n",
    "\n",
    "# train the model\n",
    "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
    "sample_history = {}\n",
    "# Creates dictionaries to store training metrics and sample reconstructions\n",
    "# history tracks training and validation losses across epochs\n",
    "# sample_history will store sample reconstruction results at each epoch\n",
    "\n",
    "pbar = tqdm.tqdm(range(N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
    "# Creates a progress bar for tracking the training process\n",
    "# Will show the current epoch and update metrics during training\n",
    "\n",
    "for epoch_idx in pbar:\n",
    "# Starts the main training loop that runs for N_EPOCHS iterations\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # Initializes the epoch loss accumulator\n",
    "    # Sets the model to training mode (enables dropout, batch normalization updates, etc.)\n",
    "    \n",
    "    for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(noisy_data)\n",
    "        loss_value = loss(output, data)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value.detach().cpu().item()\n",
    "    # Iterates through all batches in the training dataset\n",
    "    # For each batch:\n",
    "    # \n",
    "        # Zeros out previous gradients\n",
    "        # Passes the noisy input through the model\n",
    "        # Calculates the MSE loss between the reconstruction and clean data\n",
    "        # Computes gradients via backpropagation\n",
    "        # Updates model parameters using the optimizer\n",
    "        # Accumulates the loss value for epoch-level reporting\n",
    "    \n",
    "    epoch_loss /= len(train_loader)\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['epoch'].append(epoch_idx)\n",
    "    # update progress bar\n",
    "\n",
    "    # Calculates the average loss for the epoch\n",
    "    # Records the loss and epoch number in the history\n",
    "\n",
    "    # evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "            output = model(noisy_data)\n",
    "            loss_value = loss(output, data)\n",
    "            val_loss += loss_value.detach().cpu().item()\n",
    "        val_loss /= len(train_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "    # Sets the model to evaluation mode (disables dropout, etc.)\n",
    "    # Disables gradient calculation for efficiency\n",
    "    # Computes the validation loss on the entire validation set\n",
    "    # Records the average validation loss in the history\n",
    "    \n",
    "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
    "    # evaluate on samples\n",
    "    # Updates the progress bar with current epoch, training loss, and validation loss\n",
    "    \n",
    "    sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
    "    # This saves the reconstructions and the latent space thanks to\n",
    "    # the eval_on_samples function where in the application of the \n",
    "    # model to the evaluation data the return_z parameter is set \n",
    "    # to true: \n",
    "    # with torch.no_grad():\n",
    "    #     yz = ae_model(xns, return_z=True)\n",
    "    #     yz = [el.detach().cpu().numpy() for el in yz]\n",
    "# \n",
    "    #     y = yz[0]\n",
    "    #     z = yz[1:]\n",
    "    \n",
    "    # The output of eval_on_samples looks like this: \n",
    "\n",
    "    # sample_res = {'z': z, 'y': y, 'epoch': epoch}\n",
    "\n",
    "    \n",
    "    sample_history[epoch_idx] = sample_res\n",
    "    # Evaluates the model on the sample images\n",
    "    # Stores reconstructions for later visualization\n",
    "\n",
    "    # save model weights\n",
    "    torch.save({\n",
    "                'epoch': epoch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "                }, model_root/f'model_{epoch_idx:03d}.pth')\n",
    "\n",
    "    # Saves a checkpoint of the model at each epoch\n",
    "    # The checkpoint includes:\n",
    "    # \n",
    "    # Current epoch number\n",
    "    # Model parameters\n",
    "    # Optimizer state (allows resuming training)\n",
    "    # Loss function\n",
    "    # \n",
    "    # \n",
    "    # Uses a formatted filename with padded epoch number (e.g., 'model_001.pth')\n",
    "# \n",
    "# This is a complete training pipeline that not only trains the model \n",
    "# but also tracks metrics, evaluates on validation data, \n",
    "# and creates visualizations to monitor progress - \n",
    "# all while saving checkpoints for later analysis or resuming training.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5d592-5e6d-4173-9ec1-d98676f942a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d789f-c9b3-4488-b7f6-f33f40c26bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39453216-9049-4315-9a8b-fde62321b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(sample_history, samples=samples, epoch_stride=5, fig_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee63e5f3-de09-4846-a31c-d449cfba4905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e86ab4-91fa-499f-8de6-0f9fa32648c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d2526-5d3b-4dd5-b476-591490eb4ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21096189-d2e8-41e9-b546-b9be33dafd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d126d5-4c78-4bee-a935-f8fbd56047a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83367646-7326-4bf4-b2a1-ed6d4694402a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
